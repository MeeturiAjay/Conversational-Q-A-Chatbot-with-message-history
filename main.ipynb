{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "678a58a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='Large Concept Models:\\nLanguage Modeling in a Sentence Representation Space\\nThe LCM team, Loïc Barrault∗, Paul-Ambroise Duquenne∗, Maha Elbayad∗, Artyom\\nKozhevnikov∗, Belen Alastruey†, Pierre Andrews†, Mariano Coria†, Guillaume Couairon+†, Marta\\nR. Costa-jussà†, David Dale†, Hady Elsahar†, Kevin Heffernan†, João Maria Janeiro†, Tuan Tran†,\\nChristophe Ropers†, Eduardo Sánchez†, Robin San Roman†, Alexandre Mourachko‡, Safiyyah\\nSaleem‡, Holger Schwenk‡\\nFAIR at Meta\\n∗Core contributors, alphabetical order, †Contributors to data preparation, LCM extensions and\\nevaluation, alphabetical order, ‡Research and project management, alphabetical order, +Initial work\\nwhile at FAIR at Meta, new affiliation: INRIA, France\\nLLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for\\nmany tasks. The current established technology ofLLMs is to process input and generate output at\\nthe token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well\\nbeyond single words, to analyze information and to generate creative content. In this paper, we present\\nan attempt at an architecture which operates on an explicit higher-level semantic representation,\\nwhich we name a“concept”. Concepts are language- and modality-agnostic and represent a higher\\nlevel idea or action in a flow. Hence, we build a“Large Concept Model”. In this study, as\\nproof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence\\nembedding space,SONAR, which supports up to 200 languages in both text and speech modalities.\\nThe Large Concept Modelis trained to perform autoregressive sentence prediction in an embedding\\nspace. We explore multiple approaches, namely MSE regression, variants of diffusion-based generation,\\nand models operating in a quantizedSONAR space. These explorations are performed using 1.6B\\nparameter models and training data in the order of 1.3T tokens. We then scale one architecture to a\\nmodel size of 7B parameters and training data of about 2.7T tokens. We perform an experimental\\nevaluation on several generative tasks, namely summarization and a new task of summary expansion.\\nFinally, we show that our model exhibits impressive zero-shot generalization performance to many\\nlanguages, outperforming existingLLMs of the same size. The training code of our models is freely\\navailable.a\\nDate: December 12, 2024\\nCorrespondence: Holger Schwenk atschwenk@meta.com\\nahttps://github.com/facebookresearch/large_concept_model\\n1 Introduction\\nLarge Language models (LLMs) are dominating current research in natural language processing,\\nand with their recent extension to more modalities, namely images, video and speech, they seem to\\nbe considered as the de-facto technique to follow to approach human intelligence.LLMs achieve\\nindeed impressive performance on a large variety of tasks, such as providing detailed answers for\\ngeneral knowledge questions, helping in performing long document analysis, or drafting different\\ntypes of messages, and writing or debugging code. Building anLLM from scratch requires access to\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='enormous computational resources to process ever larger amounts of data and train models, the size\\nof which now exceeds four hundred billion parameters. Knowledge acquisition inLLMs is heavily\\ndata-driven and extending them to more languages or modalities usually requires injecting additional\\n(synthetic) data to cover them.\\nThe landscape of availableLLMs can be structured into open models such asLlama (The Llama3\\nteam, 2024), Mistral (Jiang et al., 2024), Bloom (BigScience Workshop, 2023) or Falcon\\n(Almazrouei et al., 2023), on the one hand, and closed models such asGemini (Gemini Team Google,\\n2024), GPT (OpenAI, 2024) orClaude (Anthropic, 2024), on the other. It is striking that all\\nthese models are based on the same underlying architecture: a transformer-based, decoder-only\\nlanguage model, which is pretrained to predict the next token, given a long context of preceding\\ntokens. Despite the undeniable success ofLLMs and continued progress, all currentLLMs miss\\na crucial characteristic of human intelligence: explicit reasoning and planning at multiple levels of\\nabstraction. The human brain does not operate at the word level only. We usually have a top-down\\nprocess to solve a complex task or compose a long document: we first plan at a higher level the\\noverall structure, and then step-by-step, add details at lower levels of abstraction. One may argue\\nthat LLMs are implicitly learning a hierarchical representation, but we stipulate that models with\\nan explicit hierarchical architecture are better suited to create coherent long-form output.\\nImagine a researcher giving a fifteen-minute talk. In such a situation, researchers do not usually\\nprepare detailed speeches by writing out every single word they will pronounce. Instead, they outline\\na flow of higher-level ideas they want to communicate. Should they give the same talk multiple\\ntimes, the actual words being spoken may differ, the talk could even be given in different languages,\\nbut the flow of higher-level abstract ideas will remain the same. Similarly, when writing a research\\npaper or essay on a specific topic, humans usually start by preparing an outline that structures the\\nwhole document into sections, which they then refine iteratively. Humans also detect and remember\\ndependencies between the different parts of a longer document at an abstract level. If we expand\\non our previous research writing example, keeping track of dependencies means that we need to\\nprovide results for each of the experiment mentioned in the introduction. Finally, when processing\\nand analyzing information, humans rarely consider every single word in a large document. Instead,\\nwe use a hierarchical approach: we remember which part of a long document we should search to\\nfind a specific piece of information.\\nTo the best of our knowledge, this explicit hierarchical structure of information processing and\\ngeneration, at an abstract level, independent of any instantiation in a particular language or modality,\\ncannot be found in any of the currentLLMs. In this work, we present a new approach which\\nmoves away from processing at the token level and closer to (hierarchical) reasoning in an abstract\\nembedding space. This abstract embedding space is designed to be independent of the language or\\nmodality in which the content is expressed; in other words, we aim to model the underlying reasoning\\nprocess at a purely semantic level, not its instantiation in a specific language. In order to verify our\\napproach, we limit our study to two levels of abstraction: subword tokens andconcepts. We define a\\nconcept as an abstract atomic idea. In practice, a concept would often correspond to a sentence in a\\ntext document, or an equivalent speech utterance. We posit that a sentence is an appropriate unit to\\nachieve language independence, in opposition to single words. This is in sharp contrast to current\\nLLMs techniques which are heavily English centric and token based.\\nOur fundamental idea could be based on any fixed-size sentence embedding space for which an\\nencoder and decoder are available. In particular, we could aim to train a new embedding space\\nspecifically optimized to our reasoning architecture. In this work, we chose an existing and freely\\navailable sentence embedding, namedSONAR (Duquenne et al., 2023b).SONAR supports text\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='Figure 1 - Left: visualization of reasoning in an embedding space of concepts (task of summarization).\\nRight: fundamental architecture of anLarge Concept Model(LCM).\\n⋆: concept encoder and decoder are frozen.\\ninput and output in 200 languages, speech input in76 languages, and speech output in English. We\\ndiscuss the constraints and impact of this choice in Section 2.1, and share some ideas on alternative\\nembedding spaces in Section 6.\\nFigure 1-left visualizes reasoning in an embedding space with the example of a summarization task,\\nwhich is materialized by a function on the embedding space, mapping five concept representations\\ninto two. Figure 1-right summarizes the overall architecture and processing flow. The input is first\\nsegmented into sentences, and each one is encoded withSONAR to achieve a sequence of concepts,\\ni.e., sentence embeddings. This sequence of concepts is then processed by aLarge Concept Model\\n(LCM) to generate at the output a new sequence of concepts. Finally, the generated concepts are\\ndecoded bySONAR into a sequence of subwords. The encoder and decoder are fixed and are not\\ntrained. It is important to highlight that the unchanged sequence of concepts at the output of\\nthe LCM can be decoded into other languages or modalities without performing again the whole\\nreasoning process. In the same spirit, a particular reasoning operation such as summarization can\\nbe performed in a zero-shot setting on input in any language or modality, since it solely operates\\non concepts. To summarize, theLCM neither has information on the input language or modality\\nnor generates output in a particular language or modality. We explore multiple architectures to\\ntrain theLCM, in particular several variants of diffusion. Finally, we envision an additional level of\\nabstraction beyond concepts which could correspond to a short description of a paragraph or small\\nsection. In Section 4.3 we report initial ideas on how conditioning and predicting such higher-level\\nrepresentations can improve consistency of output generated by anLCM.\\nTo some extent, theLCM architecture resembles theJepa approach (LeCun, 2022) that also aims to\\npredict the representation of the next observation in an embedding space. However, unlikeJepa that\\nplaces more emphasis on learning a representation space in a self-supervised way, theLCM focuses\\non accurate prediction in the existing embedding space.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='The mains characteristics of our genericLarge Concept Modelapproach are as follows:\\n• Reasoning at an abstract language- and modality-agnostic level beyond tokens:\\n– We model the underlying reasoning process, not its instantiation in a particular language.\\n– The LCM can be trained, i.e. acquire knowledge, on all languages and modalities at once,\\npromising scalability in an unbiased way.\\n• Explicit hierarchical structure:\\n– Better readability of long-form output by a human.\\n– Facilitates local interactive edits by a user.\\n• Handling of long context and long-form output:\\n– The complexity of a vanilla transformer model increases quadratically with the sequence length.\\nThis makes handling of large context windows challenging and several techniques have been\\ndeveloped to alleviate this problem,e.g., sparse attention (Child et al., 2019) or LSH attention\\n(Kitaev et al., 2020). OurLCM operates on sequences which are at least an order of magnitude\\nshorter.1\\n• Unparalleled zero-shot generalization:\\n– Independently of the language or modality theLCM is pre-trained and fine-tuned on, it can be\\napplied to any language and modality supported by theSONAR encoders, without the need of\\nadditional data or fine-tuning. We report results for multiple languages in the text modality.\\n• Modularity and extensibility:\\n– Unlike multimodalLLMs that can suffer from modality competition (Aghajanyan et al., 2023;\\nChameleon team, 2024), concept encoders and decoders can be independently developed and\\noptimized without any competition or interference.\\n– New languages or modalities can be easily added for an existing system.\\nThe goal of this paper is to provide a proof of concept of this high-level vision of an alternative\\narchitecture to current best practice in language modeling. In the next section we present the main\\ndesign principles of our models and discuss several variants to build and train aLarge Concept\\nModel. We discuss several designs to implement diffusion approaches with concept embeddings and\\ncarefully study noise scheduling. This section is completed by a compute complexity comparison\\nwith token-basedLLMs. Section 3 is dedicated to the analysis of a larger 7B parameter model. We\\ndiscuss challenges when instruction fine-tuning this model on multiple generative tasks, and provide\\na comparison with existingLLMs of comparable size. The paper concludes with a discussion of\\nrelated work, the current limitations and perspectives of our approach.\\nTo foster research in this area, we make ourLCM training code2 as well as SONAR encoders and\\ndecoders3 for up to 200 languages and multiple modalities freely available.\\n1We assume an average sentence length of 10–20 tokens.\\n2https://github.com/facebookresearch/large_concept_model\\n3https://github.com/facebookresearch/SONAR\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='2 Main Design Principles\\nIn this section, we outline the main design principles of theLCM. We first describe theSONAR\\nembedding space with its encoders and decoders. Then, we discuss details of data preparation,\\nnamely sentence segmentationi.e., how we split long documents into sentences. And finally, we\\ndescribe in details the different versions ofLCMs introduced in this work.\\n2.1 The SONAR embedding space\\nThe motivation of this work is to perform reasoning at a higher conceptual level than tokens. This\\nrequires an embedding space which is highly semantic. We choseSONAR (Duquenne et al., 2023b)\\nsince it achieves best performance on several semantic similarity metrics likexsim or xsim++ (Chen\\net al., 2023b), and it was successfully used in large-scale bitext mining for translation (Seamless\\nCommunication et al., 2023b).\\nThe SONAR text embedding space was trained as an encoder/decoder architecture, with a fixed-size\\nbottleneck instead of cross-attention (see Figure 2). The criterion combines a machine translation\\nobjective for 200 languages into and out of English, denoising auto-encoding and an explicit MSE loss\\nat the embedding bottleneck layer. Once the text embedding space was trained, a teacher-student\\napproach was applied to extend theSONAR space to the speech modality. More details on the\\narchitecture and training procedure can be found in Duquenne et al. (2023b), and detailed speech\\nrecognition and translation results in the appendix of Seamless Communication et al. (2023a).\\nOur LCM operates directly onSONAR concepts embeddings, hence, it can perform reasoning on\\nall supported languages and modalities. Table 1 compares the language coverage of several other\\nLLMs. The LCM supports substantially more languages than other models, in particular many\\nlow-resource languages. In addition to the text modality,SONAR supports 76 languages for speech\\ninput and speech output in English. We have also developed an experimental encoder for American\\nSign language (ASL). All these encoders and decoders are freely available.4 Exact listings of the\\nsupported languages can be found in theSONAR GitHub repository.\\n4https://github.com/facebookresearch/SONAR\\nFigure 2 - Encoder/decoder bottleneck architecture to train theSONAR text embeddings (right part of\\nfigure). Teacher-student approach to extendSONAR to the speech modality (left part).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='Text Speech Image Video\\nModel Input Output Input Output Input Output Input Output\\nGemini 47 47 62 ✓ ✓ ✓ ✓ ✗\\nGPT 85 85 ✓ ✓ ✓ ✓ ? ✗\\nClaude 37 37 ✓ ✓ ✓ ✓ ✗ ✗\\nBloom 46 46 ✗ ✗ ✓ ✓ ✗ ✗\\nLlama 3-400B 8 8 34 ✗ ✓ ✓ ✗ ✗\\nLCM-SONAR 200 200 76 1 ✗ ✗ (ASL) ✗\\nTable 1- Comparison of language and modality coverage for severalLLMs and ourLCM operating on the\\nSONAR embedding space. SONAR has an experimental support for American Sign Language (ASL) which\\nis not used in this paper.\\n2.2 Data preparation\\nTo train and evaluate theLCM, we need to convert raw text datasets into a sequence ofSONAR\\nembeddings, each one corresponding to a sentence. Dealing with large text corpora presents several\\npractical limitations. First, the precise segmentation of a text into sentences can be challenging due\\nto the presence of errors, specific formatting issues or any other sources of noise. This requires us to\\napply robust automatic text segmentation techniques. Second, some sentences (even well formed)\\ncan be very long and complex, which might negatively impact the quality of the encodedSONAR\\nembeddings. This is particularly prevalent for texts in the scientific domain. In the following, we\\ndiscuss strategies for sentence segmentation and how they affect theSONAR encoding.\\nSentence segmentation analysis We have identified two potential sentence segmentation tech-\\nniques; as we are exploring multilingual data, we focus on sentence segmenters with a large language\\ncoverage:\\n1. SpaCy segmenter (SpaCy) (Honnibal et al., 2020) is a well established multilingual NLP toolkit\\nthat provides a rule-based approach to sentence segmentation.SpaCy is thoroughly tested for\\nhigh-resource languages.\\n2. Segment any Text (SaT) (Minixhofer et al., 2023; Frohmann et al., 2024) offers a suite of\\nmodels and adapters that predict sentence boundaries at the token level.SaT is designed\\nto be resilient to perturbations, particularly avoiding the over-reliance on punctuation and\\ncapitalization. This is valuable in domains where these conventional markers are often missing.\\nThe quality ofSaT’s segmentation is however dependent on the choice of an “appropriate” split\\nprobability threshold.\\nWe additionally customize both methods by incorporating a maximum sentence length cap in\\ncharacters. We refer to these extensions bySpaCy Capped and SaT Capped. Long sentences\\nare broken down into smaller, logically coherent fragments using a rule-based approach based on\\npunctuation marks forSpaCy. ForSaT, we leverage the provided splitting probability estimates to\\nidentify the next best potential split.\\nTo measure the efficacy of a given segmenter, we evaluate the quality of the reconstructed sentences\\nwith AutoBLEU. It is defined as aBLEU score (Papineni et al., 2002) comparing the decoded text\\nfrom aSONAR vector after encoding a segment, to the the reference segment. A good segmentation\\nwill yield segments that can be encoded and then decoded without loss of signal, and thus score a\\nhigher AutoBLEU.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='For this analysis, we sample 10k documents from our pretraining datasets, representing approximately\\n500k sentences. The documents are processed with each segmenter, the sentences are encoded then\\ndecoded and theAutoBLEU score is calculated. We stratified the results based on the lengths of\\nthe original sentences.\\n0 100 200 300 400 500\\nSentence Size in Characters\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\nAverage Auto-BLEU Score\\nSAT\\nSpaCy\\n0 25 50 75 100 125 150 175 200\\nSentence Size in Characters\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\nSAT Capped\\nSpaCy Capped\\nFigure 3 - Segmenters quality.Average Auto-BLEU scores for different sentence segmentation methods\\ndepending on sentence length, for both out of the box (left) and capped implementations (right).\\nAs illustrated in Figure 3 and with a capping at 200 characters, theSaT Capped method demon-\\nstrates a slight but consistent advantage overSpaCy Capped. Both out-of-the-box segmenters,\\nhowever, exhibit significant under-performance across all sentence lengths. This lower performance is\\nespecially pronounced for sentences exceeding 250 characters, underscoring the limitations of using\\nthe segmenters without capping.\\nAccordingly, we prepare theLCM training data withSaT Capped. We discuss in Appendix A\\ntechnical and engineering challenges faced when handling large amounts ofSONAR embeddings.\\n2.3 Large Concept Modelvariants\\nThe design of the LCM is driven by the need to conditionally generate a continuous sentence\\nembedding. This obviously contrasts with how currentLLMs work, i.e., estimating a probability\\ndistribution over a vocabulary of discrete tokens. A straightforward way of solving the task is to\\ntrain a transformer model to generate an embedding with the objective of minimizing the MSE loss\\n(see Section 2.3.1). However, a given context may have many plausible, yet semantically different,\\ncontinuations. The model should thus be able to learn a conditional probability distribution over the\\ncontinuous embedding of the next sentence.\\nThere is a large body of work in computer vision aiming to learn such conditional probability\\ndistributions over continuous data (Dhariwal and Nichol, 2021; Rombach et al., 2021). Models like\\nDall-E 3 (Betker et al., 2023) or Imagen Video (Ho et al., 2022) use a diffusion process to generate an\\nimage or video from a text prompt. Many different real images may satisfy the same input prompt,\\nhence the model has to learn a probability distribution over continuous pixel data. This motivates\\nthe exploration of diffusion models for sentence embedding generation. Two variants are presented in\\nSections 2.3.3 and 2.3.4. Another prevalent take on continuous data generation consists of quantizing\\nsaid data to ultimately model with discrete units; we exploreLCM modeling with quantization in\\nSection 2.3.5.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='xn ˆxn\\nˆxn\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\nPreNet\\nFeature normalizer\\nLinear\\nRdSONAR → Rdmodel\\nPostNet\\nLinear\\nRdSONAR → Rdmodel\\nFeature normalizer\\nFigure 4- TheBase-LCM.Illustration of theBase-LCM. At its core is a standard decoder-only Transformer\\nsurrounded with aPreNet and aPostNet.\\n2.3.1Base-LCM\\nOur baseline architecture for next-concept prediction is a standard decoder-only Transformer that\\ntransduces a sequence of preceding concepts (read sentence embeddings) into a sequence of future\\nones. As illustrated in Figure 4, theBase-LCM is equipped with a “PostNet” and a “PreNet”. The\\nPreNet normalizes the inputSONAR embeddings and maps them to the model’s hidden dimension\\ndmodel.\\nPreNet(x) = normalize(x)Wt\\npre + bpre, (1)\\nPostNet(x) = denormalize\\n\\x00\\nxWt\\npost + bpost\\n\\x01\\n, (2)\\n(3)\\nwhere Wpost ∈ RdSONAR×dmodel , bpost ∈ RdSONAR, Wpre ∈ Rdmodel×dSONAR and bpre ∈ Rdmodel .\\nIn order to learn the maps “normalize” and its inverse “denormalize” we fit a robust scaler to a set\\nof randomly sampledSONAR vectors from different corpora and domains of text data. This scaler\\nremoves the median statistics and scales the data according to the interquartile range (IQR).\\nnormalize(x) =x − µ\\nσ , denormalize(x) =µ + σx. (4)\\nThe Base-LCM is trained on the semi-supervised task of next concept prediction, that is, the model\\npredicts the next conceptˆxn and its parametersθ are optimized to regress the ground truth next\\nconcept (xn).\\nˆxn = f(x<n; θ), MSE(ˆxn, xn) =∥ˆxn − xn∥2. (5)\\nGiven a data distribution q of documents (sequences of concepts), the training loss is evaluated as:\\nLBase-LCM(θ) =Ex∼q\\nh |x|X\\nn=1\\nMSE (f(x<n; θ), xn)\\ni\\n. (6)\\nIn order to enable the generation of variable length documents at inference time, we suffix training\\ndocuments with the sentence “End of text.”. Similar to any sentence in the document, this special\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='suffix will be encoded withSONAR. This means thatx|x| = − →eot := encode(\"End of text.\"). During\\ninference, we implement two main early stopping mechanisms: the first one measures the similarity\\nof the generated embeddingˆxn to − →eot and stops if the cosine similarity exceeds a thresholdseot. The\\nsecond mechanism compares the newly generated embeddingˆxn to the previous generationˆxn−1 and\\nstops if their cosine similarity is higher than a thresholdsprev. We set bothseot and sprev to 0.9.\\n2.3.2 Diffusion-based LCM\\nDiffusion-based LCMs are generative latent variable models that learn a model distributionpθ\\napproximating a data distributionq. Similar to theBase-LCM, we model the diffusionLCMs as\\nauto-regressive models that generate concepts in a document, one at a time. The model distribution\\nis thus expressed at each positionn of the sequence aspθ(xn|x<n) i.e., the generation of the next\\nconcept is conditioned on the preceding context.\\nIn what follows we use a superscript for the denoising/diffusion step (t ∈ [0, 1]) and a subscript (n)\\nfor indexing the sequence of concepts. We simplify for a givenn the conditional model distribution\\npθ(x0\\nn|x0\\n<n) as pθ(x0), and the conditional data distribution q(x0\\nn|x0\\n<n) as q(x0).\\nDiffusion models involve two processes: aforward noising process and areverse denoising process (Ho\\net al., 2020; Song et al., 2020):\\nForward process and noise schedule The forward process is a Gaussian diffusion process\\ncharacterized by the marginal distribution q(xt|x0), given for every timestept ∈ [0, 1] as:\\nq(xt|x0) := N(αtx0, σ2\\nt I). (7)\\nWith the reparameterization trick, we can sample from this marginal distribution via:\\nxt = αtx0 + σtϵ where ϵ ∼ N(0, I) (8)\\nWe use a variance-preserving forward process (Karras et al., 2022) for which we have:\\nα2\\nt = sigmoid(λt), σ 2\\nt = sigmoid(−λt) = 1− sigmoid(λt), λ t = log\\n\\x00\\nα2\\nt /σ2\\nt\\n\\x01\\n, (9)\\nwhere λt is the log signal-to-noise ratio (log-SNR) for timestept.\\nThe noise schedule is a strictly monotonically decreasing functionfλ that maps from the timestep\\nt ∈ [0, 1] to a log-SNR level:λt = fλ(t).\\nIt is common in previous work to also define the noise schedule based on a discrete variance schedule\\n(β0, . . . , βT ). This stems from the formulation of the forward process as a discrete-time Markov chain\\nthat gradually adds Gaussian noise to the data according to said variance schedule:\\nq(x1...T|x0) :=\\nTY\\nt=1\\nq(xt|xt−1), q(xt|xt−1) := N(xt;\\np\\n1 − βtxt−1, βtI), (10)\\nwhere to simplify the notation,xt is short forxt/T as the timesteps are now discretized.\\nFrom the variance schedule(βt)t, the noise schedule can be expressed as:\\nα2\\nt =\\ntY\\ns=1\\n(1 − βs). (11)\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='Following Kingma and Gao (2024), for any given noise schedule, we visualize the distribution over\\nnoise levelsp(λ) =−dt/dλ in order to characterize how much time we are spending at every noise\\nlevel during training.\\nIn this work, we consider three types of noise schedules:\\nCosine. The schedule formulated in Nichol and Dhariwal (2021) as:\\nα2\\nt = f(t)/f(0), where f(t) = cos2\\n\\x12t + s\\n1 +s.π\\n2\\n\\x13\\n, where s = 0.008. (12)\\nQuadratic. The schedule introduced in Ho et al. (2020) where the variances(βt)t are set to constants\\nincreasing quadratically fromβ0 to β1.\\nβt/T =\\n\\x12p\\nβ0 + t\\nT.\\n\\x10p\\nβ1 −\\np\\nβ0\\n\\x11\\x132\\n. (13)\\nSigmoid. We introduce in this work, thesigmoid schedule as a means to study the impact of the SNR\\ndistribution on the training of our models. The schedule is parametrized by two hyper-parameters\\n(γ, δ) and is defined as:\\nα2\\nt = f(t)/f(0), where f(t) = sigmoid (δ − γ logit(t)) , (14)\\nwhere “sigmoid” is the sigmoid functionsigmoid x 7→ ex/(ex + 1)and “logit” its inverse function\\nlogit : x 7→ log(x/(1 − x)). The hyper-parameterγ controls the scale of the log-SNR distribution\\np(λ) and δ its center (see Figure 5).\\nIn all our experiments, we follow Lin et al. (2024) and rescale the variance schedule(β1, . . . βT ) to\\nenforce zero terminal SNRi.e., βT = 1.\\nReverse process and objective function The joint distribution of the diffusion modelpθ(x0...1)\\nis called the reverse process and is defined as a Markov chain with learned Gaussian transitions\\nstarting at p(x1) =N(0, I). In its discretized form:\\npθ(x0:T) := p(xT)\\nTY\\nt=1\\npθ(xt−1|xt), pθ(xt−1|xt) := N(xt−1; µθ(xt, t), Σθ(xt, t)), (15)\\nwhere µθ and Σ are predicted statistics.Σ is set to to the constantσ2\\nt I (matching the transitions\\nof the forward process). µθ can be decomposed into a linear combination ofxt−1 and a noise\\napproximation modelϵθ. This prediction method is dubbedϵ-prediction (Ho et al., 2020; Nichol\\nand Dhariwal, 2021; Nichol et al., 2022). In this work we adoptx0-prediction i.e., we predict the\\nnoiseless state and optimize the simple reconstruction loss:\\nL(θ) := Et∼U(0,1)\\n\\x02\\nω(t)L(t, θ)\\n\\x03\\n, L(t, θ) := Ex0,ϵ\\nh\\r\\rx0 − µθ(αtx0 + σtϵ, t)\\n\\r\\r2\\n2\\ni\\n. (16)\\nDifferent weighting strategies for the reconstruction loss were proposed in the literature (Ho et al.,\\n2020; Salimans and Ho, 2022; Hang et al., 2023). In this work, we default to the simple reconstruction\\nloss (ω(t) = 1, ∀t) and we experiment with a clamped-SNR weighting strategy:\\nω(t) = max(min(exp(λt), λmax), λmin), λt = log(α2\\nt /σ2\\nt ), (17)\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='0.00 0.25 0.50 0.75 1.00\\nt\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nαt\\n0.00 0.25 0.50 0.75 1.00\\nt\\n7.5\\n5.0\\n2.5\\n0.0\\n2.5\\n5.0\\n7.5\\nλt = logSNRt\\n10\\n 5\\n 0 5\\nλt = logSNRt\\n0.000\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\n0.150\\n0.175\\np(λ)\\nCosine\\nSigmoid(1.5, -2)\\nSigmoid(1.5, -1)\\n0.00 0.25 0.50 0.75 1.00\\nt\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nαt\\n0.00 0.25 0.50 0.75 1.00\\nt\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\nλt = logSNRt\\n10\\n 0 10\\nλt = logSNRt\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(λ)\\nCosine\\nSigmoid(0.8, -1)\\nSigmoid(3.5, 0)\\n0.00 0.25 0.50 0.75 1.00\\nt\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nαt\\n0.00 0.25 0.50 0.75 1.00\\nt\\n10.0\\n7.5\\n5.0\\n2.5\\n0.0\\n2.5\\n5.0\\n7.5\\nλt = logSNRt\\n10\\n 5\\n 0 5\\nλt = logSNRt\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\np(λ)\\nCosine\\nQuadaratic-1\\nQuadratic-2\\nFigure 5 - Noise schedules.Illustrations of the different noise schedules explored in this work. Our default\\nschedule being cosine. Quadratic-1 is characterized by(β0 = 0.001, βT = 0.0012) and Quadratic-2 by\\n(β0 = 0.02, βT = 0.022) For each schedule we visualize the curve of(αt)t (see Equation (8)), the curve of the\\nlog-SNR and the associated distribution over noises levelsp(λ) (Kingma and Gao, 2024).\\nwhich is a generalization of Salimans and Ho (2022)’s truncated-SNR weighting and Hang et al.\\n(2023)’s min-SNR strategy where the SNR is clamped between a min- and max-valueλmin and λmax.\\nAdditionally, we consider a weighting strategy that factors in the quality of the samplex0. We use\\nas sample weight a scalarω(x0) ∈ [0, 1] correlated with the sample’s fragility scorei.e., how easy\\nit is to reconstruct a noised sample (see Section 2.5.2). Fragile samples will be assigned a smaller\\nweight and thus contribute less to the objective function.\\nLfragility(θ) := Et∼U(0,1),x0,ϵ\\nh\\nω(x0)\\n\\r\\rx0 − µθ(αtx0 + σtϵ, t)\\n\\r\\r2\\n2\\ni\\n, (18)\\nω(x0) = sigmoid(a fragility(x0) +b), (19)\\nwhere a <0 and b are hyper-parameters to tune.\\nClassifier-free diffusion guidance for theLCM Classifier-free diffusion guidance (Ho and\\nSalimans, 2022) consists of jointly training a conditional and an unconditional diffusion model. The\\nresulting conditional and unconditional score estimates are combined at inference time to achieve a\\ntrade-off between sample quality and diversity. This combined score is defined as follows:\\n∇x logγ p(x|y) = (1− γ)∇x log p(x) +γ∇x log p(x|y), (20)\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='where y is the conditioning variable, in our case the sequence of preceding embeddings(x1, . . .xn−1)\\nwhen denoisingxn.\\nThe hyper-parameterγ controls the contribution of the conditional score; Forγ = 0, this is equivalent\\nto an unconditional model, and forγ = 1, it is a fully conditional model. In practice for vision\\nmodels, γ is set to a value greater than 1, thus amplifying the signal from the conditioning model.\\nInference At inference time, the reverse process is applied.xT is obtained by sampling a random\\nnoise fromp(xT) =N(0, I), and is then iteratively denoised by taking steps in the direction of the\\nscore function (i.e., the direction in which the log-likelihood increases fastest). Additional noise is\\nadded during the process in order to avoid falling down into modes of the distribution.\\nPractically, we start fromxT ∼ N(0, σ2\\ninitI) and find that the quality of the sampled output is\\nsensitive to the initial noise scaleσinit.\\nAlthough we train the model on a large number of discretized timesteps,e.g., T=100, we only generate\\nwith a smaller number of steps,e.g., S=40, at inference via accelerated generation processes (Song\\net al., 2020). We select the sample steps following the trailing method of Lu et al. (2022) as it is\\nfound to be more efficient for smaller stepsS (Lin et al., 2024). That is we generate along the sampled\\nsteps (τ1, . . . τS) =round(flip(arange(T, 0, −T/S))). During inference, we employ the classifier-free\\nguidance rescaling technique of Lin et al. (2024) proven to alleviate the image over-exposure problem\\nencountered in image synthesis diffusion models as the terminal SNR approaches zero. We denote\\nwith gscale and grescale the guidance scale and guidance rescale factors used at inference.\\nFollowing Ning et al. (2023), we perform Epsilon-scaling at inference time as it is shown to alleviate\\nthe exposure bias problem in diffusion models. In its simplified version, this is a training-free method\\nthat consists of scaling down the over-predicted magnitude of error by a scalarλeps.\\nWe describe in Section 2.3.3 and Section 2.3.4 two variants of diffusionLCM: One-Tower and\\nTwo-Tower.\\nxtn\\n0 0 0 t\\nˆx0\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\n×(s)\\ndenoising\\nsteps\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\nxtn\\nˆx0\\nPreNet\\nSelf-attention⋆\\nCross-attention\\nFeed forward\\nPostNet\\nmodulator\\nt\\n×Ld\\n×(s)\\ndenoising\\nsteps\\nFigure 6- Inference with diffusion-based LCMs.In the left-hand side, an illustration of theOne-Tower LCM\\nand on the right-hand side an illustration of theTwo-Tower LCM.\\n2.3.3One-TowerDiffusionLCM\\nThis model, depicted in the left panel of Figure 6, consists of a single transformer backbone whose\\ntask is to predict the clean next sentence embeddingx0\\nn given a noisy inputxt\\nn, conditioned on\\nprevious clean sentence embeddingsx0\\n<n. During training, self-attention can be dropped with a\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='xt00 x00 xt11 x01 xt22\\n0 0t0 t1 t2\\nˆx00 ˆx01 ˆx02\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\nFigure 7 - Training ofOne-Towerdiffusion LCM. Interleaving the clean and noisy embeddings and sampling\\ndifferent diffusion timesteps allows for efficient training.\\ncertain probability for unconditional training. This enables classifier-free guidance at inference time\\n(see Section 2.3.2 for details).\\nEach input embedding is concatenated with the corresponding diffusion timestep embedding. The\\nlearned position embeddings are added to the input vectors prior to being fed toLCM. The backbone\\nutilizes a causal multi-head self-attention.\\nFor efficient training, the model is trained to predict each and every sentence in a document at once.\\nAs depicted in Figure 7, during the diffusion process, the model attends to the clean sentences in\\nthe context using causal multi-head attention layers. The input is specially prepared by interleaving\\nthe noisy (blue) and clean (light blue) sentence embeddings, and the attention mask is prepared\\naccordingly to only attend to the clean sentence embeddings (gray arrows).\\n2.3.4 Two-TowerDiffusion LCM\\nThis model, depicted in the right panel of Figure 6, separates the encoding of the preceding context\\nfrom the diffusion of the next embedding. A first model, labeledcontextualizer, takes as input the\\ncontext vectorsx<n and encodes them causallyi.e., we apply a decoder-only Transformer with causal\\nself-attention. The outputs of the contextualizer are then fed to a second model dubbeddenoiser,\\nwhich predicts the clean next sentence embeddingx0\\nn by iteratively denoising the latentx1\\nn ∼ N(0, I).\\nThe denoiser consists of a stack of Transformer blocks with cross-attention block to attend over\\nthe encoded context. Both the denoiser and the contextualizer share the same Transformer hidden\\ndimension dmodel. Each block of each Transformer layer in the denoiser (including the cross-attention\\nlayer) is modulated with adaptive layer norm (AdaLN, Perez et al. (2018); Peebles and Xie (2023)).\\nThe AdaLN modulator ofTwo-Tower regresses channel-wise scale (γ), shift (β) and residual gates\\n(α) from the embedding of the current diffusion timestept.\\n[β, γ, α] = SiLU(embed(t))Wt + b, (21)\\ny = x + α Block((1 +γ) x + β), (22)\\nFollowing Peebles and Xie (2023) and Goyal (2017) we initialize each residual block in a Transformer\\nlayer (“Block”) with the identity function via initializingW and b in Equation (21) to zero. The\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='x0 x1 x2\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\ns0 s1 s2\\nxt00 xt11 xt22\\nˆx0n ˆx1n ˆx2n\\nPreNet\\nSelf-attention⋆\\nCross-attention\\nFeed forward\\nPostNet\\nmodulator\\nt0, t1, t2\\n×Ld\\nh4\\nh3\\nh2\\nh1\\nh0\\n0 s0 s1 s2 s3 s4\\n1 1 1 1 1 0\\n1 0 0 0 0 0\\n1 1 1 0 0 0\\n1 1 0 0 0 0\\n1 0 0 0 0 0\\nFigure 8- Training Two-Tower diffusion LCM.On the left panel, aTwo-Tower forward pass in training time\\nin order to denoise multiple embeddings in parallel. On the right side panel a visualization of the denoiser’s\\ncross-attention masks with the red highlighted row signaling a sample dropped to train the denoiser\\nunconditionally. (h1, . . . , h4) denotes the sequence of intermediate representations in the denoiser right before\\nthe cross-attention layer.\\ndiffusion timestept is embedded using a 256-dimensional frequency embedding (Dhariwal and Nichol,\\n2021; Peebles and Xie, 2023) followed by a two-layer MLP withSiLU as activation function. “embed”\\nmaps to the denoiser’s hidden dimensiondmodel. The self-attention layers in the denoiser do only\\nattend to the current positioni.e., we do not attend to the preceding noised context. The self-\\nattention layers were kept for consistency with a standard Transformer block and for the possible\\nextension of denoising multiple vectors at once.\\nTwo-Tower training. At training time, Two-Tower’s parameters are optimized for the\\nnext-sentence prediction task on unsupervised sequences of embeddings. The causal embeddings\\nfrom the contextualizer are shifted by one position in the denoiser and a causal mask is used in its\\ncross-attention layers. A zero vector is prepended to the context vectors to enable the prediction\\nof the first position in the sequence (see Figure 8). To train the model both conditionally and\\nunconditionally in preparation for inference with classifier-free guidance scaling, we drop random\\nrows from the cross-attention mask with a rate ofpcfg and denoise the corresponding positions with\\nonly the zero vector as context.\\n2.3.5 QuantizedLCM\\nTwo major approaches currently stand to deal with continuous data generation in the image or speech\\ngeneration fields: one is diffusion modeling, the other is learning quantization of the data before\\nmodeling on top of these discrete units.\\nIn addition, the text modality remains discrete, and despite dealing with continuous representations\\nin the SONAR space, all possible text sentences (of less than a given number of characters) are a\\ncloud of points rather than a real continuous distribution in the SONAR space. These considerations\\nmotivate the exploration of quantization of SONAR representations and then modeling on these\\ndiscrete units to address the next sentence prediction task. Finally, following such an approach\\nenables the natural use of temperature, top-p or top-k sampling, to control the level of randomness\\nand diversity in the sampling of the next sentence representation.\\nIn this section, we learn residual quantizers for the SONAR space, and then build a Quantized\\nLarge Concept Modelbased on these discrete units. We tried to come up with an architecture\\nas close as the diffusionLCM models, to be able to compare approaches.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='Quantization of SONAR space. We use Residual Vector Quantization (RVQ; Zeghidour et al.\\n(2021)) as a coarse-to-fine quantization technique to discretize SONAR representations. Vector\\nquantization maps continuous input embeddings to the nearest entry in a learnt codebook. RVQ\\niteratively quantize residual errors from previous quantizations using additional codebook for each\\niteration. We use FAISS implementation (Douze et al., 2024) which performs iterative k-means\\nclustering of residuals. We use the Improved Residual Vector Quantization (IRVQ) method from Liu\\net al. (2015), with a beam size of 1 for memory efficiency. We trained the RVQ codebooks on 15\\nmillion English sentences extracted from Common Crawl usingncodebooks = 64number of quantizers\\nwith nunits-per-codebook = 8192units per codebook.\\nOne property of RVQ is that the cumulative sum of centroid embeddings of the first codebooks are\\nan intermediate coarse approximation of input SONAR vectors. In that way, we can report the\\nevolution of auto-encoding BLEU scores with the increasing number of codebooks used to quantize\\nSONAR embeddings, before using the SONAR text decoder to decode quantized embeddings. We\\nnotice in Figure 9 that auto-encoding BLEU consistently improves as the number of codebooks\\nincreases , reaching around 70% of the auto-encoding BLEU score achieved with continuous SONAR\\nembeddings, when using all 64 codebooks.\\n10 20 30 40 50 60\\nNumber of codebooks\\n0\\n20\\n40\\n60\\n80\\nAuto-Encoding BLEU score\\n Base decoder\\nFinetuned decoder\\nUnquantized topline\\nFigure 9 - Auto-encoding BLEU scores on FLORES devtest set, encoding sentences with SONAR encoder,\\nquantizing with a varying number of codebooks, dequantizing and decoding with SONAR decoder.\\nFinetuning the SONAR decoder on quantized representations.We fine-tuned SONAR\\ndecoder on quantized representations to adjust it for the space created by the quantizers on 1.2M\\nEnglishsentences. Tomakethedecodermorerobustagainstresidualrepresentationsfromintermediate\\ncodebooks, we randomly select a codebook numberk ∈\\n\\x022\\n3 · ncodebooks, ncodebooks\\n\\x03\\nduring fine-tuning,\\nwith probabilityp = 0.3, and use the quantized representation with codebooks up tok. Figure 9\\nshows the improvement in auto-encoding performance when the decoder is adapted to quantized\\nrepresentations.\\nQuant-LCM architecture. In the same spirit of diffusionLCM, we aim at coarse-to-fine\\ngeneration ofSONAR embeddings conditioned on left-context sentences. However, we do not follow\\na denoising task as in diffusion modeling, but an iterative generation ofSONAR embeddings based on\\nintermediate quantized representationsinstead. In order to generate aSONAR embedding conditioned\\non left-context sentences, theQuant-LCM model starts with theintermediate representationas\\na vector filled with zeros. We iteratively add to thisintermediate representation the predicted\\nresidual centroid embeddings. In that way, the predictedSONAR embeddings are iteratively refined\\nbased on the growing cumulative sum of centroid embeddings of first codebooks, until all codebooks\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='have been seen. We used theOne-Tower architecture forQuant-LCM experiments even though\\nit could be trained withTwo-Tower architecture too. Compared to the diffusionLCM, noisy\\ninput representations are replaced withintermediate quantized representationsand diffusion timestep\\nembeddings as input are replaced by codebook index embeddings.\\nDiscrete targets. Following previous work on modeling discrete units from residual quantizers\\n(Wang et al., 2023; Rubenstein et al., 2023; Lee et al., 2022), a Quant-LCM can be trained to\\npredict the unit from the next codebook, parameterized with a softmax output layer. For parameter\\nefficiency, we do not usencodebooks · nunits-per-codebook unique indices as discrete targets which would\\nimply ncodebooks · nunits-per-codebook output dimensions, but onlynunits-per-codebook output dimensions\\nwhile inputting the information of the codebook index to the model. At training time, similarly\\nto diffusionLCM training, we randomly sample codebook indexk between 1 andncodebooks, and\\ncompute the cumulative sum of centroid embeddings of the firstk−1 codebooks as input. We use\\nthe unit from codebookk of the target embedding as target index for cross entropy loss computation.\\nAt inference time, we iteratively predict the unit from the next codebook, get the corresponding\\ncentroid embedding and add it to the currentintermediate representationas additional predicted\\nresidual embedding. Finally, we also enable classifier-free guidance on logits at inference time (Gafni\\net al., 2022) by randomly dropping left-context conditioning during training as previously described\\nin Section 2.3.3. This modeling approach with discrete targets is dubbedQuant-LCM-d in the\\nfollowing sections. The improved SONAR decoder for quantized representations is used to bridge\\nthe compression gap coming from SONAR quantization in following ablation studies when using\\nQuant-LCM-d.\\nContinuous targets. We also explored a modeling approach that predicts continuous target\\nSONAR vectors based on left-context sentences and intermediate quantized representation of the\\ntarget vector, minimizing the Mean Squared Error between prediction and target embeddings. At\\ninference time, we can either iteratively add the closest centroid embedding based on the predicted\\nresidual ˆr or sample a centroidci from the following distribution:\\np(ci|ˆr) = e−β·∥ci−ˆr∥2\\nP\\nk e−β·∥ck−ˆr∥2\\n, (23)\\nwhere β is a temperature hyper-parameter. This modeling approach with continuous targets is\\ndenoted withQuant-LCM-c in the following sections.\\n2.4 Ablations\\nIn this section, we delineate the ablations experiments conducted to evaluate the aforementionedLCM\\ndesigns. We compare all the variants ofLCMs introduced above, namely,Base-LCM, One-Tower,\\nTwo-Tower and Quant-LCM.\\n2.4.1 Experimental setup\\nFor our ablation study and for the sake of reproducibility, we pre-train our models on theFineweb-\\nedu dataset (Lozhkov et al., 2024). All models are configured to have approximately 1.6B trainable\\nparameters and are pre-trained on Meta’s Research Super Cluster (RSC, Lee and Sengupta (2022))\\nfor 250k optimization steps spanning 32 A100 GPUs with a total batch size of 229k concepts.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='Models architectures. The Base-LCM has 32 layers and a model dimensiondmodel = 2048\\nwith 16 attention heads. It uses rotary position embeddings (RoPE, Su et al. (2024)), applies\\npre-normalization using RMSNorm (Zhang and Sennrich, 2019), uses the SwiGLU activation func-\\ntion (Shazeer, 2020) and is trained with a dropout rate of p=0.1.\\nThe One-Tower diffusion LCM is made of 32 transformer blocks, each made of a self-attention\\nlayer with 32 attention heads and followed by a feed-forward neural network with inner size 8192.\\nIt has a dimensiondmodel of 2048 and uses learned position embeddings. The noise scheduler is set\\nwith T=100 diffusion timesteps. During training, self-attention is dropped with a probability of 0.15\\nfor unconditional training, enabling classifier-free guidance at inference time.\\nThe Two-Tower diffusion LCM has 5 layers in its contextualizer and 13 layers in its denoiser.\\nSimilar to theBase-LCM, it has 16 attention heads, a model dimensiondmodel = 2048, and uses\\nSwiGLU activations and RMSNorm in both contextualizer and denoiser. The contextualizer uses\\nRoPE for embedding positions whereas the denoiser is without positional embeddings. We use by\\ndefault the cosine noise schedule withT=100 and train with a dropout rate ofp=0.1. For training\\nthe model unconditionally we use a cross-attention mask dropout of rate 0.15 (see Section 2.3.4).\\nThe pre-training documents are wrapped at 128 sentences. Unless otherwise mentioned we decode\\nwith S=40 sample steps with a guidance scalegscale = 3, a guidance rescaling factor ofgrescale = 0.7,\\nan initial noise scaleσinit = 0.6 and epsilon-scaling withλeps = 1.00045.\\nThe Quant-LCM follows exactly the same architecture as theOne-Tower diffusion LCM, except\\nfor Quant-LCM-d which differs only by its output dimension which is set tonunits-per-codebook = 8192\\nfor softmax computation. For single sentence prediction tasks, we usetopk = 1and gscale = 2for\\nQuant-LCM-d and topk = 1with gscale = 3for Quant-LCM-c, while for multi-sentence generation\\ntasks we used temperature of 1,topk = 3, gscale = 1, forQuant-LCM-d and temperature of 0.005,\\ntopk = 5, gscale = 1.5 for Quant-LCM-c, as higher guidance or lower temperature setups led to\\nrepeated generated sentences.\\n#Llama2 Tokens #Sentences Total\\nsentencesDataset #Docs Q1 Q2 Q3 Q1 Q2 Q3\\nROC-stories (dev) 2000 48 57 64 5 5 5 10K\\nROC-stories (test) 1871 50 56 62 5 5 5 9.4K\\nC4 (dev) 1000 136 288 577 6 12 24 20.6K\\nC4 (test) 1000 133 282 599 6 11 25 21.9K\\nWikipedia-en (dev) 1000 146 332 736 5 10 23 21.1K\\nWikipedia-en (test) 1000 147 312 673 5 9 21 19.1K\\nGutenberg (dev) 55 10297 15934 22259 328 530 687 216.2K\\nGutenberg (test) 61 10752 15204 23414 325 457 735 562.2K\\nTable 2- Statistics of the pre-training evaluation datasets.For each subset we report the number of documents,\\nthe total number of sentences and document lengths quartiles in sentences and inLlama2 tokens for\\nreference.\\nPre-training evaluation. Pre-trained token-level language models are typically evaluated with\\nperplexity: a measure of how well each next token is predicted given a teacher-forced (i.e., ground\\ntruth) prefix of the document. In a similar spirit, we evaluate pre-trained LCMs in a teacher-forced\\nmode. But as they cannot produce the probability explicitly, we resort to a custom set of metrics of\\nthe quality of next sentence prediction.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='Each pre-trained model is initially evaluated on the quality of its predicted next sentenceˆxn given\\na ground truth contextx<n. Practically, for a given documentx1:N, we run the LCM inference in\\nteacher-forcing mode and evaluate the following metrics:\\n• L2 distance(ℓ2). Euclidean distance in theSONAR space between the predicted embeddingˆxn\\nand the ground truth continuationxn: ℓ2 := ∥ˆxn − xn∥2.\\n• Round-trip L2 distance(ℓ2-r). Euclidean distance in theSONAR space between the re-encoded\\nsentence generated from the predicted embedding and the ground truth continuationxn,\\nℓ2-r := ∥encode(decode(ˆxn)) − xn∥2.\\nSince an LCM can predict an embedding outside of the distribution of real embeddings (obtained\\nby encoding natural sentences), theSONAR decoder might shift these embeddings to the nearest\\nplausible embeddings subspace. Theℓ2-r metric is introduced to capture the shift in embeddings\\nafter decoding them into text then re-embedding them again in theSONAR space. The more the\\ngenerated embeddings are out-of-distribution, the higher the delta betweenℓ2-r and ℓ2 would be.\\n• Contrastive accuracy(CA). The ratio of embeddings in a batch that are further away (in terms of\\nℓ2) from the predicted embeddingˆxn than the ground truthxn (for eachn, we excludexn and its\\ntwo neighboring ground truth embeddings from the comparison). This metric naturally assigns\\nhigher penalty for largeℓ2 values in the regions with high density of sentence embeddings.\\n• Paraphrasing (PAR). The maximum cosine similarity (CS) between the generated embeddingˆxn\\nand the context embeddingsx<n, normalized by the score of the ground truth sentence. Thus,\\nPAR = max\\nm<n\\nCS(ˆxn, xm)/ max\\nm<n\\nCS(xn, xm). The goal of this metric is to capture if the model is\\nsimply copying or paraphrasing a sentence from the context more (> 1) or less (< 1) than the\\nreference sentence.\\n• Mutual information(MI). This metric of text coherence evaluates the mutual information between\\nthe next predicted sentenceˆsn = decode(ˆxn) and the previousk = 10ground truth sentences by\\ncomputing the difference between the unconditional perplexity ofˆsn and its perplexity conditioned\\non the prompt:\\nMI = 1\\n|ˆsn| (log pLM(ˆsn) − log pLM(ˆsn|sn−k:n−1)) .\\nWe estimate the perplexity with a small language model, GPT-2 (Radford et al., 2019). We\\nprepend a newline symbol toˆsn, so that a probability could be assigned to its first token, and we\\ncompute the average mutual information per-token by normalizing it with|ˆsn|, the length ofˆsn in\\ntokens. When averagingMI over a dataset,|ˆsn| are used as weights.\\nPre-training evaluation data. The pre-training evaluation is performed on sampled subsets from\\nfour corpora covering different domains:ROC-stories (Mostafazadeh et al., 2016),C4 (Raffel et al.,\\n2019), Wikipedia-en (English Wikipedia dump) andGutenberg. We sample two distinct subsets\\n(dev and test) from each corpus, we use the dev split for tuning inference hyper-parameters and\\nreport the results on the test splits. The statistics of the evaluation corpora are presented in Table 2.\\nThe results of the pre-training evaluation are presented in Table 3.\\nFirst, diffusion-basedLCM and Quant-LCM variants have similarℓ2 and ℓ2-r scores despite an\\nimportant difference in their learning objectives. The only model that shows substantially lowerℓ2\\nscore is theBase-LCM. This is expected sinceBase-LCM effectively optimizesℓ2 score during\\ntraining. Yet,ℓ2-r score is not improved compared to other models. This could be explained by the\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='Model ROC-stories C4\\nℓ2 ℓ2-r PAR CA MI ℓ2 ℓ2-r PAR CA MI\\nBase-LCM 0.177 0.237 1.847 72.4% 0.062 0.204 0.261 1.964 69.1% -0.105\\nOne-Tower 0.236 0.236 1.939 80.2% 0.977 0.279 0.273 2.239 77.1% 1.110\\nTwo-Tower 0.233 0.231 2.088 80.6% 1.137 0.265 0.261 2.265 75.4% 1.134\\nQuant-LCM-c 0.236 0.237 1.683 76.0% 0.610 0.279 0.283 2.013 77.2% 0.715\\nQuant-LCM-d 0.240 0.246 1.871 81.1% 0.682 0.270 0.282 1.808 75.0% 0.359\\nModel Wikipedia-en Gutenberg\\nℓ2 ℓ2-r PAR CA MI ℓ2 ℓ2-r PAR CA MI\\nBase-LCM 0.229 0.283 1.770 69.6% 0.071 0.207 0.264 1.780 67.8% -0.184\\nOne-Tower 0.324 0.311 2.087 80.9% 1.202 0.284 0.281 2.051 75.1% 0.725\\nTwo-Tower 0.307 0.297 2.079 78.8% 1.307 0.267 0.267 2.077 73.0% 0.684\\nQuant-LCM-c 0.306 0.317 1.842 79.5% 0.744 0.269 0.281 1.774 72.1% 0.419\\nQuant-LCM-d 0.295 0.311 1.592 76.0% 0.323 0.276 0.290 1.599 72.0% 0.153\\nTable 3- Comparing architectures.Pre-training evaluation results on the four select corpora. For each subset,\\nwe reportℓ2 (L2 distance inSONAR space), ℓ2-r (round-trip L2 distance after decoding and re-encoding the\\ngenerated embeddings),PAR (similarity to preceding embeddings) andCA (contrastive accuracy)\\nfact that when many plausible next sentence continuations are possible,Base-LCM generates their\\naverage in SONAR space (instead of sampling one of plausible modes) which may not correspond to\\nany relevant point inSONAR space. This hypothesis is also highlighted by the poorBase-LCM\\nperformance in term ofCA and MI scores.\\nWe do not notice any significant difference inCA scores between diffusionLCMs and Quant-LCM\\nvariants. MI scores, on the contrary, are consistently higher for diffusion-based models compared\\nto Quant-LCM. At the same time, diffusionLCMs tend to paraphrase more the context in the\\ngenerated embeddings, which also correlates with an increasedMI score. Still, Quant-LCM variants\\nsignificantly outperformBase-LCM on MI metric. Now comparing the different variants,Quant-\\nLCM-c outperforms Quant-LCM-d modeling variant: one hypothesis is that predicting codebook\\nindices with cross-entropy loss is harder thanMSE objective whereQuant-LCM-c can more easily\\nlearn combination of left-context vectors for next sentence embedding.\\nFor diffusionLCMs, we don’t observe any consistent difference betweenOne-Tower and Two-\\nTower when looking across all metrics and datasets. Note that overall, to tackle the next sentence\\nprediction task in the SONAR space, diffusion-based methods give clearly better results compared to\\nall other models.\\nInstruction-tuning evaluation. Subsequently, the pre-trained models are instruction-tuned on\\nthe stories subset ofCosmopedia (Ben Allal et al., 2024) and are evaluated on a held-out subset\\nof Cosmopedia itself. We aim with this finetuning to evaluate the ability of the models to follow\\ninstructions and generate consistent stories.\\nFor the sake of comparison, we trained a small Llama (Touvron et al., 2023) on the same training\\ndata (Fineweb-edu) and finetuned it onCosmopedia. This model has 24 transformer layers, each\\nwith 16 attention heads and a model dimension of 2048 for a total of 1.4B parameters. This model\\nwill be referred to assmaLlama.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='We evaluate the following metrics:\\n• ROUGE-L (R-L). ROUGE-L (F-measure) (Lin, 2004) between the generated and reference stories.\\n• Coherence (Coherence). This reference-free metric is computed with a bidirectional transformer\\nmodel fine-tuned by Jwalapuram et al. (2022) to assign higher scores to positive “natural” documents\\nthan to negative examples with permuted sentences. For reporting, we normalize it with a sigmoid\\n(with a temperature 3.0, empirically set to make the scores of “certainly incoherent” documents\\nclose to 0 and those of “certainly coherent” documents close to 1).\\nModel R-L ↑ Coherence ↑\\nBase-LCM 23.69 0.482\\nOne-Tower 33.40 0.968\\nTwo-Tower 33.64 0.938\\nQuant-LCM-c 30.87 0.847\\nQuant-LCM-d 28.01 0.704\\nsmaLlama 34.88 0.984\\nTable 4- Comparing architectures.Instruction-tuning evaluation results. For each model we score the\\ngenerated stories on the held-out test prompts and reportR-L (ROUGE-L) scores.\\nThe scores in terms ofR-L and Coherence of the finetuning evaluation are presented in Table 4.\\nThose quantitative results are in line with the pretraining evaluation ones. BothR-L and Coherence\\nscores correlate with the model ordering based fromMI scores, mainlyQuant-LCM is outperformed\\nby diffusion-based models, and both outperformBase-LCM by a large margin.\\nWe also note thatsmaLlama outperforms theLCMs on this downstream task on both metrics.\\nIt is well known thatLLMs produce very fluent outputs, that explains the higherRouge-L score.\\nWe also note that theOne-Tower and Two-Tower produce coherent outputs, on par with the\\nsmaLlama outputs.\\n2.4.2 Importance of the diffusion inference hyper-parameters\\nIn this section we study the effect of different inference hyper-parameters on the quality of the\\ngenerated text. To this end, we generate outputs for theC4 test split with theTwo-Tower LCM\\nmodel above, while varying the following hyper-parameters: the guidance scalegscale, the initial\\nnoise scaleσinit, and the number of inference sample stepsS. We score the generations following the\\nsame protocol above and report the results in Figure 10. We note that as we increase the guidance\\nscale, the mutual information between the prefix and the generated suffix increases, and so does\\nparaphrasing as we are paying more attention to the conditioning context variables. In the opposite\\ndirection of the mutual information, theℓ2 distance from the ground truth continuation increases as\\nthe model prefers to stay close to the prefix. Regarding the initial noise scale, we observe that values\\nbetween 0.5 and 0.7 achieve the bestMI score. In particular, the generated individual sentences\\nare usually longer with a higherσinit. The ℓ2 distance on the other hand does not reflect this trend.\\nLastly, we increase the number of inference steps and measure the mutual information (MI) of the\\ngenerated texts. With more inference steps, we can improve the prefix-suffix mutual information, but\\nthere is diminishing returns to increasing the inference cost with little qualitative improvement.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='2 4 6\\nGuidance scale (gs)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nMutual information (MI)\\nMI\\nℓ2\\n0.4 0.6 0.8 1.0\\nInitial noise scale (σinit)\\n0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nMutual information (MI)\\nMI\\nℓ2\\n20 40 60 80\\nInference sample steps (S)\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\nMutual information (MI)\\nλeps=1.0025\\nλeps=1.00045\\nλeps=10.24\\n0.26\\n0.28\\n0.30\\nL2-distance (ℓ2)\\n0.24\\n0.26\\n0.28\\n0.30\\nL2-distance (ℓ2)\\nFigure 10 - Importance of inference hyper-parameters.The first panel shows the quality of the generated\\noutput measured withMI and ℓ2 as we vary the guidance scalegscale with fixedσinit = 0.6 and S = 40. The\\nsecond panel varies the initial noise scaleσinit with fixed guidancegscale = 3and S= 40. The third panel\\nvaries the inference stepsS while holding the guidance scalegscale = 1.5 and σinit = 0.6 fixed. We consider 3\\nvalues forλeps to see the impact of epsilon-scaling in the regime of large inference steps.\\n2.4.3 Studying the noise schedules\\nIn this section, we compare differentTwo-Tower diffusion LCMs trained with different noise\\nschedules, namely:\\nCosine. Our default cosine noise schedule.\\nQuadratic. The first quadratic schedule (Quadratic-1) hasβ0 = 0.001 and βT = 0.0012, whereas\\nthe second quadratic schedule (Quadratic-2) hasβ0 = 0.02 and βT = 0.022.\\nSigmoid. Four different sigmoid schedules with with(α, β) ∈ {(1.5, −1), (1.5, −2), (0.8, −1), (3.5, 0)}.\\nAll schedules are set with the defaultT=100. For the exact description of each noise schedule refer\\nto Section 2.3.2 and Figure 5. We selected the quadratic schedule as a commonly used schedule for\\nreference with two configurations, Quadratic-1 closer to the cosine schedule and Quadratic-2 with\\nmore weight given to lower log-SNR. The selected sigmoid schedules withδ = 1.5 are configured\\nwith γ = −1 and γ = −2, γ = −2 being slightly shifted to the left on the log-SNR distributioni.e.,\\nmore weight to lower log-SNR regimes. We then change theδ parameter of the sigmoid schedule to\\nchoose (δ = 0.8, γ= −1) for a peaked distribution of log-SNR around -1 and (δ = 3.5, γ= 0)for a\\nflat distribution over noise levels.\\nWe follow the experimental setup described in Section 2.4.1 and report the results of the pre-training\\nevaluation in Table 5. Both quadratic schedules achieve a betterMI score while the wide sigmoid\\nschedule (δ, γ) = (3.5, 0) achieves the highest accuracyCA on both C4 and Wikipedia-en. To\\nfurther understand the differences between those schedules we re-evaluate onC4 while varying the\\nguidance scalegscale. The results in Figure 11 confirm that the wide sigmoid schedule (δ = 3.5, γ= 0)\\nhas a much higher contrastive accuracy across the board (rightmost panel) while closely matching\\nthe cosine schedule in terms of mutual information (MI). This schedule being trained on a wider\\nspectrum of log-SNR learns to contrast more than to regress. Contrast this behavior to the peaked\\nsigmoid schedule (δ = 0.8, γ= −1) where the model focuses on regressing to the target (lowerℓ2),\\nakin to aBase-LCM, resulting in a lower contrastive accuracy.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='Model C4 Wikipedia-en\\nℓ2 ℓ2-r PAR CA MI ℓ2 ℓ2-r PAR CA MI\\nCosine 0.265 0.261 2.265 75.4% 1.134 0.307 0.297 2.079 78.8% 1.307\\nQuadratic-1 0.268 0.264 2.341 75.7% 1.252 0.309 0.300 2.202 79.1% 1.409\\nQuadratic-2 0.270 0.265 2.320 76.2% 1.252 0.312 0.303 2.185 79.7% 1.399\\nSigmoid(1.5, -1) 0.257 0.259 2.226 74% 1.083 0.298 0.292 2.110 77% 1.271\\nSigmoid(1.5, -2) 0.277 0.267 2.291 77.2% 1.173 0.321 0.303 2.179 80.3% 1.308\\nSigmoid(0.8, -1) 0.252 0.255 2.053 70.6% 0.936 0.285 0.283 1.883 71.7% 1.127\\nSigmoid(3.5, 0) 0.307 0.265 2.347 80.3% 1.154 0.347 0.303 2.187 83.7% 1.288\\nTable 5- Comparing noise schedules.Results of the pre-training evaluation on two corpora,C4 and\\nWikipedia-en.\\n2 4 6\\nGuidance scale (gs)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\nMutual information (MI)\\nCosine\\nQuadratic-1\\nQuadratic-2\\nSigmoid(3.5)\\nSigmoid(0.8)\\n2 4 6\\nGuidance scale (gs)\\n0.24\\n0.26\\n0.28\\n0.30\\n0.32\\n0.34\\nL2 distance (ℓ2)\\n2 4 6\\nGuidance scale (gs)\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nContrastive accuracy (AC)\\nFigure 11 - Comparing noise schedules.The prefix-suffix mutual information (MI), theℓ2 distance and\\ncontrastive accuracy (CA) scores of evaluatedC4 documents while varying the guidance scale(gscale) under\\ndifferent schedules.\\n2.4.4 Studying the loss weighting strategies\\nIn this section we compare the baselineTwo-Tower diffusion LCM trained with the simplified\\nobjective (i.e., ω(t) = 1, ∀t)) to models trained with the clamped-SNR weighting strategy. We\\nconsider two sets of(λmin, λmax): (λmin, λmax) = (0, 10) and (λmin, λmax) = (0.001, 5). All models in\\nthis section are trained with the cosine noise schedule.\\nFragility as a sample weighing strategyAs introduced in Equation (19), we train in this\\nsection aTwo-Tower LCMmodel with loss terms weighted by the sample’s fragility.\\nω(x0) = sigmoid(a fragility(x0) +b) (24)\\nGiven that estimating the fragility score of each sample as defined in Section 2.5.2 can be very\\ncostly, we resort to training a simple MLP (3-layers) on 50M sampled sentences to approximate these\\nfragility scores. This model is referred to as F. Henceforth, the sample weight is:\\nω(x0) = sigmoid(a F(x) +b) (25)\\nThe two hyper-parameters (a, b) are chosen so that extremely fragile sentences contribute less to\\nthe loss (ω(x0) ≈ 0), and so that sample weights should increase smoothly as sample robustness\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='improves. Figure 12 plots the sample weights distribution evaluated on a pre-training dataset with\\na = −4 and b = 3.5.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nSample Weight\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDensity\\nFigure 12 - Resulting distribution of fragility sample weightsω(x0) with ω(x0) = sigmoid(−4 F(x) + 3.5).\\nWe follow the experimental setup described in Section 2.4.1 and report the results of the pre-training\\nevaluation in Table 6. We observe that weighting with clamped SNRs does not improve the quality\\nof generated texts as measured with our pre-training evaluation metrics. When it comes to the\\nfragility-aware weighting strategy, we observe an improvement in the contrastive accuracy of the\\nmodel. In the remainder of this work we default to the simplified training objective (ω(t) = 1, ∀t).\\nModel C4 Wikipedia-en\\nℓ2 ℓ2-r PAR CA MI ℓ2 ℓ2-r PAR CA MI\\nBaseline ω(t) = 1 0.265 0.261 2.265 75.4% 1.134 0.307 0.297 2.079 78.8% 1.307\\nSNR (0,10) 0.280 0.264 2.334 74.8% 1.107 0.320 0.296 2.102 77.9% 1.212\\nSNR (0.001,5) 0.266 0.261 2.269 73.4% 1.094 0.304 0.291 2.007 76.6% 1.295\\nFragility 0.2815 0.273 2.306 76.5% 1.103 0.321 0.308 2.118 80.7% 1.193\\nTable 6- Comparing weighting strategies.Results of the pre-training evaluation on two corpora,C4 and\\nWikipedia-en.\\n2.5 Analysis\\n2.5.1 Inference efficiency ofLCMs\\nWe compare in this section the inference computational cost of theTwo-Tower LCMto that of a\\nvanilla LLM as a function of the total length in tokens of the prompt and output combined. We chose\\nthe theoretical number of FLOPs, independent of any specific optimization. These optimizations are\\ngeneric to the transformer architecture and also apply to ourLCM. We include in this comparison\\ntwo configurations ofLCMs; the 1.6B used in the previous ablation studies and a 7B model we\\nscale up to in the following sections. For bothLCMs, we estimate the inference cost with inference\\nsample stepsS = 40. Given the quadratic complexity of the attention mechanism in transformers, the\\ncomplexity sharply increases with the context size (see upper right corner of Figure 13’s left panel).\\nThe complexity of theLCM depends on how the context is sentencized: a context length of 200\\ntokens split into 10 sentences (20 tokens each) will incur a higher cost than the same 200 tokens split\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='0 10000 20000 30000 40000 50000 60000\\nTotal context size in tokens\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\nInference Flops\\n×1015\\nLLama2-7b\\nTwo-tower LCM (7B)\\nTwo-tower LCM (1.6B)\\n0 500 1000 1500 2000 2500 3000\\nTotal context size in tokens\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n×1014\\nLLama2-7b\\nTwo-tower LCM (7B)\\nTwo-tower LCM (1.6B)\\nFigure 13 - Theoretical inference Flops ofLCMs and LLLms. We evaluate the inference flops for different text\\nlengths (inLlama2 tokens) with a variable average sentence length. Only extremely short sentences (≤ 10\\ntokens) favor LLMs.\\ninto 5 sentences (40 tokens each). We account for this by computing the cost on a range of sentence\\nlengths but report the total context size on the x-axis (context size = sentence length× number\\nof sentences). TheLCM shows substantially better scalability with respect to increasing context\\nsize. The inference computational cost of theLCM includes the three steps of (1) encoding into\\nSONAR, (2)LCM prediction in the sentence space then (3) decoding with aSONAR decoder. The\\ninference cost ofLCMs varies significantly depending on the average length in tokens per sentence.\\nFor extremely short sentences (less than 10 tokens), anLLM is more computationally efficient (see\\nlower left corner of Figure 13’s right panel).\\n2.5.2 Fragility of SONAR space\\nWhen we perform modeling in a latent space, we primarily rely on the induced geometry (L2-distance).\\nHowever, the homogeneous Euclidean geometry of any latent representation will not perfectly match\\nthe underlying text semantics. This is evidenced by the fact that a small perturbation in the\\nembedding space may result in a drastic loss of semantic information after decoding. We dub such\\nembeddings “fragile”. For this reason, we aim to quantify the fragility of semantic embeddings\\n(namely SONAR codes) to understand the quality of theLCM training data and how this fragility\\ncan hinder theLCM training dynamics.\\nGiven a text fragmentw and itsSONAR code x = encode(w), we define the fragility ofw as:\\nfragility(w) := −Eα∼U([0,1]), ϵ∼N(0,I) [score(w, wα,ϵ)] , (26)\\nxα,ϵ = denormalize\\n\\x00√\\n1 − α normalize(x) +√α ϵ\\n\\x01\\n, (27)\\nwα,ϵ = decode(xα,ϵ), (28)\\nwhere normalize and denormalize are the normalization and denormalization operators introduced\\nin Equation (4) with the goal of makingx’s coordinates scale-independent. The “encode“ operation\\nmaps text fragments intoSONAR space, and the “decode“ operation produces a text fragment from\\na given vector in theSONAR space. For eachα in [0, 1], xα,ϵ is the perturbed version ofx where a\\nnoise vector of varianceα is linearly combined withx. The perturbed vector is then decoded into a\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='text fragmentwα,ϵ. This perturbation is similar to the variance-preserving noising used in diffusion\\nLCMs (see Section 2.3.2).\\nThe “score” operator in Equation (26) is set to be a semantic similarity metric comparing the\\nperturbed textwα,ϵ to the originalw. We considered the following options:\\n• Auto-Encoding BLEU. score(w, wα,ϵ) =BLEU(w, wα,ϵ).\\n• External cosine similarity.Provided an external text encoder (read unrelated toSONAR) that\\nencodes a text fragmentw into encodeext(w) score(w, wα,ϵ) =CS(encodeext(w), encodeext(wα,ϵ)),\\nwhere CS is the cosine similarity measure. Compared to Auto-EncodingBLEU, this method is\\ntypically more robust to paraphrasing.\\nFinetuned robust decoder. To serve as a testbed for our fragility analysis, a newSONAR\\ndecoder for English text is finetuned on a sample of our pre-training data. In order to improve the\\ndecoder’s robustness to imperfectly generated embeddings from theLCM, we follow Equation (27)\\nand add random noise vectors toSONAR embeddings during training. As reported in Table 7, the\\nfinetuned SONAR decoder exhibits stronger performance across a range of public corpora.\\nModel Flores CNN DailyMail Gutenberg C4\\nBase SONARdecoder 79.5 75.9 70.5 75.7\\nFinetuned SONARdecoder 88.0 87.6 85.6 87.5\\nTable 7- Comparing SONAR decoders.Raw reconstruction performance of our baseSONAR decoder vs. the\\nnew decoder trained with noised embeddings. Scores are Auto-Encoding BLEU on random subsets of 10k\\nsentences from each dataset, except forFlores where we use the entire dev split.\\nFragility study. We sample 50M random text fragments, and for each sample we generate 9\\nperturbations corresponding to different noise levelsα ∈ [0.1, 0.2, . . . ,0.9]. For the external cosine\\nsimilarity metric we usemGTE as external encoder (Zhang et al., 2024).\\n0 50 100 150 200 250\\nSentence Length in Characters\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\n0.50\\n0.55\\nAuto-Encoding BLEU Score\\n0.0 0.2 0.4 0.6 0.8 1.0\\nα\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nAuto-Encoding BLEU Score\\nBLEU: Base decoder\\nBLEU: Finetuned decoder\\nCosSim: Base decoder\\nCosSim: Finetuned decoder\\n0.72\\n0.74\\n0.76\\n0.78\\n0.80\\n0.82\\n0.84\\n0.86\\nCosine Similarity\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nCosine Similarity\\nBLEU: Base decoder\\nBLEU: Finetuned decoder\\nCosSim: Base decoder\\nCosSim: Finetuned decoder\\nFigure 14 - Fragility scores.Auto-Encoding BLEU and external cosine similarity. In the left-hand panel as a\\nfunction of the text length (α-averaged) and in the right-hand panel as a function of the noise varianceα.\\nWe depict in the right panel of Figure 14 the curves of both score functions with respect to the noise\\nlevel α. We observe thatBLEU scores decrease faster than the cosine similarity. Most importantly,\\nfragility scores are sensitive to the choice of the decoder. In particular, both Auto-Encoding BLEU\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='0.0 0.2 0.4 0.6 0.8 1.0\\nScore value\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDensity\\nScore\\nBLEU: Base decoder\\nBLEU: Finetuned decoder\\nCosSim: Base decoder\\nCosSim: Finetuned decoder\\nFigure 15 - Auto-Encoding BLEU scores and cosine similarity (α-averaged) distributions.\\nand cosine similarity scores decrease at a markedly slower rate for theFinetuned decoder than for\\nthe Base one as the amount of noise increases. We note also that the overall score distribution (after\\naveraging over allα), shown in Figure 15, exhibits a large spread of fragility scores acrossSONAR\\nsamples.\\nOne factor that can explain such a discrepancy is the text length. Compared to the Auto-Encoding\\nBLEU metric (which drops only by 1–2% for long sentences), fragility is more sensitive to the length\\nof sentences and drops faster for both similarity metrics. This shows that using a max sentence\\nlength over250 can be extremely challenging forSONAR and theLCM model. On the other hand,\\neven if short sentences are on average more robust, splitting a long sentence in the wrong place may\\nresult in shorter but more fragile sub-sentences.\\nTaking a closer look at the 5% most fragile embeddings, we notice that they are very noisy. Typically,\\nthey correspond to hyperlinks, references, unique ids, code-switched or numerical entries. These are\\nlikely artifacts that theSONAR models were not exposed to during training or where theSONAR\\ntokenizer fails. Fragility can thus be used to filter out hard samples from the training data. We also\\nobserve that short but complex technical phrases can be more fragile than common language phrases\\nof similar length.\\n3 Scaling the model to 7B\\nThis section describes our effort to scale our model to 7B parameters and compare the performance\\nagainst other approaches such as token-based LLMs on more challenging tasks such as summarization\\nand summarization expansion (detailed in Section 3.1).\\nBased on the results in Section 2.4 where the two diffusion-basedLCMs (One-Tower and Two-\\nTower) outperform the other variants, we decided to scale a diffusion model to 7B parameters. We\\nchose to scaleTwo-Tower given its smaller memory footprint, particularly when processing long\\ncontexts with a shallower contextualizer tower\\nThe large 7BTwo-Tower diffusion LCM has 5 layers in its contextualizer and 14 layers in its\\ndenoiser. Its dimension has been extended todmodel=4096. Each self-attention layer has 32 attention\\nheads. All other parameters are kept the same as for the 1.6BTwo-Tower model. The model is\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='pre-trained on a dataset of 2.3B documents, representing 2.7T tokens and 142.4B concepts/sentences.\\nWe pre-trained this model on Meta’s RSC for 124k optimization steps spanning 256 A100 GPUs\\nwith a total batch size of 1M concepts. We further extend the context length of this model to cover\\n2048 concepts instead of the 128 concepts in the ablation experiments. We trained using the AdamW\\noptimizer with(β1, β2) = (0.9, 0.95), ϵ = 1e-5 and a weight decay of 0.1. We use a cosine learning\\nrate schedule, with warm-up of 10,000 steps up toLR = 3e-4. To improve training stability we clip\\ngradients at a maximum norm ofg = 10. We subsequently finetune the 7BTwo-Tower LCMon\\npublicly available instruction tuning datasets following Chung et al. (2024). Each sample consists of\\na prompt and an answer and we back-propagate on answer sentences only. Each answer sequence\\nis suffixed with the phrase “End of response.” to teach the model when to stop generating. The\\nfinetuning data totals 389M sentences, of which 53M are answers (i.e., targets). For supervised\\nfinetuning, we use a cosine learning rate schedule with an initial rate ofLR = 3e-5 and finetune the\\nmodel for 7 epochs with a batch size of 262K sentences (prompts and answers combined). We will\\nrefer to the pre-trained model asTwo-Tower-7B and the finetuned model asTwo-Tower-7B-IT.\\n3.1 Evaluation Tasks and Data\\nThis section describes the tasks on which we are evaluating and benchmarking our proposed model.\\nWe detail datasets, baselines and metrics. For each task, the dataset was processed with the same\\nsentence splitter andSONAR encoder as used in theLCM training.\\n3.1.1 Metrics\\nAs longform text generation is the main challenge forLCM, our benchmarking is mainly focused\\non generative tasks, which are notoriously more difficult to evaluate automatically. Therefore,\\nwe evaluate them with multiple automatic metrics, chosen to focus on complementary aspects on\\ngeneration quality. All metrics used in this section are summarized in Table 8.\\nFor summarization and summary expansion (defined below), we report the traditional reference-based\\nRouge-L metric (Lin, 2004). As summarization models have a tendency to copy content from the\\nsource or from its own generated prefix, we report two additional word-based metrics. To evaluate\\nhow much content is directly copied from the source, we report the proportion of word 3-grams of\\nthe source that are present in the output (OVL-3). To evaluate repetitiveness of the generated texts,\\nwe report the portion of duplicated word 4-grams in the output (REP-4).\\nTo complement word-based metrics with summarization-focused neural evaluation, we use two metrics\\nintroduced by Clark et al. (2023): average probabilities of the SEAHORSE classifiers for Q4 (whether\\nall the information in the summary is fully attributable to the source), denoted asSH-4 in the\\nfollowing and Q5 (whether the summary captures the main ideas of the source), denoted asSH-5.\\nAs a metric of the overall fluency of the generated sentences, we report an average probability that\\nthe sentence is linguistically acceptable, as predicted by a classifier trained by Krishna et al. (2020)\\non theCoLA dataset (Warstadt et al., 2019), further referred to asCoLA. To evaluate the local\\ncoherence of the generated text, we report the average cosine similarity between eachn’th and\\nn + 2’th sentence (Parola et al., 2023).\\n3.1.2 Summarization\\nTask and datasets. When considering a relatively long document, a summarization task can be\\ndescribed as the act of generating a much shorter corresponding document that includes the essential\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='Task Area Metric Description Reference\\nSummarization Target similarity R-L ROUGE-L Lin (2004)\\nSource similarity OVL-3 N-grams overlap (N=3)\\nGrammaticality REP-4 Portion of duplicated N-grams (N=4) Welleck et al. (2019)\\nFluency CoLA Sentence fluency classifier score Krishna et al. (2020)\\nAttribution SH-4 Seahorse-Large-Q4 score Clark et al. (2023)\\nSemantic coverage SH-5 Seahorse-Large-Q5 coverage score Clark et al. (2023)\\nSummary\\nExpansion\\nGrammaticality REP-4 (see above) Welleck et al. (2019)\\nFluency CoLA (see above) Krishna et al. (2020)\\nTable 8- Summary of automatic metrics used in different tasks in Section 3.1. Order mostly follows paper’s\\nnarrative.\\ninformation contained in the long document and the same logical structure linking the various pieces\\nof essential information.\\nSummarization techniques can range from more extractive to more abstractive. Extractive techniques\\nattempt to preserve in the summary the same vocabulary as that found in the long document,\\nthereby shortening the long by removing details and superfluous wording. Abstractive techniques, on\\nthe other hand, attempt to produce the summary by rephrasing the essential pieces of information\\nfound in the long document. Our work focuses more on abstractive summarization, as such type of\\nsummarization cannot be performed without some form of understanding and reasoning.\\nWe use theCNN DailyMail(Hermann et al., 2015) andXSum (Narayan et al., 2018) datasets. We\\nalso report results on the challengingLCFO corpus which takes long documents as input, approx.\\n5k words (Costa-jussà et al., 2024). The task is to provide abstractive summaries with lengths\\nrepresenting 20%, 10%, and 5% of the input document. Detailed statistics are provided in Table 9.\\n#Llama2 Tokens #Sentences\\nDataset #Docs Q1 Q2 Q3 Q1 Q2 Q3\\nCNN DailyMail 11.5k 605/61 892/78 1266/97 10/3 14/4 21/4\\nXSum 11.3k 273/25 445/30 735/35 25/1 30/1 35/1\\nLCFO.5% 249 6559/341 7214/378 7916/418 209/12 295/15 527/18\\nLCFO.10% 249 6559/654 7214/718 7916/796 209/22 295/27 527/32\\nLCFO.20% 249 6559/1276 7214/1403 7916/1524 209/41 295/48 527/59\\nTable 9- Statistics of the test split of evaluation benchmarks. For each subset we report the number of\\ndocuments and statistics of document and summary length in terms of sentences andLlama2 tokens. Each\\ntable cell shows “document/summary” length quartiles.\\nBaselines. For CNN DailyMailand XSum, we compare against several baselines of different\\narchitectures (encoder-decoder transformer, decoder-only LLMs) that are known to perform well\\non summarization tasks. For encoder-decoder transformer models, we use T5 (Raffel et al., 2020).\\nFor decoder-only LLMs, we chooseGemma-7B, Llama-3.1-8B andMistral-7B-v0.3. We chose\\nthe published instruction-tuned models to compare with theLCM with the same training regime,\\nand have a similar size (7B). Note that while T5 has much smaller sizes than theLCM, this is\\ncompensated by using models that are fine-tuned explicitly on the target evaluation dataset.\\nSummarization results. Table 10 contains the results of different baselines and ourLCM model for\\nsummarization (CNN DailyMailand XSum). We can notice that theLCM produces competitive\\nRouge-L scores when compared to a specifically tunedLLM (T5-3B) and even surpasses the\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='Model Paradigm CNN DailyMail\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGround truth — 100.00 0.170 0.684 0.850 0.683 0.586\\nT5-3B SFT 37.56 0.174 0.854 0.946 0.773 0.503\\nGemma-7B-IT IFT 31.14 0.245 1.032 0.963 0.740 0.560\\nMistral-7B-v0.3-IT IFT 36.06 0.200 0.780 0.972 0.780 0.676\\nLlama-3.1-8B-IT IFT 34.97 0.248 0.928 0.973 0.763 0.692\\nTwo-Tower-7B-IT IFT 36.47 0.177 0.757 0.767 0.723 0.459\\nModel Paradigm XSum\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGround truth — 100.00 0.108 0.399 0.987 0.352 0.418\\nT5-3B — 17.11 0.221 0.671 0.939 0.680 0.450\\nGemma-7B-IT IFT 18.20 0.177 0.620 0.769 0.546 0.446\\nMistral-7B-v0.3-IT IFT 21.22 0.162 0.480 0.922 0.633 0.621\\nLlama-3.1-8B-IT IFT 20.35 0.186 0.501 0.941 0.687 0.658\\nTwo-Tower-7B-IT IFT 23.71 0.106 0.464 0.683 0.358 0.284\\nTable 10- Performance on theCNN DailyMailand XSum summarization tasks.\\ninstruct-finetuned LLMs. Our model tends to generate more abstractive summaries rather than\\nextractive ones, as shown by the lowerOVL-3 scores. TheLCM produces fewer repetitions compared\\nto LLMs, and more importantly, the repetition rate is closer to the ground truth one. TheLCM\\ngenerates globally less fluent summaries according toCoLA classifier. However, we can remark that\\neven the human generated ground truth gets a lower score compared to theLLM. A similar behavior\\nis observed for the source attribution (SH-4) and semantic coverage (SH-5). This may be explained\\nby model-based metrics that are more biased towardsLLM generated content.\\nLong-context summarization results. Table 11 presents the results for long-context summariza-\\ntion (LCFO.5%, LCFO.10% and LCFO.20%). This is a challenging task for most of the models. For\\nexample, Mistral-7B-v0.3-IT seems to be unable to follow the length instruction of the summary–it\\nalways generates summaries which length is about 50% of the source.Mistral-7B-v0.3-IT also\\nhas the highestSH-4 score, i.e., source attribution. The summaries generated byGemma-7B-IT\\ntend to be longer than requested, whileLlama-3.1-8B-IT generates summaries which length is the\\nclosest to the requested size.\\nThe LCM has only seen a limited amount of long documents in the pretraining and fine-tuning data.\\nNevertheless, it performs well for this task. It outperformsMistral-7B-v0.3-IT and Gemma-7B-IT\\nin the metricRouge-L for the 5 and 10% conditions, and is close toGemma-7B-IT for the 20%\\ncondition. We also observe that the LCM yields high SH-5 scores for all conditions, i.e., the\\nsummaries can be attributed to the source.\\nFinally, we observe thatLlama-3.1-8B-IT performs substantially better than the otherLLMs,\\naccording toRouge-L, while allLLMs have similar performance on theCNN DailyMailand XSum\\nsummarization tasks. This could be explained by training data contamination forLlama-3.1-8B-IT,\\nor by the fact that the other twoLLMs struggle to handle the long input context.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='Method WR LCFO.5%\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGemma-7B-IT 0.107 25.21 0.151 4.711 0.688 0.357 0.174\\nMistral-7B-v0.3-IT 0.512 21.36 0.532 5.997 0.854 0.656 0.296\\nLlama-3.1-8B-IT 0.076 37.67 0.190 2.767 0.931 0.488 0.314\\nTwo-Tower-7B-IT 0.060 26.88 0.162 2.473 0.796 0.628 0.196\\nLCFO.10%\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGemma-7B-IT 0.150 29.25 0.164 6.427 0.667 0.377 0.194\\nMistral-7B-v0.3-IT 0.549 25.00 0.537 6.289 0.848 0.660 0.306\\nLlama-3.1-8B-IT 0.128 42.85 0.243 3.804 0.907 0.486 0.310\\nTwo-Tower-7B-IT 0.089 29.38 0.202 3.00 0.791 0.623 0.183\\nLCFO.20%\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGemma-7B-IT 0.257 33.32 0.201 9.188 0.603 0.425 0.239\\nMistral-7B-v0.3-IT 0.493 28.82 0.527 5.806 0.858 0.658 0.293\\nLlama-3.1-8B-IT 0.179 46.92 0.272 4.783 0.888 0.485 0.315\\nTwo-Tower-7B-IT 0.140 31.74 0.253 3.664 0.779 0.613 0.187\\nTable 11- Performance on the long-context summarization task of LCFO. WR is the word count ratio\\nbetween the generated text and the source document.\\n4 Large Concept ModelExtensions\\nIn this section, we explore several extension of theLarge Concept Model. First, we evaluate\\nthe LCM on the new task of summary expansion,i.e., given a summary, create a longer text. We\\nthen showcase the good zero-shot generalization performance of theLCM. Finally, we explore an\\napproach to add higher level information beyond sentences.\\n4.1 Summary Expansion\\nTask and datasets. When considering a short and concise document that has similar properties\\nto those of a summary (i.e., mainly a stand-alone document that abstracts from details), a summary\\nexpansion task can be described as the act of generating a much longer document that preserves\\nthe essential elements found in the corresponding short document, as well as the logical structure\\nthat connects such elements. As this is a more freely generative task, an additional requirement\\nto be taken into consideration is that of coherence (for example, the detailed information included\\nin one generated sentence should not contradict that included in another sentence). The summary\\nexpansion task presented here consists in taking summaries as inputs, fromCNN DailyMailand\\nXSum, and generating a long document. Note that the goal is not to recreate the factual information\\nof the initial document rather than evaluating the capability of the model to extend the input text in\\na meaningful and fluent way. We use similar baselines and metrics as in the previous section 3.1.2.\\nResults. Table 12 shows the results of the summary expansion forCNN DailyMailand XSum.\\nFirst of all, regarding the word count ratio, we can see different behaviours for the two corpora.\\nFor CNN DailyMail, the models tend to generate texts that are 6 times larger than the input.\\nLlama-3.1-8B-IT produces even longer outputs (factor 8 instead of 6). But forXSum, while\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='CNN DailyMail\\nMethod WR R-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑)\\nGemma-7B-IT 6.8 35.54 0.801 2.104 0.951\\nMistral-7B-v0.3-IT 6.4 34.24 0.817 2.063 0.959\\nLlama-3.1-8B-IT 8.5 37.76 0.822 2.582 0.844\\nTwo-Tower-7B-IT 6.3 30.85 0.726 2.911 0.474\\nXSum\\nMethod WR R-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑)\\nGemma-7B-IT 19.5 17.89 0.963 10.238 0.116\\nMistral-7B-v0.3-IT 1.6 29.31 0.893 2.268 0.939\\nLlama-3.1-8B-IT 19.8 28.84 0.915 2.543 0.898\\nTwo-Tower-7B-IT 7.1 23.82 0.561 1.542 0.603\\nTable 12- Performance on the summary expansion tasks ofCNN DailyMailand XSum, evaluated with the\\nmetrics described in Table 8. WR is the word count ratio between the hypothesis and the source summary.\\nGemma-7B-IT and Llama-3.1-8B-IT generate very long texts (almost 20 times longer than the\\nprompt), theLCM generates output of approximately the same length ratio as forCNN DailyMail.\\nOnly Mistral-7B-v0.3-IT fails to generate long outputs for this corpus.\\nThen, we clearly see a different trend compared to the summarization task. TheLLMs get higher\\nRouge-L scores compared to theLCM. As mentioned above, the goal of this task is not to recreate\\nthe original document. However, theR-L score tells us how much of the content of the full document\\ncan be recreated. Contrary to theLLMs, our model tends to generate different sentences compared\\nto the original document from which the summary has been created (with an exception forGemma-\\n7B-IT on XSum). This is expected since our model generates embeddings that are then processed\\nby a decoder trained on a translation task that tends to paraphrase the initial content. However, the\\nCoLA results show that this comes along with lower fluency, especially forCNN DailyMail.\\n4.2 Zero-shot generalization performance\\nSONAR is a semantic space that can represent 200 languages. In this paper, all experiments\\npresented so far have been done on English text. In this section, we explore the capability of our\\nproposed LCM approach to process other languages in a zero-shot fashion by leveragingSONAR’s\\nability to represent multilingual data.\\nWe use theXLSum (Hasan et al., 2021) corpus, a large scale multilingual abstractive news summa-\\nrization benchmark covering 45 languages. We score model outputs using the multilingual rouge\\nscoring scripts released with the benchmark.5 Note thatRouge-L scores are heavily dependent on\\nlanguage-specific text tokenization and stemming. Unless provided in the aforementioned scripts, we\\ntokenize the model outputs and references with the default tokenization from Lin (2004). Languages\\nlike Korean, Telugu and Tamil can benefit from a more appropriate stemming and tokenization.\\nWe compare the LCM performance withLlama-3.1-8B-IT which officially supports eight languages:\\nEnglish, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. According to The Llama3\\nteam (2024), the model has seen many additional languages during pretraining, but was instruction\\n5https://github.com/csebuetnlp/xl-sum/tree/master/multilingual_rouge_scoring\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='Vietnamese\\nEnglish\\nPashto\\nSwahili\\nBurmese\\nFrench\\nHausa\\nUrdu\\nHindi\\nIndonesian\\nPortuguese\\nArabic\\nSpanish\\nWelsh\\nSomali\\nTurkish\\nRussian\\nPersian\\nJapanese\\nIgbo\\nChinese simplified\\nKirundi\\nThai\\nTigrinya\\nSerbian Cyrillic\\nScottish Gaelic\\nChinese traditional\\nYoruba\\nUkrainian\\nAzerbaijani\\nAmharic\\nMarathi\\nTamil\\nSinhala\\nOromo\\nTelugu\\nKorean\\nPunjabi\\nBengali\\nGujarati\\nKyrgyz\\nNepali\\n0\\n10\\n20\\n30\\nXLSUM Rouge-L\\nTwo-Tower-7B-IT Llama-3.1-8B-IT Officially supported Llama3 language\\nFigure 16 - Rouge-L scores onXLSum for Llama-3.1-8B-IT and Two-Tower-7B-IT.\\nfinetuned on those eight languages only. TheLCM, on the other hand, has never seen any language\\nother than English, and we do not use the EnglishXLSum training data either.\\nWe reportRouge-L scores for 42 languages in Figure 16. Three languages were excluded since they\\nare currently not supported bySONAR: Pidgin, Serbian in Latin script and Uzbek in Cyrillic script.\\nThe LCM substantially outperformsLlama-3.1-8B-IT on English (23.5 compared to 20.7Rouge-L)\\nand on the average over all six languages officially supported by both models and included inXLSum\\n(20.2 versus 19.7Rouge-L).6 We also observe that theLCM generalizes very well to many other\\nlanguages, in particular low-resource languages like Southern Pashto, Burmese, Hausa or Welsch\\nwhich all haveRouge-L scores greater than 20. Other well performing low-resource languages are\\nSomali, Igbo or Kirundi. Finally, theLCM obtains aRouge-L score of 30.4 on Vietnamese. Overall,\\nthese results highlight the impressive zero-shot generalization performance of theLCM to languages\\nit has never seen.\\n4.3 Exploring explicit planning\\nWhen writing long-form text, it is often practical to first think about how to structure our narrative.\\nIndeed Dijk (1977) states that all texts have an innate macrostructurei.e., a global discourse structure\\nspanning the entirety of the text. In general scientific discourse, one such macrostructure is that of\\nproblem-solving (Heffernan and Teufel, 2022), where an author must first motivate and describe the\\nproblem, before proposing an appropriate solution.\\nComposing such a macrostructure is no easy task. Yet in the realm ofLLMs, there have been many\\nrecent efforts to help guide model generation using similar structures. One such popular approach\\nis that of creating outlines, which provide a high-level overview of the desired narrative (Li et al.,\\n2024). An alternative approach to outlines is creating summaries of future targets, which the model\\ncan then expand upon (Sun et al., 2022).\\nGiven the nature of theLCM operating at the concept level, it naturally creates long-form output.\\nTherefore, it is important to ensure that the model is capable of creating coherent generations given\\nthe multitudes of possibilities for next concept prediction. In order to address this, we envision an\\n6English, French, Hindi, Portuguese, Spanish and Thai.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='explicit capability for planning. Similar to creating summaries, we propose a complementaryplanning\\nmodel which creates a high-level overview of what should be generated next, given the prior context.\\nThe proposed plan could span multiple concepts, such as a paragraph. TheLCM is conditioned on\\nthis plan, before it then generates the subsequent output sequence.\\nOperationally the model predicts auto-regressively a sequence of concepts followed by abreak concept,\\nwhich represents a natural topic cadence such as a paragraph break. Once thebreak concept is\\npredicted, the large planning model (LPM) generates a plan in order to condition theLCM for\\nprediction of the subsequent sequence. The model then continues generation as usual conditioned on\\nboth the prior concepts and the proposed plan. An overview of the approach is shown in Figure 17.\\nFigure 17 - LCM conditioned on both the prior context and a high-level plan for the next sequence.\\nAlthough we envision a separate (but complementary) model for planning, we present here an initial\\nexperiment using a simplified single-model approach where theLCM is trained in a multitask setting\\nto also predict both thebreak concepts and plans. Additionally, instead of the idealized plan spanning\\nmultiple concepts (such as a paragraph), we use a single concept in order to capture what should\\ncome next (i.e. aplan concept). We call this simplified multitask approach a Large Planning Concept\\nModel (LPCM).\\nMethodology\\nIn order to evaluate this single-model approach, we perform an ablation study. As a baseline method,\\nwe train aOne-Tower LCM(cf. Section 2.3.3) without any visibility tobreak or plan concepts.\\nWe then subsequently train aLPCM with the same number of parameters as the baseline. Both\\nmodels were trained on the same data7 for the same number of steps.\\nData preprocessing. In order to representbreak concepts, we begin by first segmenting the data\\ninto paragraphs. Given that most real world datasets are absent of paragraph structure (or it is\\nnot easy to recover), we apply the Segment Any Text (Frohmann et al., 2024) paragraph splitting\\nAPI8. We additionally force each paragraph to be less than 10 sentences, and merge small (e.g. one\\nsentence) consecutive paragraphs together. In order to representplan concepts, we generate synthetic\\nhigh-level topic description for each preceding segmented paragraph using an existing open-sourced\\nLLM, namely Llama-3.1-8B-IT, which offers a good trade-off between the generated topic quality\\nand the generation speed. The system prompt used to generate these topic descriptions is listed in\\nAppendix C. In total we process approximately320M paragraphs with topic descriptions, spanning\\n1.5B segmented concepts (i.e. approximately 30B tokens).\\nMetrics. We focus on coherence as our main measure of evaluation. Previous ablations (cf.\\nSection 2.4) used the coherence metric introduced by Jwalapuram et al. (2022). However, we explore\\n7Compared to previous experiments, we use a different data mixture more favorable for long-form generation.\\n8https://github.com/segment-any-text/wtpsplit\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='here LLM-as-a-judge as an alternative. Specifically, we use Llama-3.1-8B-IT in order to evaluate the\\ncoherence of the generated model outputs, which is prompted to return an overall coherence score\\nbetween [0, 5]. The prompt used is listed in Appendix D. In order to validate this prompt, we evaluate\\nit against a dataset of human judgements introduced by Jwalapuram et al. (2022), and observed it\\nreported an agreement9 with human annotators which improves upon their coherence model. We\\ntherefore choose this metric for our evaluation. To be consistent across both model results, we do\\nnot include the specialbreak or plan concepts generated by theLPCM when calculating coherence\\nscores.\\nLlama-3.1-8B-IT (↑)\\nLPCM 2.82 ± 0.62\\nBaseline 2.74 ± 0.70\\nTable 13- LPCM ablation coherence score results.\\nResults\\nWe provide the results of our ablation experiment in Table 13. Results are reported over a held-out\\nsubset ofCosmopedia (Ben Allal et al., 2024) following instruction fine-tuning, similar to previous\\nablations (cf. Section 2.4). We observe that theLPCM achieves significantly10 higher coherence\\nscores (significance was measured using a paired t-test) than the baselineOne-Tower LCM. This\\nfinding suggests that theLPCM is capable of producing significantly more coherent outputs than\\nthe LCM as a result of the additional structure coming from predicted plan concepts, helping the\\nLPCM produce a more coherent narrative, which is essential for the objective of generating long-form\\noutput.\\n5 Related work\\n5.1 Sentence representations\\nMultilingual sentence representations Learning effective sentence embeddings has been a well\\nstudied subject in recent years. Significant progress has been made in this field, largely due to the\\ncapabilities of transformer-based language models that by learning contextual representations for\\nindividual tokens (Devlin et al., 2018; Conneau et al., 2020), are able to effectively learn the semantics\\nof language. However, such models are not optimal to create sentence representations.\\nFollowing approaches built upon these initial works, and aimed at learning general sentence repre-\\nsentations, leveraging dual encoder architectures (Guo et al., 2018; Reimers and Gurevych, 2019;\\nNi et al., 2021). These architectures encode the source and target into a common embedding space,\\nand use a distance-based metric to create an alignment loss that approximate semantically identical\\nsentences. Such architectures have been extended to leverage multilingual data to create general,\\naligned embedding spaces across languages (Feng et al., 2020; Janeiro et al., 2024; Sturua et al.,\\n2024). Initial approaches leveraged the contrastive loss to align translations across languages (Feng\\net al., 2020; Yang et al., 2019), using only translation data to train. Other architectural changes,\\nnamely using token-level objectives combined with the sentence level objectives, have proven useful\\nto improve the quality of multilingual sentence representations based on translation data only (Li\\n9Krippendorff’s α = 0.48\\n10Significance observed at the 99.9% level.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='et al., 2023; Janeiro et al., 2024). Recent approaches explore using data from other tasks, besides\\ntranslation data, to increase the generality of the sentence representations (Wang et al., 2024b; Mohr\\net al., 2024). Other approaches change their embeddings per task, either with task-specific prompts\\n(Wang et al., 2024b; Su et al., 2022; Lee et al., 2024b) or with task-specific parameters (Sturua et al.,\\n2024).\\nAnother successful line of work to create general purpose, multilingual, sentence representations is to\\nleverage the translation objective.LASER (Artetxe and Schwenk, 2019), andSONAR (Duquenne\\net al., 2023b) leverage an encoder-decoder architecture, with a fixed-size sentence representation\\nbetween the encoder and the decoder, trained with a translation objective.SONAR is initialized\\nfrom the NLLB-200 model (NLLB Team et al., 2022), and covers 200 languages, making it one of the\\nopen-source models with the widest language coverage.SONAR also provides open-source speech\\nencoders aligned to their sentence encoders for 73 languages (Seamless Communication et al., 2023a),\\naligned through a teacher-student approach.SONAR has been used as the basis for several works\\n(Seamless Communication et al., 2023a,b; Chen et al., 2023a), and its speech decoders have been\\nextended to keep the expressiveness of the original speech (Duquenne et al., 2023a).\\nJoint speech/text sentence representations There has been a large body of research on\\nunsupervised representation learning for monolingual (Baevski et al., 2020) and multilingual speech\\n(Babu et al., 2022), with recently w2v-bert (Chung et al., 2021) that combines contrastive learning and\\nmasked language modeling to learn self-supervised representations from speech. Other works explored\\nmultilingual and multimodal (speech/text) pre-training methods, including mSLAM (Bapna et al.,\\n2022). Finally, Duquenne et al. (2021), followed by Khurana et al. (2022), introduced multilingual\\nand multimodal sentence embeddings, extending a pre-existing multilingual text sentence embedding\\nspace to the speech modality with a distillation approach. Duquenne et al. (2022, 2023c) also showed\\nthat it is possible to efficiently decode multilingual speech sentence embeddings with decoders trained\\non text sentence embeddings into different languages, to perform zero-shot speech translation.\\nLLM based sentence representations Several text representation methods have been proposed\\nwhich are based on existingLLMs. Wang et al. (2024a) proposed extracting text embeddings from\\nthe last token of LLMs fine-tuned with instructions on contrastive data. Lee et al. (2024a) improved\\ntext embedding capabilities of fine-tuned LLMs by removing the causal attention mask and applying\\nextra nonlinear layers before pooling the token embeddings. Embeddings as a service are supported\\nby some commercial LLM providers, for example,Mistral-embed.11 Such embeddings proved\\ncompetitive on retrieval benchmarks; however, to the best of our knowledge, their applicability to\\nreconstructing the texts back from the embedding space has not been demonstrated.\\n5.2 Multilingual LLMs\\nMost of the leadingLLMs have been trained on texts in several languages. Table 1 summarizes\\nthe coverage of several of them. Nevertheless, the pretraining data of theseLLMs seems to be\\nmainly English texts. For example, The Llama3 team (2024) mentions that pretraining data contains\\nsignificantly more English texts, requiring continued pre-training with multilingual data, out of which\\n34.6% is translated reasoning data.\\nThere are also several efforts to trainLLMs optimized on specific languages, e.g. LeoLM for\\nGerman,12 Fuano for Italian (Bacciu et al., 2024),ALLaM for Arabic (Bari et al., 2024), and several\\n11https://docs.mistral.ai/capabilities/embeddings/\\n12https://laion.ai/blog/leo-lm/\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='models for Chinese:ErniBot,13 Tongyi Qianwen,14 or ChatGLM (Team GLM et al., 2024).\\nSome adaptations of LLMs to a massive number of languages also exist. LOLA (Srivastava et al., 2024)\\nis a recent mixture-of-experts LLM supporting 160 languages, MALA-500 (Ji et al., 2024) adapts\\nLLaMA2 to 546 languages. However, such models typically face a trade-off between language coverage\\nand other capabilities. For example, the Aya model (Üstün et al., 2024), following instructions in\\n101 languages, was superseded by Aya-23 (Aryabumi et al., 2024) that exchanged some breadth for\\ndepth, focusing on 23 languages only. The LCM architecture, combining a language-agnostic model\\nfor knowledge and reasoning with potentially language-specialized encoders and decoders, is expected\\nto exhibit this trade-off to a lesser extent.\\n5.3 Alternative LLM architectures\\nPredicting the next state in the embedding space is a core idea of the Joint Embedding Predictive\\nArchitecture (Jepa) proposed by LeCun (2022). This idea has been implemented for images (I-JEPA\\nby Assran et al. (2024)) and video (V-JEPA by Bardes et al. (2024)) as a self-supervised approach\\nto learning representations. For language, equivalent models have not yet been explored.\\nSentence embeddings for language modeling. For text completion, Ippolito et al. (2020)\\nproposed a sentence-level language model operating by choosing the next sentence from a finite set\\nof candidates. Their model demonstrated success in selecting appropriate continuations for short\\nstories, but it has not been scaled to longer inputs or to fully generative outputs. Golestani et al.\\n(2021) studied a similar problem in the even more restrictive sentence ordered setting, but with a\\nmore thorough study of architectural choices. The INSET architecture (Huang et al., 2020) solves the\\nsentence infilling task by combining a denoising autoencoder that encodes sentences into fixed-size\\nvectors and decodes them back and a bidirectional transformer that predicts the embedding of a\\nmissing sentence.\\nMarfurt and Henderson (2021) and Cornille et al. (2024) used predicted next sentence embeddings in\\na fully generative setting, for summarization and generic language modeling, respectively. However,\\ntheir architectures considered sentence-level connections only as an addition to the token-level\\nconnections across sentences, not as their replacement.\\nIn a recent work of An et al. (2024), the SentenceVAE architecture performs language modeling on\\nthe sentence level using a sentence encoder to prepare the inputs and a sentence decoder to produce\\nthe outputs. However, its input and output embedding spaces are not tied, so the inference is only\\npossible by decoding each predicted sentence into text and then re-encoding it for adding it to the\\ncontext.\\nLanguage modeling with diffusion. A series of more recent works tried adapting diffusion\\nmodeling, originally developed for continuous data, to the discrete text domain. The PLANNER\\narchitecture (Zhang et al., 2023) consists of a variational autoencoder for paragraphs and a diffusion\\nmodel trained to predict latent autoencoder representations conditional on the textual context or on\\nthe class label. Lovelace et al. (2024) augmented a decoder-only language model with an encoded\\nsemantic proposal of the continuation text, with an easily guidable diffusion model predicting the\\nembedding of the next proposal. A TEncDM model (Shabalin et al., 2024) performs diffusion in the\\nspace of contextual token embeddings which are then decoded non-autoregressively.\\n13http://research.baidu.com/Blog/index-view?id=183\\n14https://www.alibabacloud.com/en/solutions/generative-ai\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='Some applications of diffusion to sequence modeling have targeted the planning capabilities of the\\nsequence models. Semformer (Yin et al., 2024) proposed training transformers language models\\nto plan several steps ahead by including special planning tokens, the representations of which are\\ntrained to be informative about the future tokens. Ye et al. (2024) applied discrete diffusion to\\nlanguage models as an alternative to autoregressive generation, more suitable for tasks that require\\nmulti-step planning. Ubukata et al. (2024) give an overview of applications of diffusion for planning\\ntasks, but most of them are not concerned with the language domain.\\nOverall, while many of the previous works used hidden representations for language modeling or\\nrelated tasks, all of them either relied on token-level inputs or outputs, or were not intented for\\ngenerating texts of arbitrary length. TheLCM seems to be the first fully generative language model\\nimplemented fully in a highly semantic, reconstructable sentence representation space.\\n6 Limitations\\nIn this section we discuss the possible limitations of the presented Large Concept Modeling approach.\\nChoice of the embedding space.The choice and design of the embedding space plays a crucial\\nrole in theLCM modeling approach.\\n• The SONAR embedding space was chosen for its good multilingual and multimodal represen-\\ntations, as well as the availability of a massively multilingual decoder, which achieves excellent\\nresults in both translation and auto-encoding. However, theSONAR model was trained on\\nvery specific training data, namely bitext machine translation data containing rather short\\nsentences. This has several consequences:\\n1. SONAR is trained to sustain a local geometry (sentences with very similar meanings are\\ngeometrically close) with no special guarantees for sentences that are only loosely related.\\nYet, predicting next sentences distribution requires the space to operate well globally.\\n2. SONAR auto-encodes surprisingly well texts containing links, references, or merely\\nnumbers or code data. Yet, such texts tend to be fragile, highlighting a distribution\\nmismatch between theSONAR training data and commonly usedLLM pre-training text\\ncorpora. Therefore, the accurate prediction of the sentences containing such a content\\n(non-negligible inLCM pre-training data) will be hard for anyLCM SONARbased model.\\nFor instance, the factuality of fragile generated sentences may easily be compromised.\\n• Using a frozen encoder represents some interesting trade-offs. Any frozen encoder which is\\nlearned in a different data context, and with no a-priori strong connection toLCM modeling,\\nmay be suboptimal compared to encoders that are learned in an end-to-end fashion (with\\nthe loss coming from the decoder). At the same time, learning an encoder within end-to-end\\ntraining can be challenging and the resulting space is not guaranteed to result in good semantic\\nrepresentations shared across languages and modalities.\\nTraining the concept representation and theLCM end-to-end would also be less data and\\ncompute efficient since all modeling data should be multilingual and -modal, bearing the risk\\nof modality competition.\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='Concept granularity\\n• In this work, the definition of concepts is interpreted at sentence level. However, the manifold of\\npossible next sentences is very wide, attributing a proper probability to each of such sentences\\nis much harder (even with a modeling within the latent space) that to the discrete set of tokens.\\n• In NLP, we encounter sentences of variable length. Combinatorial complexity of possible next\\nsentences grows exponentially with the maximum character length. The choice of granularity\\nfor LCM is not trivial as long sentences (>120 characters) could reasonably be considered as\\nseveral concepts. However, any finer splitting of such sentences does not necessary separate\\nwell these concepts. This shows the limitation of a fixed size embedding representation for one\\nsentence. Text splitting (such as sentence splitting) or one-to-many mapping of a sentence into\\nseveral embeddings is a major future direction of research.\\n• Each document in a training corpus typically contains a sequence of unique sentences or a little\\nnumber of repetitions. This data sparsity effect manifests as well at large corpora level: the\\nlarge majority of sentences are merely unique. In principle, this issue can be addressed with\\nhigher-level semantic embedding representations. These higher-level representations come with\\ntrade-off between requirement of lossless data encoding (think of named-entities or numbers,\\ncritical in many language modeling tasks) and good level of abstraction to enable reasoning\\ncapabilities. Compared to a monolingual auto-encoder which would simply compress input\\nsentences, SONAR offers semantic representations with good auto-encoding quality but still\\ncertainly sacrificing generalization capacities.\\n• This generalization issue can be partially mitigated by splitting or encoding input text as new\\nconceptual units which are more commonly shared across source documents. This is in the spirit\\nof stemming or lemmatization techniques studied in NLP for words. That being said, building\\nsuch conceptual units that are also language and modality agnostic is a challenging task. Such\\nshared multilingual and multimodal conceptual units are also key for generalization across\\nlanguages and across modalities. To maximize cross-lingual and cross-modal transfers,Large\\nConcept Modelsshould be exposed to a richer variety of multilingual and multi-modal data.\\nContinuous versus discrete\\n• Diffusion modeling has proven to be very efficient in generative modeling of continuous data\\nlike images or speech. As previously stated, sentences in theSONAR space, despite being\\nrepresented as continuous vectors, remain discrete combinatorial objects. This makes diffusion\\nmodeling struggle on the text modality (either at word or sentence embedding level).\\n• The contrastive nature of cross-entropy loss based on softmax outputs which is used for next\\ntoken prediction plays a critical role for many downstream task where higher accuracy is\\nrequired (e.g. MCQ tasks, code or math generation). On the opposite, continuous diffusion\\nmodeling does not allow to integrate such a contrastive objective.\\n• The Quant-LCM could be a way to address the discrete nature of text while modeling on\\ncoarse-to-fine semantic units shared across languages and modalities. The limited performance\\nof theQuant-LCM approaches presented in this paper may be explained by the fact that\\nSONAR space was not trained to be efficiently quantizable, yielding a significant number\\nof codebooks and a large amount of units per codebook. Therefore, the currentSONAR\\nquantization suffers from the exponentially increasing number of RVQ units combinations which\\ndoes not solve the data sparsity/uniqueness issue discussed earlier. This indicates once again\\nthe importance of developing a new representation space, either continuous or discrete, for the\\nLarge Concept Model.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='7 Acknowledgments\\nWe would like to thank Robbie Adkins, Can Balioglu, Joy Chen, Pascale Fung, Jason Holland, Amita\\nKamath, Justine Kao, Sagar Miglani, Alice Rakotoarison, Abhilasha Sancheti, Arjang Talattof, Ellen\\nTan, Carleigh Wood, Shireen Yates, Bokai Yu and Luke Zettlemoyer for comments and suggestions\\non this work, as well helping us to improve this paper.\\n8 Conclusion and Future Work\\nCurrent best practice for large scale language modeling is to operate at the token level, i.e. to\\nlearn to predict the next tokens given a sequence of preceding tokens. There is a large body of\\nresearch on improvements ofLLMs, but most works concentrate on incremental changes and do\\nnot question the main underlying architecture. In this paper, we have proposed a new architecture,\\nnamed aLarge Concept Model(LCM), which substantially differs from currentLLMs in two\\naspects: 1) all modeling is performed in a high-dimensional embedding space instead of on a discrete\\ntoken representation; and 2) modeling is not instantiated in a particular language or modality, but\\nat a higher semantic and abstract level. We have named the general form of this representation\\na “concept”.\\nInthispaper, toverifythefeasibilityofthehigh-levelidea, wehaveassumedthataconceptcorresponds\\nto a sentence in the text domain, or an equivalent speech segment, and that the embeddings are\\nobtained by the freely availableSONAR sentence encoder (Duquenne et al., 2023b). With respect to\\nthe specific architecture of theLCM, we have first shown that directly minimizing the MSE loss in\\nthe embedding space does not yield good results. We then explored several architectures based on a\\ndiffusion process: theOne-Tower and Two-Tower LCM, as well as aQuant-LCM which uses\\nquantization of SONAR representations and then modeling on these discrete units. These ablation\\nexperiments were performed with models with 1.6B parameters and focused on the generative task of\\ncontinuing a sequence of sentences. We have then scaled our models to a size of 7B parameters and\\ninstruction-finetuned them on several summarization and summary expansion tasks. We provide a\\ndetailed comparison to other public models of the same size, namelyGemma, Mistral and Llama.\\nBy design, aLCM exhibits strong zero-shot generalization performance. In this paper, we trained\\nmodels on English texts only, and applied them to text in other languages, without any additional\\ntraining data, neither aligned nor unlabeled. TheLCM outperforms Llama-3.1-8B-IT on English\\nand on the average over foreign languages officially supported by theLLM. TheLCM itself could\\nalso be trained on multilingual- and model data to acquire knowledge from these sources. We will\\nexplore this in future versions of theLCM. In short, all languages and modalities are first class\\ncitizens and handled equally at all stages of aLCM.\\nWe have observed that next sentence prediction is substantially more challenging than next token\\nprediction. First, given that we operate in an embedding space and at a higher semantic level, the\\nnumber of possible sentences is virtually unlimited, while token vocabularies are usually in the range\\nof 100k. Second, even given a long context, there is unavoidably more ambiguity in choosing the\\nnext sentence than the next token. And third, the usual softmax output layer over the fixed size\\ntoken vocabulary provides a normalized probability distribution over all possible token continuations.\\nTheoretically, a diffusion process should be able to learn a probability distribution over an output\\nembedding space, but our current experimental evidence indicates that more research is needed to\\ntake full advantage of the properties ofLarge Concept Models. As an example, the ability\\nto sample multiple embeddings and associate a score would enable beam search to find the best\\nsequence of sentences. Finally, small modeling errors could yield predictions in the embedding space\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='which do not correspond to valid sentences, i.e. that cannot be decoded into a syntactically and\\nsemantically correct sentence. We will work on alternative concept embeddings toSONAR which\\nwould be better suited to the next sentence prediction task, and would improve modeling approaches\\nin that concept embedding space.\\nWe see the models and results discussed in this paper as a step towards increasing scientific diversity\\nand a move away from current best practice in large scale language modeling. We acknowledge that\\nthere is still a long path to reach the performance of current flagshipLLMs. This will require of\\ncourse further improving the core architecture, but also careful data selection and curation, extensive\\nablations, optimized and diverse instruction fine-tuning, and finally, scaling to models with more\\nthan 70B parameters.\\nWe open-source the full training code of all ourLCM variants, together with a set of supporting\\nscripts,15 to make it easy for other teams to trainLCM models. By these means, we hope to foster\\nresearch on alternativeLLMs and contribute to advance the field of machine intelligence.\\nReferences\\nA. Aghajanyan, L. Yu, A. Conneau, W.-N. Hsu, K. Hambardzumyan, S. Zhang, S. Roller, N. Goyal, O. Levy,\\nand L. Zettlemoyer. Scaling laws for generative mixed-modal language models. InInternational Conference\\non Machine Learning, pages 265–279. PMLR, 2023.\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, Étienne Goffinet, D. Hesslow,\\nJ. Launay, Q. Malartic, D. Mazzotta, B. Noune, B. Pannier, and G. Penedo. The Falcon series of open\\nlanguage models. ArXiv, abs/2311.16867, 2023. URLhttps://arxiv.org/pdf/2311.16867.\\nH. An, Y. Chen, Z. Sun, and X. Li. SentenceVAE: Enable next-sentence prediction for large language models\\nwith faster speed, higher accuracy and longer context.arXiv preprint arXiv:2408.00655, 2024.\\nAnthropic. The Claude 3 model family: Opus, sonnet, haiku, 2024. URLhttps://www-cdn.anthropic.com/\\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\\nM. Artetxe and H. Schwenk. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer\\nand beyond. TACL, pages 597–610, 2019.\\nV.Aryabumi, J.Dang, D.Talupuru, S.Dash, D.Cairuz, H.Lin, B.Venkitesh, M.Smith, K.Marchisio, S.Ruder,\\net al. Aya 23: Open weight releases to further multilingual progress.arXiv preprint arXiv:2405.15032, 2024.\\nM. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas. Self-supervised\\nlearning from images with a joint-embedding predictive architecture.ArXiv, abs/2301.08243, 2024. URL\\nhttps://arxiv.org/pdf/2301.08243.\\nA. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino,\\nA. Baevski, A. Conneau, and M. Auli. XLS-R: Self-supervised Cross-lingual Speech Representation Learning\\nat Scale. InProc. Interspeech 2022, pages 2278–2282, 2022. doi: 10.21437/Interspeech.2022-143.\\nA. Bacciu, G. Trappolini, A. Santilli, E. Rodolà, and F. Silvestri. Fauno: The italian large language model\\nthat will leave you senza parole!ArXiv, abs/2306.14457, 2024. URLhttps://arxiv.org/pdf/2306.14457.\\nA. Baevski, Y. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: A framework for self-supervised learning of\\nspeech representations.NeurIPS, 33:12449–12460, 2020.\\nC. Balioglu. fairseq2, 2023. URLhttp://github.com/facebookresearch/fairseq2.\\nA. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau. mslam:\\nMassively multilingual joint pre-training for speech and text.arXiv preprint arXiv:2202.01374, 2022.\\n15https://github.com/facebookresearch/large_concept_model\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='A. Bardes, Q. Garrido, J. Ponce, X. Chen, M. Rabbat, Y. LeCun, M. Assran, and N. Ballas. Revisiting\\nfeature prediction for learning visual representations from video.ArXiv, abs/2404.08471, 2024. URL\\nhttps://arxiv.org/pdf/2404.08471.\\nM. S. Bari, Y. Alnumay, N. A. Alzahrani, N. M. Alotaibi, H. A. Alyahya, S. AlRashed, F. A. Mirza, S. Z.\\nAlsubaie, H. A. Alahmed, G. Alabduljabbar, R. Alkhathran, Y. Almushayqih, R. Alnajim, S. Alsubaihi,\\nM. A. Mansour, M. Alrubaian, A. Alammari, Z. Alawami, A. Al-Thubaity, A. Abdelali, J. Kuriakose,\\nA. Abujabal, N. Al-Twairesh, A. Alowisheq, and H. Khan. ALLaM: Large language models for arabic and\\nenglish. ArXiv, abs/2407.15390, 2024. URLhttps://arxiv.org/pdf/2407.15390.\\nL. Ben Allal, A. Lozhkov, G. Penedo, T. Wolf, and L. von Werra. Cosmopedia, 2024. URL https://\\nhuggingface.co/datasets/HuggingFaceTB/cosmopedia.\\nJ. Betker, G. Goh, L. Jing, TimBrooks, J. Wang, L. Li, LongOuyang, JuntangZhuang, JoyceLee, YufeiGuo,\\nWesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao, and A. Ramesh. Improving image generation\\nwith better captions, 2023. URLhttps://api.semanticscholar.org/CorpusID:264403242.\\nBigScience Workshop. BLOOM: a 176b-parameter open-access multilingual language model. ArXiv,\\nabs/2211.05100, 2023. URLhttps://arxiv.org/pdf/2211.05100.\\nChameleon team. Chameleon: Mixed-modal early-fusion foundation models.ArXiv, abs/2405.09818, 2024.\\nURL https://arxiv.org/pdf/2405.09818.\\nM. Chen, P.-A. Duquenne, P. Andrews, J. Kao, A. Mourachko, H. Schwenk, and M. R. Costa-jussà. BLASER:\\nA text-free speech-to-speech translation evaluation metric. In ACL, pages 9064–9079, 2023a. URL\\nhttps://aclanthology.org/2023.acl-long.504.\\nM. Chen, K. Heffernan, O. Çelebi, A. Mourachko, and H. Schwenk. xSIM++: An improved proxy to\\nbitext mining performance for low-resource languages. In ACL, pages 101–109, 2023b. URL https:\\n//aclanthology.org/2023.acl-short.10.\\nR. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers.arXiv\\npreprint arXiv:1904.10509, 2019.\\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma,\\net al. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):1–53,\\n2024.\\nY.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu. W2v-bert: Combining contrastive\\nlearning and masked language modeling for self-supervised speech pre-training. In2021 IEEE Automatic\\nSpeech Recognition and Understanding Workshop (ASRU), pages 244–250. IEEE, 2021.\\nE. Clark, S. Rijhwani, S. Gehrmann, J. Maynez, R. Aharoni, V. Nikolaev, T. Sellam, A. Siddhant, D. Das,\\nand A. Parikh. Seahorse: A multilingual, multifaceted dataset for summarization evaluation. InProceedings\\nof the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9397–9413, 2023.\\nA. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettle-\\nmoyer, and V. Stoyanov. Unsupervised cross-lingual representation learning at scale. InACL, 2020.\\nN. Cornille, M.-F. Moens, and F. Mai. Learning to plan for language modeling from unlabeled data.arXiv\\npreprint arXiv:2404.00614, 2024.\\nM. R. Costa-jussà, P. Andrews, M. C. Megliogli, J. Chen, J. Chuang, D. Dale, C. Ropers, A. Mourachko,\\nE. Sánchez, H. Schwenk, T. Tran, A. Turkatenko, and C. Wood. LCFO: Long context and long form output\\ndataset and benchmarking.ArXiv, 2024.\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers\\nfor language understanding.arXiv preprint arXiv:1810.04805, 2018.\\nP. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis.Advances in neural information\\nprocessing systems, 34:8780–8794, 2021.\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='V. Dijk. Text and Context: Explorations in the Semantics and Pragmatics of Discourse.Longman, 1977.\\nM. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazaré, M. Lomeli, L. Hosseini, and H. Jégou.\\nThe Faiss library.ArXiv, abs/2401.08281, 2024. URLhttps://arxiv.org/pdf/2401.08281.\\nP.-A. Duquenne, H. Gong, and H. Schwenk. Multimodal and multilingual embeddings for large-scale speech\\nmining. In NeurIPS, volume 34, pages 15748–15761, 2021. URLhttps://proceedings.neurips.cc/paper/\\n2021/file/8466f9ace6a9acbe71f75762ffc890f1-Paper.pdf.\\nP.-A. Duquenne, H. Gong, B. Sagot, and H. Schwenk. T-modules: Translation modules for zero-shot cross-\\nmodal machine translation. In EMNLP, pages 5794–5806, 2022. URLhttps://aclanthology.org/2022.\\nemnlp-main.391.pdf.\\nP.-A. Duquenne, K. Heffernan, A. Mourachko, B. Sagot, and H. Schwenk. Sonar expressive: Zero-shot\\nexpressive speech-to-speech translation, 2023a.\\nP.-A. Duquenne, H. Schwenk, and B. Sagot. SONAR: sentence-level multimodal and language-agnostic\\nrepresentations, 2023b. URLhttps://arxiv.org/abs/2308.11466.\\nP.-A. Duquenne, H. Schwenk, and B. Sagot. Modular speech-to-text translation for zero-shot cross-modal\\ntransfer. In Interspeech, 2023c.\\nF. Feng, Y. Yang, D. Cer, N. Arivazhagan, and W. Wang. Language-agnostic BERT sentence embedding.\\narXiv preprint arXiv:2007.01852, 2020.\\nM. Frohmann, I. Sterner, I. Vulić, B. Minixhofer, and M. Schedl. Segment Any Text: A universal approach\\nfor robust, efficient and adaptable sentence segmentation. InEMNLP, pages 11908–11941, 2024. URL\\nhttps://aclanthology.org/2024.emnlp-main.665.\\nO. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene: Scene-based text-to-\\nimage generation with human priors. InEuropean Conference on Computer Vision, pages 89–106. Springer,\\n2022.\\nGemini Team Google. Gemini 1.5 unlocking multimodal understanding across millions of tokens of conte.\\nArXiv, abs/2403.05530, 2024. URLhttps://arxiv.org/pdf/2403.05530.\\nM. Golestani, S. Z. Razavi, Z. Borhanifard, F. Tahmasebian, and H. Faili. Using BERT encoding and\\nsentence-level language model for sentence ordering. InInternational Conference on Text, Speech, and\\nDialogue, pages 318–330. Springer, 2021.\\nP. Goyal. Accurate, large minibatch sg d: training imagenet in 1 hour.arXiv preprint arXiv:1706.02677, 2017.\\nM. Guo, Q. Shen, Y. Yang, H. Ge, D. Cer, G. H. Abrego, K. Stevens, N. Constant, Y.-H. Sung, B. Strope, et al.\\nEffective parallel corpus mining using bilingual sentence embeddings.arXiv preprint arXiv:1807.11906,\\n2018.\\nT. Hang, S. Gu, C. Li, J. Bao, D. Chen, H. Hu, X. Geng, and B. Guo. Efficient diffusion training via min-snr\\nweighting strategy. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages\\n7441–7451, 2023.\\nT. Hasan, A. Bhattacharjee, M. S. Islam, K. Mubasshir, Y.-F. Li, Y.-B. Kang, M. S. Rahman, and R. Shahriyar.\\nXL-sum: Large-scale multilingual abstractive summarization for 44 languages. InFindings of the Association\\nfor Computational Linguistics: ACL-IJCNLP 2021, pages 4693–4703, Online, Aug. 2021. Association for\\nComputational Linguistics. URLhttps://aclanthology.org/2021.findings-acl.413.\\nK. Heffernan and S. Teufel. Problem-solving recognition in scientific text. InProceedings of the Thirteenth\\nLanguage Resources and Evaluation Conference, pages 6045–6058, Marseille, France, June 2022. European\\nLanguage Resources Association. URLhttps://aclanthology.org/2022.lrec-1.650.\\nK. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching\\nmachines to read and comprehend.Advances in neural information processing systems, 28, 2015.\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='J. Ho and T. Salimans. Classifier-free diffusion guidance.arXiv preprint arXiv:2207.12598, 2022.\\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models.CoRR, abs/2006.11239, 2020. URL\\nhttps://arxiv.org/abs/2006.11239.\\nJ. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J.\\nFleet, and T. Salimans. Imagen video: High definition video generation with diffusion models.ArXiv,\\nabs/2210.02303, 2022. URLhttps://arxiv.org/abs/2210.02303.\\nM. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd. spaCy: Industrial-strength Natural Language\\nProcessing in Python, 2020.\\nY. Huang, Y. Zhang, O. Elachqar, and Y. Cheng. INSET: Sentence infilling with inter-sentential transformer.\\nIn ACL, pages 2502–2515, 2020. URLhttps://aclanthology.org/2020.acl-main.226.pdf.\\nD. Ippolito, D. Grangier, D. Eck, and C. Callison-Burch. Toward better storylines with sentence-level language\\nmodels. ArXiv, abs/2005.05255, 2020. URLhttps://arxiv.org/pdf/2005.05255.\\nJ. M. Janeiro, B. Piwowarski, P. Gallinari, and L. Barrault. Mexma: Token-level objectives improve sentence\\nrepresentations, 2024. URLhttps://arxiv.org/abs/2409.12737.\\nS. Ji, Z. Li, I. Paul, J. Paavola, P. Lin, P. Chen, D. O’Brien, H. Luo, H. Schütze, J. Tiedemann, et al. Emma-500:\\nEnhancing massively multilingual adaptation of large language models.arXiv preprint arXiv:2409.17892,\\n2024.\\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas,\\nE. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux,\\nP. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix,\\nand W. E. Sayed. Mixtral.ArXiv, abs/2401.04088, 2024. URLhttps://arxiv.org/pdf/2401.04088.\\nP. Jwalapuram, S. Joty, and X. Lin. Rethinking self-supervision objectives for generalizable coherence modeling.\\nIn ACL, pages 6044–6059, 2022. URLhttps://aclanthology.org/2022.acl-long.418.\\nT. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generative\\nmodels. Advances in neural information processing systems, 35:26565–26577, 2022.\\nS. Khurana, A. Laurent, and J. Glass. Samu-xlsr: Semantically-aligned multimodal utterance-level cross-lingual\\nspeech representation.arXiv preprint arXiv:2205.08180, 2022.\\nD. Kingma and R. Gao. Understanding diffusion objectives as the elbo with simple data augmentation.\\nAdvances in Neural Information Processing Systems, 36, 2024.\\nN. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efficient transformer.arXiv preprint arXiv:2001.04451,\\n2020.\\nK. Krishna, J. Wieting, and M. Iyyer. Reformulating unsupervised style transfer as paraphrase generation. In\\nEmpirical Methods in Natural Language Processing, 2020.\\nY. LeCun. A path towards autonomous machine intelligence, 2022. URLhttps://openreview.net/pdf?id=\\nBZ5a1r-kVsf.\\nC. Lee, R. Roy, M. Xu, J. Raiman, M. Shoeybi, B. Catanzaro, and W. Ping. NV-Embed: Improved techniques\\nfor training LLMs as generalist embedding models.arXiv preprint arXiv:2405.17428, 2024a.\\nD. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523–\\n11532, 2022.\\nJ. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, Y. Luan,\\nS. M. K. Duddu, G. H. Abrego, W. Shi, N. Gupta, A. Kusupati, P. Jain, S. R. Jonnalagadda, M.-W.\\nChang, and I. Naim. Gecko: Versatile text embeddings distilled from large language models, 2024b. URL\\nhttps://arxiv.org/abs/2403.20327.\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='K. Lee and S. Sengupta. Introducing the ai research supercluster — meta’s cutting-edge ai supercomputer for\\nai research, 2022. URLhttps://ai.facebook.com/blog/ai-rsc/.\\nY. Li, Q. Chen, W. Yan, W. Wang, Q. Zhang, and H. Sundaram. Advancing precise outline-conditioned text\\ngeneration with task duality and explicit outline control, 2024. URLhttps://arxiv.org/abs/2305.14459.\\nZ. Li, S. Huang, Z. Zhang, Z.-H. Deng, Q. Lou, H. Huang, J. Jiao, F. Wei, W. Deng, and Q. Zhang. Dual-\\nalignment pre-training for cross-lingual sentence embedding. In A. Rogers, J. Boyd-Graber, and N. Okazaki,\\neditors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers), pages 3466–3478, Toronto, Canada, July 2023. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2023.acl-long.191. URL https://aclanthology.org/2023.acl-long.191.\\nC.-Y. Lin. Rouge: A package for automatic evaluation of summaries. InText summarization branches out,\\npages 74–81, 2004.\\nS. Lin, B. Liu, J. Li, and X. Yang. Common diffusion noise schedules and sample steps are flawed. In\\nProceedings of the IEEE/CVF winter conference on applications of computer vision, pages 5404–5411, 2024.\\nS. Liu, H. Lu, and J. Shao. Improved residual vector quantization for high-dimensional approximate nearest\\nneighbor search.arXiv preprint arXiv:1509.05195, 2015.\\nJ. Lovelace, V. Kishore, Y. Chen, and K. Q. Weinberger. Diffusion guided language modeling.ArXiv,\\nabs/2408.04220, 2024. URLhttps://arxiv.org/pdf/2408.04220.\\nA. Lozhkov, L. Ben Allal, L. von Werra, and T. Wolf. Fineweb-edu, May 2024. URLhttps://huggingface.\\nco/datasets/HuggingFaceFW/fineweb-edu.\\nC. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic\\nmodel sampling in around 10 steps.Advances in Neural Information Processing Systems, 35:5775–5787,\\n2022.\\nA. Marfurt and J. Henderson. Sentence-level planning for especially abstractive summarization. InACL,\\npages 1–14, 2021. URLhttps://aclanthology.org/2021.newsum-1.1.pdf.\\nB. Minixhofer, J. Pfeiffer, and I. Vulić. Where’s the point? self-supervised multilingual punctuation-agnostic\\nsentence segmentation. InProceedings of the 61st Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 7215–7235, Toronto, Canada, July 2023. Association for\\nComputational Linguistics. URLhttps://aclanthology.org/2023.acl-long.398.\\nI. Mohr, M. Krimmel, S. Sturua, M. K. Akram, A. Koukounas, M. Günther, G. Mastrapas, V. Ravishankar,\\nJ. F. Martínez, F. Wang, Q. Liu, Z. Yu, J. Fu, S. Ognawala, S. Guzman, B. Wang, M. Werk, N. Wang,\\nand H. Xiao. Multi-task contrastive learning for 8192-token bilingual text embeddings, 2024. URL\\nhttps://arxiv.org/abs/2402.17016.\\nN. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen. A\\ncorpus and cloze evaluation for deeper understanding of commonsense stories. InNAACL, pages 839–849,\\nJune 2016. URLhttps://aclanthology.org/N16-1098.\\nS. Narayan, S. B. Cohen, and M. Lapata. Don’t give me the details, just the summary! topic-aware\\nconvolutional neural networks for extreme summarization.arXiv preprint arXiv:1808.08745, 2018.\\nJ. Ni, G. H. Abrego, N. Constant, J. Ma, K. B. Hall, D. Cer, and Y. Yang. Sentence-t5: Scalable sentence\\nencoders from pre-trained text-to-text models.arXiv preprint arXiv:2108.08877, 2021.\\nA. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. InInternational conference\\non machine learning, pages 8162–8171. PMLR, 2021.\\nA. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen. GLIDE:\\nTowards photorealistic image generation and editing with text-guided diffusion models. In K. Chaudhuri,\\nS. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors,Proceedings of the 39th International\\nConference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 16784–\\n16804. PMLR, 17–23 Jul 2022. URLhttps://proceedings.mlr.press/v162/nichol22a.html.\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='M. Ning, M. Li, J. Su, A. A. Salah, and I. O. Ertugrul. Elucidating the exposure bias in diffusion models.\\narXiv preprint arXiv:2308.15321, 2023.\\nNLLB Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi,\\nJ. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. Mejia-\\nGonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews,\\nN. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko,\\nC. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine\\ntranslation, 2022. URLhttps://arxiv.org/abs/2207.04672.\\nOpenAI. GPT-4 technical report.ArXiv, abs/2303.08774, 2024. URLhttps://arxiv.org/pdf/2303.08774.\\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine\\ntranslation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,\\npages 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.\\ndoi: 10.3115/1073083.1073135. URLhttps://aclanthology.org/P02-1040.\\nA. Parola, J. M. Lin, A. Simonsen, V. Bliksted, Y. Zhou, H. Wang, L. Inoue, K. Koelkebeck, and R. Fusaroli.\\nSpeech disturbances in schizophrenia: Assessing cross-linguistic generalizability of nlp automated measures\\nof coherence. Schizophrenia Research, 259:59–70, 2023.\\nW. Peebles and S. Xie. Scalable diffusion models with transformers. InProceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 4195–4205, 2023.\\nE. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general\\nconditioning layer. InProceedings of the AAAI conference on artificial intelligence, 2018.\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised\\nmultitask learners.OpenAI blog, 1(8):9, 2019.\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring\\nthe limits of transfer learning with a unified text-to-text transformer.arXiv e-prints, 2019.\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the\\nlimits of transfer learning with a unified text-to-text transformer.Journal of Machine Learning Research,\\n21(140):1–67, 2020. URLhttp://jmlr.org/papers/v21/20-074.html.\\nN. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese Bert-networks.arXiv preprint\\narXiv:1908.10084, 2019.\\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent\\ndiffusion models. CoRR, abs/2112.10752, 2021. URLhttps://arxiv.org/abs/2112.10752.\\nP. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Borsos, F. de Chaumont Quitry,\\nP. Chen, D. E. Badawy, W. Han, E. Kharitonov, H. Muckenhirn, D. Padfield, J. Qin, D. Rozenberg,\\nT. N. Sainath, J. Schalkwyk, M. Sharifi, M. T. Ramanovich, M. Tagliasacchi, A. Tudor, M. Velimirovic,\\nD. Vincent, J. Yu, Y. Wang, V. Zayats, N. Zeghidour, Y. Zhang, Z. Zhang, L. Zilka, and C. H. Frank.\\nAudiopalm: A large language model that can speak and listen. CoRR, abs/2306.12925, 2023. URL\\nhttps://doi.org/10.48550/arXiv.2306.12925.\\nT. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint\\narXiv:2202.00512, 2022.\\nSeamless Communication, L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler,\\nP.-A. Duquenne, B. Ellis, H. Elsahar, J. Haaheim, J. Hoffman, M.-J. Hwang, H. Inaguma, C. Klaiber,\\nI. Kulikov, P. Li, D. Licht, J. Maillard, R. Mavlyutov, A. Rakotoarison, K. R. Sadagopan, A. Ramakrishnan,\\nT. Tran, G. Wenzek, Y. Yang, E. Ye, I. Evtimov, P. Fernandez, C. Gao, P. Hansanti, E. Kalbassi, A. Kallet,\\nA. Kozhevnikov, G. M. Gonzalez, R. S. Roman, C. Touret, C. Wong, C. Wood, B. Yu, P. Andrews,\\nC. Balioglu, P.-J. Chen, M. R. Costa-jussà, M. Elbayad, H. Gong, F. Guzmán, K. Heffernan, S. Jain, J. Kao,\\nA. Lee, X. Ma, A. Mourachko, B. Peloquin, J. Pino, S. Popuri, C. Ropers, S. Saleem, H. Schwenk, A. Sun,\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='P. Tomasello, C. Wang, J. Wang, S. Wang, and M. Williamson. Seamless: Multilingual expressive and\\nstreaming speech translation.ArXiv, abs/2312.05187, 2023a. URLhttps://arxiv.org/abs/2312.05187.\\nSeamless Communication, L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, P.-A. Duquenne,\\nH. Elsahar, H. Gong, K. Heffernan, J. Hoffman, C. Klaiber, P. Li, D. Licht, J. Maillard, A. Rakotoarison,\\nK. R. Sadagopan, G. Wenzek, E. Ye, B. Akula, P.-J. Chen, N. E. Hachem, B. Ellis, G. M. Gonzalez,\\nJ. Haaheim, P. Hansanti, R. Howes, B. Huang, M.-J. Hwang, H. Inaguma, S. Jain, E. Kalbassi, A. Kallet,\\nI. Kulikov, J. Lam, D. Li, X. Ma, R. Mavlyutov, B. Peloquin, M. Ramadan, A. Ramakrishnan, A. Sun,\\nK. Tran, T. Tran, I. Tufanov, V. Vogeti, C. Wood, Y. Yang, B. Yu, P. Andrews, C. Balioglu, M. R.\\nCosta-jussà, O. Celebi, M. Elbayad, C. Gao, F. Guzmán, J. Kao, A. Lee, A. Mourachko, J. Pino, S. Popuri,\\nC. Ropers, S. Saleem, H. Schwenk, P. Tomasello, C. Wang, J. Wang, and S. Wang. SeamlessM4T - massively\\nmultilingual & multimodal machine translation, 2023b. URLhttps://arxiv.org/abs/2308.11596.\\nA. Shabalin, V. Meshchaninov, E. Chimbulatov, V. Lapikov, R. Kim, G. Bartosh, D. Molchanov, S. Markov,\\nand D. Vetrov. Tencdm: Understanding the properties of diffusion model in the space of language model\\nencodings. ArXiv, abs/2402.19097, 2024. URLhttps://arxiv.org/pdf/2402.19097.\\nN. Shazeer. Glu variants improve transformer.arXiv preprint arXiv:2002.05202, 2020.\\nJ. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models.CoRR, abs/2010.02502, 2020. URL\\nhttps://arxiv.org/abs/2010.02502.\\nN. Srivastava, D. Kuchelev, T. M. Ngoli, K. Shetty, M. Röder, D. Moussallem, H. Zahera, and A.-C. N. Ngomo.\\nLola–an open-source massively multilingual large language model.arXiv preprint arXiv:2409.11272, 2024.\\nS. Sturua, I. Mohr, M. K. Akram, M. Günther, B. Wang, M. Krimmel, F. Wang, G. Mastrapas, A. Koukounas,\\nN. Wang, and H. Xiao. jina-embeddings-v3: Multilingual embeddings with task lora, 2024. URLhttps:\\n//arxiv.org/abs/2409.10173.\\nH. Su, W. Shi, J. Kasai, Y. Wang, Y. Hu, M. Ostendorf, W.-t. Yih, N. A. Smith, L. Zettlemoyer, and T. Yu.\\nOne embedder, any task: Instruction-finetuned text embeddings.arXiv preprint arXiv:2212.09741, 2022.\\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position\\nembedding. Neurocomputing, 568:127063, 2024.\\nX. Sun, Z. Sun, Y. Meng, J. Li, and C. Fan. Summarize, outline, and elaborate: Long-text generation via\\nhierarchical supervision from extractive summaries. InProceedings of the 29th International Conference\\non Computational Linguistics, pages 6392–6402, Gyeongju, Republic of Korea, Oct. 2022. International\\nCommittee on Computational Linguistics. URLhttps://aclanthology.org/2022.coling-1.556.\\nTeam GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang, D. Rojas, G. Feng, H. Zhao, H. Lai,\\nH. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Sun, J. Li, L. Zhao, L. Wu,\\nL. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam,\\nW. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An,\\nY. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang.\\nChatGLM: A family of large language models from glm-130b to glm-4 all tools.ArXiv, abs/2406.12793,\\n2024. URL https://arxiv.org/pdf/2406.12793.\\nThe Llama3 team. The Llama 3 herd of models.ArXiv, abs/2407.21783, 2024. URLhttps://arxiv.org/pdf/\\n2407.21783.\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\\nS. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu,\\nB. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez,\\nM. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu,\\nY. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta,\\nK. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams,\\nJ. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,\\nS. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='T. Ubukata, J. Li, and K. Tei. Diffusion model for planning: A systematic literature review. ArXiv,\\nabs/2408.10266, 2024. URLhttps://arxiv.org/pdf/2408.10266.\\nA. Üstün, V. Aryabumi, Z.-X. Yong, W.-Y. Ko, D. D’souza, G. Onilude, N. Bhandari, S. Singh, H.-L. Ooi,\\nA. Kayid, et al. Aya model: An instruction finetuned open-access multilingual language model.arXiv\\npreprint arXiv:2402.07827, 2024.\\nC. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and\\nF. Wei. Neural codec language models are zero-shot text to speech synthesizers.CoRR, abs/2301.02111,\\n2023. URL https://doi.org/10.48550/arXiv.2301.02111.\\nL. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with large\\nlanguage models. InACL, pages 11897–11916, 2024a. URLhttps://aclanthology.org/2024.acl-long.642.\\nL. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Multilingual e5 text embeddings: A\\ntechnical report, 2024b. URLhttps://arxiv.org/abs/2402.05672.\\nA. Warstadt, A. Singh, and S. R. Bowman. Neural network acceptability judgments. Transactions of\\nthe Association for Computational Linguistics, 7:625–641, 2019. doi: 10.1162/tacl_a_00290. URL\\nhttps://aclanthology.org/Q19-1040.\\nS. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with unlikelihood\\ntraining. arXiv preprint arXiv:1908.04319, 2019.\\nY. Yang, G. H. Abrego, S. Yuan, M. Guo, Q. Shen, D. Cer, Y.-H. Sung, B. Strope, and R. Kurzweil. Improving\\nmultilingual sentence embedding using bi-directional dual encoder with additive margin softmax.arXiv\\npreprint arXiv:1902.08564, 2019.\\nJ. Ye, J. Gao, S. Gong, L. Zheng, X. Jiang, Z. Li, and L. Kong. Beyond autoregression: Discrete diffusion for\\ncomplex reasoning and planning.ArXiv, abs/2410.14157, 2024. URLhttps://arxiv.org/pdf/2410.14157.\\nY. Yin, J. Ding, K. Song, and Y. Zhang. Semformer: Transformer language models with semantic planning.\\nArXiv, abs/2409.11143, 2024. URLhttps://arxiv.org/pdf/2409.11143.\\nN. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural\\naudio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495–507, 2021.\\nB. Zhang and R. Sennrich. Root mean square layer normalization.Advances in Neural Information Processing\\nSystems, 32, 2019.\\nX. Zhang, Y. Zhang, D. Long, W. Xie, Z. Dai, J. Tang, H. Lin, B. Yang, P. Xie, F. Huang, M. Zhang, W. Li,\\nand M. Zhang. mgte: Generalized long-context text representation and reranking models for multilingual\\ntext retrieval, 2024. URLhttps://arxiv.org/abs/2407.19669.\\nY. Zhang, J. Gu, Z. Wu, S. Zhai, J. Susskind, and N. Jaitly. Planner: Generating diversified paragraph via\\nlatent language diffusion model. InNeurIPS, 2023.\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='A Technical consideration for data preparation\\nSince our modeling approach uses a fixed encoder and a fixed document segmentation method, we\\ndecided to use pre-computedSONAR embeddings instead of producing them on-the-fly for each\\ntraining run. This allows for faster iteration on the same data mix, trading expensive GPU compute\\nagainst storage capacity.\\nAs we are storing sequences ofSONAR embedding, which are fixed size tensors of 1024 floats, the\\nstorage requirements become more demanding than storing the raw text. For one terra bytes of raw\\ntext data we need to store between fifteen and twenty terra bytes of encoded data. Overall, this\\ntrade-off in space vs compute reduces the GPU memory occupation and the compute load and lets\\nuse iterate faster. Typically, with on single GPU we can produce around 300-400SONAR sentence\\nembeddings per second whereas by loading precomputed data (potentially from remote storage) we\\ncan load over 20 thousand embeddings per second per GPU (with around 15 CPU per GPU).\\nWe store sequences ofSONAR embeddings with 16 bits precision (FP16) in parquet datasets.\\nEmbeddings remain aligned with the segmented texts and the parquet binary format and library\\necosystem is well suited for storing and loading efficiently such complex data structures. Parquet\\nalso lets us store extra data (such as quality metrics for each sentences) and enables non-trivial last\\nmile data filtering and transformation.\\nFor training theLCM, we processed around four billion documents, generating 310 billion sentences\\nwith an average of 27 tokens per sentences for 88 characters length on average; totaling a bit more\\nthan 889 terra-bytes of raw text.\\nB Open Sourced Code\\nIn the spirit of reproducibility, we release under an open source license the training, evaluation\\nand data processing code for theLCM. This is available athttps://github.com/facebookresearch/\\nlarge_concept_model.\\nThe training code is based on theFairseq2 framework (Balioglu, 2023) that allowed us to build and\\niterate over the different model architectures discussed above. WhileFairseq2 shares the same name\\nas the popular fairseq toolchain, its API architecture is different. It is not a monolithic toolchain but\\na set of modules that can be composed, this allows us to build different architectures side by side\\nand easily share training components.\\nWe also release our evaluation framework so the evaluation tasks reported in 3.1 and comparison\\nbetween theLCM and other models can easily be reproduced. The evaluation framework provides a\\nclear abstraction betweenpredictors, tasks and data loading, which again, makes it modular and lets\\nus describe a set of tasks to be evaluated. The evaluation framework can be run locally or distributed\\nover a SLURM cluster to run evaluations at scale.\\nFinally, we release an updated version of stopes16 to simplify large scale data pre-processing on a\\nSLURM cluster. This was used to run the sentence segmentation andSONAR encoding described\\nin section 2.2. The stopes data processing framework deals with scheduling and monitoring large\\nnumber of jobs on SLURM or to run everything locally for small scale jobs. It provides an API\\ncompatible withray.data17 that makes it easy to process large datasets in blocks and apply transform\\n16https://github.com/facebookresearch/stopes\\n17https://docs.ray.io/\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='function over it. This makes our code reusable outside of a SLURM cluster as it can also be used\\nwith a ray.io cluster.\\nC System prompt: Generation of Topic Descriptions\\nYou are a topic description generator. Your job is to read an extract of text and then generate a\\ntopic description. The extract may be well formed or not. The topic description you will write will\\nbe at most one sentence in length, and use as few words as possible. However, it can not be generic\\nand it can not contain any profanity.\\nHere is an example of an extract, an ideal topic description, and some examples of bad topic\\ndescriptions:\\nExample extract: “One day, one neighborhood of the city was completely devasted. Glass windows\\nwere shattered, shops turned upside down, and many civilian killed. Superman instantly recognized\\nthe signature of one of his old enemies, Voltar, who he had barely beaten in the past. This was a\\nmessage to him: \"I challenge you! Come find me!\"\"\\nAn example of a good topic description: An old enemy of Superman’s, Voltar, appeared and challenged\\nhim.\\nAn example of a bad topic description: Superman\\nAn example of a bad topic description: Voltar\\nAn example of a bad topic description:\\nD User prompt: LLM As a Judge - Coherence\\nBelow is a text extract. Your task is to analyze the extract and assign a coherence score between 0\\nand 5 inclusive, where:\\n0: The text is completely incoherent and lacks any logical connection.\\n1: The text has some minor connections, but overall it is disjointed and hard to follow.\\n2: The text has some coherence, but it is still difficult to understand due to unclear relationships\\nbetween ideas.\\n3: The text is moderately coherent, with some clear connections between ideas, but may lack depth\\nor clarity.\\n4: The text is highly coherent, with clear and logical connections between ideas, making it easy to\\nfollow.\\n5: The text is extremely coherent, with a clear and concise structure, making it effortless to\\nunderstand.\\nYou will provide a score ONLY. Do NOT also provide an explanation.\\nThe extract: <extract>\\nAfter examining the extract, the coherence score between 0 and 5 inclusive is:\\n49')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(file_path = \"LargeConceptModels.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ccce9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='Large Concept Models:\\nLanguage Modeling in a Sentence Representation Space\\nThe LCM team, Loïc Barrault∗, Paul-Ambroise Duquenne∗, Maha Elbayad∗, Artyom\\nKozhevnikov∗, Belen Alastruey†, Pierre Andrews†, Mariano Coria†, Guillaume Couairon+†, Marta'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='R. Costa-jussà†, David Dale†, Hady Elsahar†, Kevin Heffernan†, João Maria Janeiro†, Tuan Tran†,\\nChristophe Ropers†, Eduardo Sánchez†, Robin San Roman†, Alexandre Mourachko‡, Safiyyah\\nSaleem‡, Holger Schwenk‡\\nFAIR at Meta'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='Saleem‡, Holger Schwenk‡\\nFAIR at Meta\\n∗Core contributors, alphabetical order, †Contributors to data preparation, LCM extensions and\\nevaluation, alphabetical order, ‡Research and project management, alphabetical order, +Initial work'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='while at FAIR at Meta, new affiliation: INRIA, France\\nLLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='many tasks. The current established technology ofLLMs is to process input and generate output at\\nthe token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='beyond single words, to analyze information and to generate creative content. In this paper, we present\\nan attempt at an architecture which operates on an explicit higher-level semantic representation,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='which we name a“concept”. Concepts are language- and modality-agnostic and represent a higher\\nlevel idea or action in a flow. Hence, we build a“Large Concept Model”. In this study, as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='proof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence\\nembedding space,SONAR, which supports up to 200 languages in both text and speech modalities.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='The Large Concept Modelis trained to perform autoregressive sentence prediction in an embedding\\nspace. We explore multiple approaches, namely MSE regression, variants of diffusion-based generation,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='and models operating in a quantizedSONAR space. These explorations are performed using 1.6B\\nparameter models and training data in the order of 1.3T tokens. We then scale one architecture to a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='model size of 7B parameters and training data of about 2.7T tokens. We perform an experimental\\nevaluation on several generative tasks, namely summarization and a new task of summary expansion.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='Finally, we show that our model exhibits impressive zero-shot generalization performance to many\\nlanguages, outperforming existingLLMs of the same size. The training code of our models is freely\\navailable.a\\nDate: December 12, 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='available.a\\nDate: December 12, 2024\\nCorrespondence: Holger Schwenk atschwenk@meta.com\\nahttps://github.com/facebookresearch/large_concept_model\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='1 Introduction\\nLarge Language models (LLMs) are dominating current research in natural language processing,\\nand with their recent extension to more modalities, namely images, video and speech, they seem to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='be considered as the de-facto technique to follow to approach human intelligence.LLMs achieve\\nindeed impressive performance on a large variety of tasks, such as providing detailed answers for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='general knowledge questions, helping in performing long document analysis, or drafting different\\ntypes of messages, and writing or debugging code. Building anLLM from scratch requires access to\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='enormous computational resources to process ever larger amounts of data and train models, the size\\nof which now exceeds four hundred billion parameters. Knowledge acquisition inLLMs is heavily'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='data-driven and extending them to more languages or modalities usually requires injecting additional\\n(synthetic) data to cover them.\\nThe landscape of availableLLMs can be structured into open models such asLlama (The Llama3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='team, 2024), Mistral (Jiang et al., 2024), Bloom (BigScience Workshop, 2023) or Falcon\\n(Almazrouei et al., 2023), on the one hand, and closed models such asGemini (Gemini Team Google,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='2024), GPT (OpenAI, 2024) orClaude (Anthropic, 2024), on the other. It is striking that all\\nthese models are based on the same underlying architecture: a transformer-based, decoder-only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='language model, which is pretrained to predict the next token, given a long context of preceding\\ntokens. Despite the undeniable success ofLLMs and continued progress, all currentLLMs miss'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='a crucial characteristic of human intelligence: explicit reasoning and planning at multiple levels of\\nabstraction. The human brain does not operate at the word level only. We usually have a top-down'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='process to solve a complex task or compose a long document: we first plan at a higher level the\\noverall structure, and then step-by-step, add details at lower levels of abstraction. One may argue'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='that LLMs are implicitly learning a hierarchical representation, but we stipulate that models with\\nan explicit hierarchical architecture are better suited to create coherent long-form output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='Imagine a researcher giving a fifteen-minute talk. In such a situation, researchers do not usually\\nprepare detailed speeches by writing out every single word they will pronounce. Instead, they outline'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='a flow of higher-level ideas they want to communicate. Should they give the same talk multiple\\ntimes, the actual words being spoken may differ, the talk could even be given in different languages,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='but the flow of higher-level abstract ideas will remain the same. Similarly, when writing a research\\npaper or essay on a specific topic, humans usually start by preparing an outline that structures the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='whole document into sections, which they then refine iteratively. Humans also detect and remember\\ndependencies between the different parts of a longer document at an abstract level. If we expand'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='on our previous research writing example, keeping track of dependencies means that we need to\\nprovide results for each of the experiment mentioned in the introduction. Finally, when processing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='and analyzing information, humans rarely consider every single word in a large document. Instead,\\nwe use a hierarchical approach: we remember which part of a long document we should search to\\nfind a specific piece of information.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='find a specific piece of information.\\nTo the best of our knowledge, this explicit hierarchical structure of information processing and\\ngeneration, at an abstract level, independent of any instantiation in a particular language or modality,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='cannot be found in any of the currentLLMs. In this work, we present a new approach which\\nmoves away from processing at the token level and closer to (hierarchical) reasoning in an abstract'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='embedding space. This abstract embedding space is designed to be independent of the language or\\nmodality in which the content is expressed; in other words, we aim to model the underlying reasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='process at a purely semantic level, not its instantiation in a specific language. In order to verify our\\napproach, we limit our study to two levels of abstraction: subword tokens andconcepts. We define a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='concept as an abstract atomic idea. In practice, a concept would often correspond to a sentence in a\\ntext document, or an equivalent speech utterance. We posit that a sentence is an appropriate unit to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='achieve language independence, in opposition to single words. This is in sharp contrast to current\\nLLMs techniques which are heavily English centric and token based.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='Our fundamental idea could be based on any fixed-size sentence embedding space for which an\\nencoder and decoder are available. In particular, we could aim to train a new embedding space'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='specifically optimized to our reasoning architecture. In this work, we chose an existing and freely\\navailable sentence embedding, namedSONAR (Duquenne et al., 2023b).SONAR supports text\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='Figure 1 - Left: visualization of reasoning in an embedding space of concepts (task of summarization).\\nRight: fundamental architecture of anLarge Concept Model(LCM).\\n⋆: concept encoder and decoder are frozen.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='⋆: concept encoder and decoder are frozen.\\ninput and output in 200 languages, speech input in76 languages, and speech output in English. We\\ndiscuss the constraints and impact of this choice in Section 2.1, and share some ideas on alternative'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='embedding spaces in Section 6.\\nFigure 1-left visualizes reasoning in an embedding space with the example of a summarization task,\\nwhich is materialized by a function on the embedding space, mapping five concept representations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='into two. Figure 1-right summarizes the overall architecture and processing flow. The input is first\\nsegmented into sentences, and each one is encoded withSONAR to achieve a sequence of concepts,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='i.e., sentence embeddings. This sequence of concepts is then processed by aLarge Concept Model\\n(LCM) to generate at the output a new sequence of concepts. Finally, the generated concepts are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='decoded bySONAR into a sequence of subwords. The encoder and decoder are fixed and are not\\ntrained. It is important to highlight that the unchanged sequence of concepts at the output of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='the LCM can be decoded into other languages or modalities without performing again the whole\\nreasoning process. In the same spirit, a particular reasoning operation such as summarization can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='be performed in a zero-shot setting on input in any language or modality, since it solely operates\\non concepts. To summarize, theLCM neither has information on the input language or modality'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='nor generates output in a particular language or modality. We explore multiple architectures to\\ntrain theLCM, in particular several variants of diffusion. Finally, we envision an additional level of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='abstraction beyond concepts which could correspond to a short description of a paragraph or small\\nsection. In Section 4.3 we report initial ideas on how conditioning and predicting such higher-level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='representations can improve consistency of output generated by anLCM.\\nTo some extent, theLCM architecture resembles theJepa approach (LeCun, 2022) that also aims to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='predict the representation of the next observation in an embedding space. However, unlikeJepa that\\nplaces more emphasis on learning a representation space in a self-supervised way, theLCM focuses'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='on accurate prediction in the existing embedding space.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='The mains characteristics of our genericLarge Concept Modelapproach are as follows:\\n• Reasoning at an abstract language- and modality-agnostic level beyond tokens:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='– We model the underlying reasoning process, not its instantiation in a particular language.\\n– The LCM can be trained, i.e. acquire knowledge, on all languages and modalities at once,\\npromising scalability in an unbiased way.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='promising scalability in an unbiased way.\\n• Explicit hierarchical structure:\\n– Better readability of long-form output by a human.\\n– Facilitates local interactive edits by a user.\\n• Handling of long context and long-form output:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='• Handling of long context and long-form output:\\n– The complexity of a vanilla transformer model increases quadratically with the sequence length.\\nThis makes handling of large context windows challenging and several techniques have been'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='developed to alleviate this problem,e.g., sparse attention (Child et al., 2019) or LSH attention\\n(Kitaev et al., 2020). OurLCM operates on sequences which are at least an order of magnitude\\nshorter.1\\n• Unparalleled zero-shot generalization:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='• Unparalleled zero-shot generalization:\\n– Independently of the language or modality theLCM is pre-trained and fine-tuned on, it can be\\napplied to any language and modality supported by theSONAR encoders, without the need of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='additional data or fine-tuning. We report results for multiple languages in the text modality.\\n• Modularity and extensibility:\\n– Unlike multimodalLLMs that can suffer from modality competition (Aghajanyan et al., 2023;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='Chameleon team, 2024), concept encoders and decoders can be independently developed and\\noptimized without any competition or interference.\\n– New languages or modalities can be easily added for an existing system.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='The goal of this paper is to provide a proof of concept of this high-level vision of an alternative\\narchitecture to current best practice in language modeling. In the next section we present the main'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='design principles of our models and discuss several variants to build and train aLarge Concept\\nModel. We discuss several designs to implement diffusion approaches with concept embeddings and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='carefully study noise scheduling. This section is completed by a compute complexity comparison\\nwith token-basedLLMs. Section 3 is dedicated to the analysis of a larger 7B parameter model. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='discuss challenges when instruction fine-tuning this model on multiple generative tasks, and provide\\na comparison with existingLLMs of comparable size. The paper concludes with a discussion of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='related work, the current limitations and perspectives of our approach.\\nTo foster research in this area, we make ourLCM training code2 as well as SONAR encoders and\\ndecoders3 for up to 200 languages and multiple modalities freely available.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='1We assume an average sentence length of 10–20 tokens.\\n2https://github.com/facebookresearch/large_concept_model\\n3https://github.com/facebookresearch/SONAR\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='2 Main Design Principles\\nIn this section, we outline the main design principles of theLCM. We first describe theSONAR\\nembedding space with its encoders and decoders. Then, we discuss details of data preparation,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='namely sentence segmentationi.e., how we split long documents into sentences. And finally, we\\ndescribe in details the different versions ofLCMs introduced in this work.\\n2.1 The SONAR embedding space'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='2.1 The SONAR embedding space\\nThe motivation of this work is to perform reasoning at a higher conceptual level than tokens. This\\nrequires an embedding space which is highly semantic. We choseSONAR (Duquenne et al., 2023b)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='since it achieves best performance on several semantic similarity metrics likexsim or xsim++ (Chen\\net al., 2023b), and it was successfully used in large-scale bitext mining for translation (Seamless\\nCommunication et al., 2023b).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='Communication et al., 2023b).\\nThe SONAR text embedding space was trained as an encoder/decoder architecture, with a fixed-size\\nbottleneck instead of cross-attention (see Figure 2). The criterion combines a machine translation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='objective for 200 languages into and out of English, denoising auto-encoding and an explicit MSE loss\\nat the embedding bottleneck layer. Once the text embedding space was trained, a teacher-student'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='approach was applied to extend theSONAR space to the speech modality. More details on the\\narchitecture and training procedure can be found in Duquenne et al. (2023b), and detailed speech'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='recognition and translation results in the appendix of Seamless Communication et al. (2023a).\\nOur LCM operates directly onSONAR concepts embeddings, hence, it can perform reasoning on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='all supported languages and modalities. Table 1 compares the language coverage of several other\\nLLMs. The LCM supports substantially more languages than other models, in particular many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='low-resource languages. In addition to the text modality,SONAR supports 76 languages for speech\\ninput and speech output in English. We have also developed an experimental encoder for American'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='Sign language (ASL). All these encoders and decoders are freely available.4 Exact listings of the\\nsupported languages can be found in theSONAR GitHub repository.\\n4https://github.com/facebookresearch/SONAR'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='4https://github.com/facebookresearch/SONAR\\nFigure 2 - Encoder/decoder bottleneck architecture to train theSONAR text embeddings (right part of\\nfigure). Teacher-student approach to extendSONAR to the speech modality (left part).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='Text Speech Image Video\\nModel Input Output Input Output Input Output Input Output\\nGemini 47 47 62 ✓ ✓ ✓ ✓ ✗\\nGPT 85 85 ✓ ✓ ✓ ✓ ? ✗\\nClaude 37 37 ✓ ✓ ✓ ✓ ✗ ✗\\nBloom 46 46 ✗ ✗ ✓ ✓ ✗ ✗\\nLlama 3-400B 8 8 34 ✗ ✓ ✓ ✗ ✗\\nLCM-SONAR 200 200 76 1 ✗ ✗ (ASL) ✗'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='LCM-SONAR 200 200 76 1 ✗ ✗ (ASL) ✗\\nTable 1- Comparison of language and modality coverage for severalLLMs and ourLCM operating on the\\nSONAR embedding space. SONAR has an experimental support for American Sign Language (ASL) which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='is not used in this paper.\\n2.2 Data preparation\\nTo train and evaluate theLCM, we need to convert raw text datasets into a sequence ofSONAR\\nembeddings, each one corresponding to a sentence. Dealing with large text corpora presents several'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='practical limitations. First, the precise segmentation of a text into sentences can be challenging due\\nto the presence of errors, specific formatting issues or any other sources of noise. This requires us to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='apply robust automatic text segmentation techniques. Second, some sentences (even well formed)\\ncan be very long and complex, which might negatively impact the quality of the encodedSONAR'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='embeddings. This is particularly prevalent for texts in the scientific domain. In the following, we\\ndiscuss strategies for sentence segmentation and how they affect theSONAR encoding.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='Sentence segmentation analysis We have identified two potential sentence segmentation tech-\\nniques; as we are exploring multilingual data, we focus on sentence segmenters with a large language\\ncoverage:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='coverage:\\n1. SpaCy segmenter (SpaCy) (Honnibal et al., 2020) is a well established multilingual NLP toolkit\\nthat provides a rule-based approach to sentence segmentation.SpaCy is thoroughly tested for\\nhigh-resource languages.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='high-resource languages.\\n2. Segment any Text (SaT) (Minixhofer et al., 2023; Frohmann et al., 2024) offers a suite of\\nmodels and adapters that predict sentence boundaries at the token level.SaT is designed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='to be resilient to perturbations, particularly avoiding the over-reliance on punctuation and\\ncapitalization. This is valuable in domains where these conventional markers are often missing.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='The quality ofSaT’s segmentation is however dependent on the choice of an “appropriate” split\\nprobability threshold.\\nWe additionally customize both methods by incorporating a maximum sentence length cap in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='characters. We refer to these extensions bySpaCy Capped and SaT Capped. Long sentences\\nare broken down into smaller, logically coherent fragments using a rule-based approach based on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='punctuation marks forSpaCy. ForSaT, we leverage the provided splitting probability estimates to\\nidentify the next best potential split.\\nTo measure the efficacy of a given segmenter, we evaluate the quality of the reconstructed sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='with AutoBLEU. It is defined as aBLEU score (Papineni et al., 2002) comparing the decoded text\\nfrom aSONAR vector after encoding a segment, to the the reference segment. A good segmentation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='will yield segments that can be encoded and then decoded without loss of signal, and thus score a\\nhigher AutoBLEU.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='For this analysis, we sample 10k documents from our pretraining datasets, representing approximately\\n500k sentences. The documents are processed with each segmenter, the sentences are encoded then'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='decoded and theAutoBLEU score is calculated. We stratified the results based on the lengths of\\nthe original sentences.\\n0 100 200 300 400 500\\nSentence Size in Characters\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\nAverage Auto-BLEU Score\\nSAT\\nSpaCy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='0.85\\n0.90\\n0.95\\nAverage Auto-BLEU Score\\nSAT\\nSpaCy\\n0 25 50 75 100 125 150 175 200\\nSentence Size in Characters\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\nSAT Capped\\nSpaCy Capped'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='0.75\\n0.80\\n0.85\\n0.90\\n0.95\\nSAT Capped\\nSpaCy Capped\\nFigure 3 - Segmenters quality.Average Auto-BLEU scores for different sentence segmentation methods\\ndepending on sentence length, for both out of the box (left) and capped implementations (right).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='As illustrated in Figure 3 and with a capping at 200 characters, theSaT Capped method demon-\\nstrates a slight but consistent advantage overSpaCy Capped. Both out-of-the-box segmenters,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='however, exhibit significant under-performance across all sentence lengths. This lower performance is\\nespecially pronounced for sentences exceeding 250 characters, underscoring the limitations of using\\nthe segmenters without capping.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='the segmenters without capping.\\nAccordingly, we prepare theLCM training data withSaT Capped. We discuss in Appendix A\\ntechnical and engineering challenges faced when handling large amounts ofSONAR embeddings.\\n2.3 Large Concept Modelvariants'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='2.3 Large Concept Modelvariants\\nThe design of the LCM is driven by the need to conditionally generate a continuous sentence\\nembedding. This obviously contrasts with how currentLLMs work, i.e., estimating a probability'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='distribution over a vocabulary of discrete tokens. A straightforward way of solving the task is to\\ntrain a transformer model to generate an embedding with the objective of minimizing the MSE loss'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='(see Section 2.3.1). However, a given context may have many plausible, yet semantically different,\\ncontinuations. The model should thus be able to learn a conditional probability distribution over the\\ncontinuous embedding of the next sentence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='continuous embedding of the next sentence.\\nThere is a large body of work in computer vision aiming to learn such conditional probability\\ndistributions over continuous data (Dhariwal and Nichol, 2021; Rombach et al., 2021). Models like'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='Dall-E 3 (Betker et al., 2023) or Imagen Video (Ho et al., 2022) use a diffusion process to generate an\\nimage or video from a text prompt. Many different real images may satisfy the same input prompt,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='hence the model has to learn a probability distribution over continuous pixel data. This motivates\\nthe exploration of diffusion models for sentence embedding generation. Two variants are presented in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content='Sections 2.3.3 and 2.3.4. Another prevalent take on continuous data generation consists of quantizing\\nsaid data to ultimately model with discrete units; we exploreLCM modeling with quantization in\\nSection 2.3.5.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='xn ˆxn\\nˆxn\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\nPreNet\\nFeature normalizer\\nLinear\\nRdSONAR → Rdmodel\\nPostNet\\nLinear\\nRdSONAR → Rdmodel\\nFeature normalizer\\nFigure 4- TheBase-LCM.Illustration of theBase-LCM. At its core is a standard decoder-only Transformer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='surrounded with aPreNet and aPostNet.\\n2.3.1Base-LCM\\nOur baseline architecture for next-concept prediction is a standard decoder-only Transformer that\\ntransduces a sequence of preceding concepts (read sentence embeddings) into a sequence of future'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='ones. As illustrated in Figure 4, theBase-LCM is equipped with a “PostNet” and a “PreNet”. The\\nPreNet normalizes the inputSONAR embeddings and maps them to the model’s hidden dimension\\ndmodel.\\nPreNet(x) = normalize(x)Wt\\npre + bpre, (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='PreNet(x) = normalize(x)Wt\\npre + bpre, (1)\\nPostNet(x) = denormalize\\n\\x00\\nxWt\\npost + bpost\\n\\x01\\n, (2)\\n(3)\\nwhere Wpost ∈ RdSONAR×dmodel , bpost ∈ RdSONAR, Wpre ∈ Rdmodel×dSONAR and bpre ∈ Rdmodel .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='In order to learn the maps “normalize” and its inverse “denormalize” we fit a robust scaler to a set\\nof randomly sampledSONAR vectors from different corpora and domains of text data. This scaler'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='removes the median statistics and scales the data according to the interquartile range (IQR).\\nnormalize(x) =x − µ\\nσ , denormalize(x) =µ + σx. (4)\\nThe Base-LCM is trained on the semi-supervised task of next concept prediction, that is, the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='predicts the next conceptˆxn and its parametersθ are optimized to regress the ground truth next\\nconcept (xn).\\nˆxn = f(x<n; θ), MSE(ˆxn, xn) =∥ˆxn − xn∥2. (5)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='ˆxn = f(x<n; θ), MSE(ˆxn, xn) =∥ˆxn − xn∥2. (5)\\nGiven a data distribution q of documents (sequences of concepts), the training loss is evaluated as:\\nLBase-LCM(θ) =Ex∼q\\nh |x|X\\nn=1\\nMSE (f(x<n; θ), xn)\\ni\\n. (6)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='h |x|X\\nn=1\\nMSE (f(x<n; θ), xn)\\ni\\n. (6)\\nIn order to enable the generation of variable length documents at inference time, we suffix training\\ndocuments with the sentence “End of text.”. Similar to any sentence in the document, this special\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='suffix will be encoded withSONAR. This means thatx|x| = − →eot := encode(\"End of text.\"). During\\ninference, we implement two main early stopping mechanisms: the first one measures the similarity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='of the generated embeddingˆxn to − →eot and stops if the cosine similarity exceeds a thresholdseot. The\\nsecond mechanism compares the newly generated embeddingˆxn to the previous generationˆxn−1 and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='stops if their cosine similarity is higher than a thresholdsprev. We set bothseot and sprev to 0.9.\\n2.3.2 Diffusion-based LCM\\nDiffusion-based LCMs are generative latent variable models that learn a model distributionpθ'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='approximating a data distributionq. Similar to theBase-LCM, we model the diffusionLCMs as\\nauto-regressive models that generate concepts in a document, one at a time. The model distribution'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='is thus expressed at each positionn of the sequence aspθ(xn|x<n) i.e., the generation of the next\\nconcept is conditioned on the preceding context.\\nIn what follows we use a superscript for the denoising/diffusion step (t ∈ [0, 1]) and a subscript (n)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='for indexing the sequence of concepts. We simplify for a givenn the conditional model distribution\\npθ(x0\\nn|x0\\n<n) as pθ(x0), and the conditional data distribution q(x0\\nn|x0\\n<n) as q(x0).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='n|x0\\n<n) as q(x0).\\nDiffusion models involve two processes: aforward noising process and areverse denoising process (Ho\\net al., 2020; Song et al., 2020):\\nForward process and noise schedule The forward process is a Gaussian diffusion process'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='characterized by the marginal distribution q(xt|x0), given for every timestept ∈ [0, 1] as:\\nq(xt|x0) := N(αtx0, σ2\\nt I). (7)\\nWith the reparameterization trick, we can sample from this marginal distribution via:\\nxt = αtx0 + σtϵ where ϵ ∼ N(0, I) (8)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='xt = αtx0 + σtϵ where ϵ ∼ N(0, I) (8)\\nWe use a variance-preserving forward process (Karras et al., 2022) for which we have:\\nα2\\nt = sigmoid(λt), σ 2\\nt = sigmoid(−λt) = 1− sigmoid(λt), λ t = log\\n\\x00\\nα2\\nt /σ2\\nt\\n\\x01\\n, (9)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='\\x00\\nα2\\nt /σ2\\nt\\n\\x01\\n, (9)\\nwhere λt is the log signal-to-noise ratio (log-SNR) for timestept.\\nThe noise schedule is a strictly monotonically decreasing functionfλ that maps from the timestep\\nt ∈ [0, 1] to a log-SNR level:λt = fλ(t).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='t ∈ [0, 1] to a log-SNR level:λt = fλ(t).\\nIt is common in previous work to also define the noise schedule based on a discrete variance schedule\\n(β0, . . . , βT ). This stems from the formulation of the forward process as a discrete-time Markov chain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='that gradually adds Gaussian noise to the data according to said variance schedule:\\nq(x1...T|x0) :=\\nTY\\nt=1\\nq(xt|xt−1), q(xt|xt−1) := N(xt;\\np\\n1 − βtxt−1, βtI), (10)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='p\\n1 − βtxt−1, βtI), (10)\\nwhere to simplify the notation,xt is short forxt/T as the timesteps are now discretized.\\nFrom the variance schedule(βt)t, the noise schedule can be expressed as:\\nα2\\nt =\\ntY\\ns=1\\n(1 − βs). (11)\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='Following Kingma and Gao (2024), for any given noise schedule, we visualize the distribution over\\nnoise levelsp(λ) =−dt/dλ in order to characterize how much time we are spending at every noise\\nlevel during training.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='level during training.\\nIn this work, we consider three types of noise schedules:\\nCosine. The schedule formulated in Nichol and Dhariwal (2021) as:\\nα2\\nt = f(t)/f(0), where f(t) = cos2\\n\\x12t + s\\n1 +s.π\\n2\\n\\x13\\n, where s = 0.008. (12)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='\\x12t + s\\n1 +s.π\\n2\\n\\x13\\n, where s = 0.008. (12)\\nQuadratic. The schedule introduced in Ho et al. (2020) where the variances(βt)t are set to constants\\nincreasing quadratically fromβ0 to β1.\\nβt/T =\\n\\x12p\\nβ0 + t\\nT.\\n\\x10p\\nβ1 −\\np\\nβ0\\n\\x11\\x132\\n. (13)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='βt/T =\\n\\x12p\\nβ0 + t\\nT.\\n\\x10p\\nβ1 −\\np\\nβ0\\n\\x11\\x132\\n. (13)\\nSigmoid. We introduce in this work, thesigmoid schedule as a means to study the impact of the SNR\\ndistribution on the training of our models. The schedule is parametrized by two hyper-parameters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='(γ, δ) and is defined as:\\nα2\\nt = f(t)/f(0), where f(t) = sigmoid (δ − γ logit(t)) , (14)\\nwhere “sigmoid” is the sigmoid functionsigmoid x 7→ ex/(ex + 1)and “logit” its inverse function'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='logit : x 7→ log(x/(1 − x)). The hyper-parameterγ controls the scale of the log-SNR distribution\\np(λ) and δ its center (see Figure 5).\\nIn all our experiments, we follow Lin et al. (2024) and rescale the variance schedule(β1, . . . βT ) to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='enforce zero terminal SNRi.e., βT = 1.\\nReverse process and objective function The joint distribution of the diffusion modelpθ(x0...1)\\nis called the reverse process and is defined as a Markov chain with learned Gaussian transitions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='starting at p(x1) =N(0, I). In its discretized form:\\npθ(x0:T) := p(xT)\\nTY\\nt=1\\npθ(xt−1|xt), pθ(xt−1|xt) := N(xt−1; µθ(xt, t), Σθ(xt, t)), (15)\\nwhere µθ and Σ are predicted statistics.Σ is set to to the constantσ2\\nt I (matching the transitions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='t I (matching the transitions\\nof the forward process). µθ can be decomposed into a linear combination ofxt−1 and a noise\\napproximation modelϵθ. This prediction method is dubbedϵ-prediction (Ho et al., 2020; Nichol'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='and Dhariwal, 2021; Nichol et al., 2022). In this work we adoptx0-prediction i.e., we predict the\\nnoiseless state and optimize the simple reconstruction loss:\\nL(θ) := Et∼U(0,1)\\n\\x02\\nω(t)L(t, θ)\\n\\x03\\n, L(t, θ) := Ex0,ϵ\\nh\\r\\rx0 − µθ(αtx0 + σtϵ, t)\\n\\r\\r2\\n2\\ni'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='h\\r\\rx0 − µθ(αtx0 + σtϵ, t)\\n\\r\\r2\\n2\\ni\\n. (16)\\nDifferent weighting strategies for the reconstruction loss were proposed in the literature (Ho et al.,\\n2020; Salimans and Ho, 2022; Hang et al., 2023). In this work, we default to the simple reconstruction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='loss (ω(t) = 1, ∀t) and we experiment with a clamped-SNR weighting strategy:\\nω(t) = max(min(exp(λt), λmax), λmin), λt = log(α2\\nt /σ2\\nt ), (17)\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='0.00 0.25 0.50 0.75 1.00\\nt\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nαt\\n0.00 0.25 0.50 0.75 1.00\\nt\\n7.5\\n5.0\\n2.5\\n0.0\\n2.5\\n5.0\\n7.5\\nλt = logSNRt\\n10\\n 5\\n 0 5\\nλt = logSNRt\\n0.000\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\n0.150\\n0.175\\np(λ)\\nCosine\\nSigmoid(1.5, -2)\\nSigmoid(1.5, -1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='p(λ)\\nCosine\\nSigmoid(1.5, -2)\\nSigmoid(1.5, -1)\\n0.00 0.25 0.50 0.75 1.00\\nt\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nαt\\n0.00 0.25 0.50 0.75 1.00\\nt\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\nλt = logSNRt\\n10\\n 0 10\\nλt = logSNRt\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\np(λ)\\nCosine\\nSigmoid(0.8, -1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='0.15\\n0.20\\n0.25\\n0.30\\np(λ)\\nCosine\\nSigmoid(0.8, -1)\\nSigmoid(3.5, 0)\\n0.00 0.25 0.50 0.75 1.00\\nt\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nαt\\n0.00 0.25 0.50 0.75 1.00\\nt\\n10.0\\n7.5\\n5.0\\n2.5\\n0.0\\n2.5\\n5.0\\n7.5\\nλt = logSNRt\\n10\\n 5\\n 0 5\\nλt = logSNRt\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\np(λ)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='0 5\\nλt = logSNRt\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\np(λ)\\nCosine\\nQuadaratic-1\\nQuadratic-2\\nFigure 5 - Noise schedules.Illustrations of the different noise schedules explored in this work. Our default'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='schedule being cosine. Quadratic-1 is characterized by(β0 = 0.001, βT = 0.0012) and Quadratic-2 by\\n(β0 = 0.02, βT = 0.022) For each schedule we visualize the curve of(αt)t (see Equation (8)), the curve of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='log-SNR and the associated distribution over noises levelsp(λ) (Kingma and Gao, 2024).\\nwhich is a generalization of Salimans and Ho (2022)’s truncated-SNR weighting and Hang et al.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='(2023)’s min-SNR strategy where the SNR is clamped between a min- and max-valueλmin and λmax.\\nAdditionally, we consider a weighting strategy that factors in the quality of the samplex0. We use'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='as sample weight a scalarω(x0) ∈ [0, 1] correlated with the sample’s fragility scorei.e., how easy\\nit is to reconstruct a noised sample (see Section 2.5.2). Fragile samples will be assigned a smaller'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='weight and thus contribute less to the objective function.\\nLfragility(θ) := Et∼U(0,1),x0,ϵ\\nh\\nω(x0)\\n\\r\\rx0 − µθ(αtx0 + σtϵ, t)\\n\\r\\r2\\n2\\ni\\n, (18)\\nω(x0) = sigmoid(a fragility(x0) +b), (19)\\nwhere a <0 and b are hyper-parameters to tune.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='where a <0 and b are hyper-parameters to tune.\\nClassifier-free diffusion guidance for theLCM Classifier-free diffusion guidance (Ho and\\nSalimans, 2022) consists of jointly training a conditional and an unconditional diffusion model. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='resulting conditional and unconditional score estimates are combined at inference time to achieve a\\ntrade-off between sample quality and diversity. This combined score is defined as follows:\\n∇x logγ p(x|y) = (1− γ)∇x log p(x) +γ∇x log p(x|y), (20)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='where y is the conditioning variable, in our case the sequence of preceding embeddings(x1, . . .xn−1)\\nwhen denoisingxn.\\nThe hyper-parameterγ controls the contribution of the conditional score; Forγ = 0, this is equivalent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='to an unconditional model, and forγ = 1, it is a fully conditional model. In practice for vision\\nmodels, γ is set to a value greater than 1, thus amplifying the signal from the conditioning model.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='Inference At inference time, the reverse process is applied.xT is obtained by sampling a random\\nnoise fromp(xT) =N(0, I), and is then iteratively denoised by taking steps in the direction of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='score function (i.e., the direction in which the log-likelihood increases fastest). Additional noise is\\nadded during the process in order to avoid falling down into modes of the distribution.\\nPractically, we start fromxT ∼ N(0, σ2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='Practically, we start fromxT ∼ N(0, σ2\\ninitI) and find that the quality of the sampled output is\\nsensitive to the initial noise scaleσinit.\\nAlthough we train the model on a large number of discretized timesteps,e.g., T=100, we only generate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='with a smaller number of steps,e.g., S=40, at inference via accelerated generation processes (Song\\net al., 2020). We select the sample steps following the trailing method of Lu et al. (2022) as it is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='found to be more efficient for smaller stepsS (Lin et al., 2024). That is we generate along the sampled\\nsteps (τ1, . . . τS) =round(flip(arange(T, 0, −T/S))). During inference, we employ the classifier-free'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='guidance rescaling technique of Lin et al. (2024) proven to alleviate the image over-exposure problem\\nencountered in image synthesis diffusion models as the terminal SNR approaches zero. We denote'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='with gscale and grescale the guidance scale and guidance rescale factors used at inference.\\nFollowing Ning et al. (2023), we perform Epsilon-scaling at inference time as it is shown to alleviate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='the exposure bias problem in diffusion models. In its simplified version, this is a training-free method\\nthat consists of scaling down the over-predicted magnitude of error by a scalarλeps.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='We describe in Section 2.3.3 and Section 2.3.4 two variants of diffusionLCM: One-Tower and\\nTwo-Tower.\\nxtn\\n0 0 0 t\\nˆx0\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\n×(s)\\ndenoising\\nsteps\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\nxtn\\nˆx0\\nPreNet\\nSelf-attention⋆'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='Decoder\\nPostNet\\nxtn\\nˆx0\\nPreNet\\nSelf-attention⋆\\nCross-attention\\nFeed forward\\nPostNet\\nmodulator\\nt\\n×Ld\\n×(s)\\ndenoising\\nsteps\\nFigure 6- Inference with diffusion-based LCMs.In the left-hand side, an illustration of theOne-Tower LCM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='and on the right-hand side an illustration of theTwo-Tower LCM.\\n2.3.3One-TowerDiffusionLCM\\nThis model, depicted in the left panel of Figure 6, consists of a single transformer backbone whose\\ntask is to predict the clean next sentence embeddingx0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='n given a noisy inputxt\\nn, conditioned on\\nprevious clean sentence embeddingsx0\\n<n. During training, self-attention can be dropped with a\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='xt00 x00 xt11 x01 xt22\\n0 0t0 t1 t2\\nˆx00 ˆx01 ˆx02\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\nFigure 7 - Training ofOne-Towerdiffusion LCM. Interleaving the clean and noisy embeddings and sampling\\ndifferent diffusion timesteps allows for efficient training.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='certain probability for unconditional training. This enables classifier-free guidance at inference time\\n(see Section 2.3.2 for details).\\nEach input embedding is concatenated with the corresponding diffusion timestep embedding. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='learned position embeddings are added to the input vectors prior to being fed toLCM. The backbone\\nutilizes a causal multi-head self-attention.\\nFor efficient training, the model is trained to predict each and every sentence in a document at once.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='As depicted in Figure 7, during the diffusion process, the model attends to the clean sentences in\\nthe context using causal multi-head attention layers. The input is specially prepared by interleaving'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='the noisy (blue) and clean (light blue) sentence embeddings, and the attention mask is prepared\\naccordingly to only attend to the clean sentence embeddings (gray arrows).\\n2.3.4 Two-TowerDiffusion LCM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='2.3.4 Two-TowerDiffusion LCM\\nThis model, depicted in the right panel of Figure 6, separates the encoding of the preceding context\\nfrom the diffusion of the next embedding. A first model, labeledcontextualizer, takes as input the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='context vectorsx<n and encodes them causallyi.e., we apply a decoder-only Transformer with causal\\nself-attention. The outputs of the contextualizer are then fed to a second model dubbeddenoiser,\\nwhich predicts the clean next sentence embeddingx0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='n by iteratively denoising the latentx1\\nn ∼ N(0, I).\\nThe denoiser consists of a stack of Transformer blocks with cross-attention block to attend over\\nthe encoded context. Both the denoiser and the contextualizer share the same Transformer hidden'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='dimension dmodel. Each block of each Transformer layer in the denoiser (including the cross-attention\\nlayer) is modulated with adaptive layer norm (AdaLN, Perez et al. (2018); Peebles and Xie (2023)).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='The AdaLN modulator ofTwo-Tower regresses channel-wise scale (γ), shift (β) and residual gates\\n(α) from the embedding of the current diffusion timestept.\\n[β, γ, α] = SiLU(embed(t))Wt + b, (21)\\ny = x + α Block((1 +γ) x + β), (22)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='y = x + α Block((1 +γ) x + β), (22)\\nFollowing Peebles and Xie (2023) and Goyal (2017) we initialize each residual block in a Transformer\\nlayer (“Block”) with the identity function via initializingW and b in Equation (21) to zero. The\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='x0 x1 x2\\nPreNet\\nTransformer\\nDecoder\\nPostNet\\ns0 s1 s2\\nxt00 xt11 xt22\\nˆx0n ˆx1n ˆx2n\\nPreNet\\nSelf-attention⋆\\nCross-attention\\nFeed forward\\nPostNet\\nmodulator\\nt0, t1, t2\\n×Ld\\nh4\\nh3\\nh2\\nh1\\nh0\\n0 s0 s1 s2 s3 s4\\n1 1 1 1 1 0\\n1 0 0 0 0 0\\n1 1 1 0 0 0\\n1 1 0 0 0 0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='1 1 1 1 1 0\\n1 0 0 0 0 0\\n1 1 1 0 0 0\\n1 1 0 0 0 0\\n1 0 0 0 0 0\\nFigure 8- Training Two-Tower diffusion LCM.On the left panel, aTwo-Tower forward pass in training time'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='in order to denoise multiple embeddings in parallel. On the right side panel a visualization of the denoiser’s\\ncross-attention masks with the red highlighted row signaling a sample dropped to train the denoiser'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='unconditionally. (h1, . . . , h4) denotes the sequence of intermediate representations in the denoiser right before\\nthe cross-attention layer.\\ndiffusion timestept is embedded using a 256-dimensional frequency embedding (Dhariwal and Nichol,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='2021; Peebles and Xie, 2023) followed by a two-layer MLP withSiLU as activation function. “embed”\\nmaps to the denoiser’s hidden dimensiondmodel. The self-attention layers in the denoiser do only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='attend to the current positioni.e., we do not attend to the preceding noised context. The self-\\nattention layers were kept for consistency with a standard Transformer block and for the possible\\nextension of denoising multiple vectors at once.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='extension of denoising multiple vectors at once.\\nTwo-Tower training. At training time, Two-Tower’s parameters are optimized for the\\nnext-sentence prediction task on unsupervised sequences of embeddings. The causal embeddings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='from the contextualizer are shifted by one position in the denoiser and a causal mask is used in its\\ncross-attention layers. A zero vector is prepended to the context vectors to enable the prediction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='of the first position in the sequence (see Figure 8). To train the model both conditionally and\\nunconditionally in preparation for inference with classifier-free guidance scaling, we drop random'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='rows from the cross-attention mask with a rate ofpcfg and denoise the corresponding positions with\\nonly the zero vector as context.\\n2.3.5 QuantizedLCM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='2.3.5 QuantizedLCM\\nTwo major approaches currently stand to deal with continuous data generation in the image or speech\\ngeneration fields: one is diffusion modeling, the other is learning quantization of the data before'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='modeling on top of these discrete units.\\nIn addition, the text modality remains discrete, and despite dealing with continuous representations\\nin the SONAR space, all possible text sentences (of less than a given number of characters) are a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='cloud of points rather than a real continuous distribution in the SONAR space. These considerations\\nmotivate the exploration of quantization of SONAR representations and then modeling on these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='discrete units to address the next sentence prediction task. Finally, following such an approach\\nenables the natural use of temperature, top-p or top-k sampling, to control the level of randomness'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='and diversity in the sampling of the next sentence representation.\\nIn this section, we learn residual quantizers for the SONAR space, and then build a Quantized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='Large Concept Modelbased on these discrete units. We tried to come up with an architecture\\nas close as the diffusionLCM models, to be able to compare approaches.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='Quantization of SONAR space. We use Residual Vector Quantization (RVQ; Zeghidour et al.\\n(2021)) as a coarse-to-fine quantization technique to discretize SONAR representations. Vector'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='quantization maps continuous input embeddings to the nearest entry in a learnt codebook. RVQ\\niteratively quantize residual errors from previous quantizations using additional codebook for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='iteration. We use FAISS implementation (Douze et al., 2024) which performs iterative k-means\\nclustering of residuals. We use the Improved Residual Vector Quantization (IRVQ) method from Liu'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='et al. (2015), with a beam size of 1 for memory efficiency. We trained the RVQ codebooks on 15\\nmillion English sentences extracted from Common Crawl usingncodebooks = 64number of quantizers\\nwith nunits-per-codebook = 8192units per codebook.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='One property of RVQ is that the cumulative sum of centroid embeddings of the first codebooks are\\nan intermediate coarse approximation of input SONAR vectors. In that way, we can report the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='evolution of auto-encoding BLEU scores with the increasing number of codebooks used to quantize\\nSONAR embeddings, before using the SONAR text decoder to decode quantized embeddings. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='notice in Figure 9 that auto-encoding BLEU consistently improves as the number of codebooks\\nincreases , reaching around 70% of the auto-encoding BLEU score achieved with continuous SONAR\\nembeddings, when using all 64 codebooks.\\n10 20 30 40 50 60'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='10 20 30 40 50 60\\nNumber of codebooks\\n0\\n20\\n40\\n60\\n80\\nAuto-Encoding BLEU score\\n Base decoder\\nFinetuned decoder\\nUnquantized topline\\nFigure 9 - Auto-encoding BLEU scores on FLORES devtest set, encoding sentences with SONAR encoder,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='quantizing with a varying number of codebooks, dequantizing and decoding with SONAR decoder.\\nFinetuning the SONAR decoder on quantized representations.We fine-tuned SONAR'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='decoder on quantized representations to adjust it for the space created by the quantizers on 1.2M\\nEnglishsentences. Tomakethedecodermorerobustagainstresidualrepresentationsfromintermediate\\ncodebooks, we randomly select a codebook numberk ∈\\n\\x022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='\\x022\\n3 · ncodebooks, ncodebooks\\n\\x03\\nduring fine-tuning,\\nwith probabilityp = 0.3, and use the quantized representation with codebooks up tok. Figure 9\\nshows the improvement in auto-encoding performance when the decoder is adapted to quantized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='representations.\\nQuant-LCM architecture. In the same spirit of diffusionLCM, we aim at coarse-to-fine\\ngeneration ofSONAR embeddings conditioned on left-context sentences. However, we do not follow'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='a denoising task as in diffusion modeling, but an iterative generation ofSONAR embeddings based on\\nintermediate quantized representationsinstead. In order to generate aSONAR embedding conditioned'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='on left-context sentences, theQuant-LCM model starts with theintermediate representationas\\na vector filled with zeros. We iteratively add to thisintermediate representation the predicted'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='residual centroid embeddings. In that way, the predictedSONAR embeddings are iteratively refined\\nbased on the growing cumulative sum of centroid embeddings of first codebooks, until all codebooks\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='have been seen. We used theOne-Tower architecture forQuant-LCM experiments even though\\nit could be trained withTwo-Tower architecture too. Compared to the diffusionLCM, noisy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='input representations are replaced withintermediate quantized representationsand diffusion timestep\\nembeddings as input are replaced by codebook index embeddings.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='Discrete targets. Following previous work on modeling discrete units from residual quantizers\\n(Wang et al., 2023; Rubenstein et al., 2023; Lee et al., 2022), a Quant-LCM can be trained to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='predict the unit from the next codebook, parameterized with a softmax output layer. For parameter\\nefficiency, we do not usencodebooks · nunits-per-codebook unique indices as discrete targets which would'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='imply ncodebooks · nunits-per-codebook output dimensions, but onlynunits-per-codebook output dimensions\\nwhile inputting the information of the codebook index to the model. At training time, similarly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='to diffusionLCM training, we randomly sample codebook indexk between 1 andncodebooks, and\\ncompute the cumulative sum of centroid embeddings of the firstk−1 codebooks as input. We use'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='the unit from codebookk of the target embedding as target index for cross entropy loss computation.\\nAt inference time, we iteratively predict the unit from the next codebook, get the corresponding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='centroid embedding and add it to the currentintermediate representationas additional predicted\\nresidual embedding. Finally, we also enable classifier-free guidance on logits at inference time (Gafni'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='et al., 2022) by randomly dropping left-context conditioning during training as previously described\\nin Section 2.3.3. This modeling approach with discrete targets is dubbedQuant-LCM-d in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='following sections. The improved SONAR decoder for quantized representations is used to bridge\\nthe compression gap coming from SONAR quantization in following ablation studies when using\\nQuant-LCM-d.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='Quant-LCM-d.\\nContinuous targets. We also explored a modeling approach that predicts continuous target\\nSONAR vectors based on left-context sentences and intermediate quantized representation of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='target vector, minimizing the Mean Squared Error between prediction and target embeddings. At\\ninference time, we can either iteratively add the closest centroid embedding based on the predicted'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='residual ˆr or sample a centroidci from the following distribution:\\np(ci|ˆr) = e−β·∥ci−ˆr∥2\\nP\\nk e−β·∥ck−ˆr∥2\\n, (23)\\nwhere β is a temperature hyper-parameter. This modeling approach with continuous targets is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='denoted withQuant-LCM-c in the following sections.\\n2.4 Ablations\\nIn this section, we delineate the ablations experiments conducted to evaluate the aforementionedLCM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='designs. We compare all the variants ofLCMs introduced above, namely,Base-LCM, One-Tower,\\nTwo-Tower and Quant-LCM.\\n2.4.1 Experimental setup\\nFor our ablation study and for the sake of reproducibility, we pre-train our models on theFineweb-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='edu dataset (Lozhkov et al., 2024). All models are configured to have approximately 1.6B trainable\\nparameters and are pre-trained on Meta’s Research Super Cluster (RSC, Lee and Sengupta (2022))'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='for 250k optimization steps spanning 32 A100 GPUs with a total batch size of 229k concepts.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='Models architectures. The Base-LCM has 32 layers and a model dimensiondmodel = 2048\\nwith 16 attention heads. It uses rotary position embeddings (RoPE, Su et al. (2024)), applies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='pre-normalization using RMSNorm (Zhang and Sennrich, 2019), uses the SwiGLU activation func-\\ntion (Shazeer, 2020) and is trained with a dropout rate of p=0.1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='The One-Tower diffusion LCM is made of 32 transformer blocks, each made of a self-attention\\nlayer with 32 attention heads and followed by a feed-forward neural network with inner size 8192.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='It has a dimensiondmodel of 2048 and uses learned position embeddings. The noise scheduler is set\\nwith T=100 diffusion timesteps. During training, self-attention is dropped with a probability of 0.15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='for unconditional training, enabling classifier-free guidance at inference time.\\nThe Two-Tower diffusion LCM has 5 layers in its contextualizer and 13 layers in its denoiser.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='Similar to theBase-LCM, it has 16 attention heads, a model dimensiondmodel = 2048, and uses\\nSwiGLU activations and RMSNorm in both contextualizer and denoiser. The contextualizer uses'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='RoPE for embedding positions whereas the denoiser is without positional embeddings. We use by\\ndefault the cosine noise schedule withT=100 and train with a dropout rate ofp=0.1. For training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='the model unconditionally we use a cross-attention mask dropout of rate 0.15 (see Section 2.3.4).\\nThe pre-training documents are wrapped at 128 sentences. Unless otherwise mentioned we decode'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='with S=40 sample steps with a guidance scalegscale = 3, a guidance rescaling factor ofgrescale = 0.7,\\nan initial noise scaleσinit = 0.6 and epsilon-scaling withλeps = 1.00045.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='The Quant-LCM follows exactly the same architecture as theOne-Tower diffusion LCM, except\\nfor Quant-LCM-d which differs only by its output dimension which is set tonunits-per-codebook = 8192'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='for softmax computation. For single sentence prediction tasks, we usetopk = 1and gscale = 2for\\nQuant-LCM-d and topk = 1with gscale = 3for Quant-LCM-c, while for multi-sentence generation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='tasks we used temperature of 1,topk = 3, gscale = 1, forQuant-LCM-d and temperature of 0.005,\\ntopk = 5, gscale = 1.5 for Quant-LCM-c, as higher guidance or lower temperature setups led to\\nrepeated generated sentences.\\n#Llama2 Tokens #Sentences Total'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='#Llama2 Tokens #Sentences Total\\nsentencesDataset #Docs Q1 Q2 Q3 Q1 Q2 Q3\\nROC-stories (dev) 2000 48 57 64 5 5 5 10K\\nROC-stories (test) 1871 50 56 62 5 5 5 9.4K\\nC4 (dev) 1000 136 288 577 6 12 24 20.6K\\nC4 (test) 1000 133 282 599 6 11 25 21.9K'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='C4 (test) 1000 133 282 599 6 11 25 21.9K\\nWikipedia-en (dev) 1000 146 332 736 5 10 23 21.1K\\nWikipedia-en (test) 1000 147 312 673 5 9 21 19.1K\\nGutenberg (dev) 55 10297 15934 22259 328 530 687 216.2K'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='Gutenberg (test) 61 10752 15204 23414 325 457 735 562.2K\\nTable 2- Statistics of the pre-training evaluation datasets.For each subset we report the number of documents,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='the total number of sentences and document lengths quartiles in sentences and inLlama2 tokens for\\nreference.\\nPre-training evaluation. Pre-trained token-level language models are typically evaluated with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='perplexity: a measure of how well each next token is predicted given a teacher-forced (i.e., ground\\ntruth) prefix of the document. In a similar spirit, we evaluate pre-trained LCMs in a teacher-forced'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='mode. But as they cannot produce the probability explicitly, we resort to a custom set of metrics of\\nthe quality of next sentence prediction.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='Each pre-trained model is initially evaluated on the quality of its predicted next sentenceˆxn given\\na ground truth contextx<n. Practically, for a given documentx1:N, we run the LCM inference in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='teacher-forcing mode and evaluate the following metrics:\\n• L2 distance(ℓ2). Euclidean distance in theSONAR space between the predicted embeddingˆxn\\nand the ground truth continuationxn: ℓ2 := ∥ˆxn − xn∥2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='• Round-trip L2 distance(ℓ2-r). Euclidean distance in theSONAR space between the re-encoded\\nsentence generated from the predicted embedding and the ground truth continuationxn,\\nℓ2-r := ∥encode(decode(ˆxn)) − xn∥2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='ℓ2-r := ∥encode(decode(ˆxn)) − xn∥2.\\nSince an LCM can predict an embedding outside of the distribution of real embeddings (obtained\\nby encoding natural sentences), theSONAR decoder might shift these embeddings to the nearest'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='plausible embeddings subspace. Theℓ2-r metric is introduced to capture the shift in embeddings\\nafter decoding them into text then re-embedding them again in theSONAR space. The more the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='generated embeddings are out-of-distribution, the higher the delta betweenℓ2-r and ℓ2 would be.\\n• Contrastive accuracy(CA). The ratio of embeddings in a batch that are further away (in terms of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='ℓ2) from the predicted embeddingˆxn than the ground truthxn (for eachn, we excludexn and its\\ntwo neighboring ground truth embeddings from the comparison). This metric naturally assigns'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='higher penalty for largeℓ2 values in the regions with high density of sentence embeddings.\\n• Paraphrasing (PAR). The maximum cosine similarity (CS) between the generated embeddingˆxn'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='and the context embeddingsx<n, normalized by the score of the ground truth sentence. Thus,\\nPAR = max\\nm<n\\nCS(ˆxn, xm)/ max\\nm<n\\nCS(xn, xm). The goal of this metric is to capture if the model is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='simply copying or paraphrasing a sentence from the context more (> 1) or less (< 1) than the\\nreference sentence.\\n• Mutual information(MI). This metric of text coherence evaluates the mutual information between'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='the next predicted sentenceˆsn = decode(ˆxn) and the previousk = 10ground truth sentences by\\ncomputing the difference between the unconditional perplexity ofˆsn and its perplexity conditioned\\non the prompt:\\nMI = 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='on the prompt:\\nMI = 1\\n|ˆsn| (log pLM(ˆsn) − log pLM(ˆsn|sn−k:n−1)) .\\nWe estimate the perplexity with a small language model, GPT-2 (Radford et al., 2019). We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='prepend a newline symbol toˆsn, so that a probability could be assigned to its first token, and we\\ncompute the average mutual information per-token by normalizing it with|ˆsn|, the length ofˆsn in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='tokens. When averagingMI over a dataset,|ˆsn| are used as weights.\\nPre-training evaluation data. The pre-training evaluation is performed on sampled subsets from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='four corpora covering different domains:ROC-stories (Mostafazadeh et al., 2016),C4 (Raffel et al.,\\n2019), Wikipedia-en (English Wikipedia dump) andGutenberg. We sample two distinct subsets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='(dev and test) from each corpus, we use the dev split for tuning inference hyper-parameters and\\nreport the results on the test splits. The statistics of the evaluation corpora are presented in Table 2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='The results of the pre-training evaluation are presented in Table 3.\\nFirst, diffusion-basedLCM and Quant-LCM variants have similarℓ2 and ℓ2-r scores despite an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='important difference in their learning objectives. The only model that shows substantially lowerℓ2\\nscore is theBase-LCM. This is expected sinceBase-LCM effectively optimizesℓ2 score during'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='training. Yet,ℓ2-r score is not improved compared to other models. This could be explained by the\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='Model ROC-stories C4\\nℓ2 ℓ2-r PAR CA MI ℓ2 ℓ2-r PAR CA MI\\nBase-LCM 0.177 0.237 1.847 72.4% 0.062 0.204 0.261 1.964 69.1% -0.105\\nOne-Tower 0.236 0.236 1.939 80.2% 0.977 0.279 0.273 2.239 77.1% 1.110'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='Two-Tower 0.233 0.231 2.088 80.6% 1.137 0.265 0.261 2.265 75.4% 1.134\\nQuant-LCM-c 0.236 0.237 1.683 76.0% 0.610 0.279 0.283 2.013 77.2% 0.715\\nQuant-LCM-d 0.240 0.246 1.871 81.1% 0.682 0.270 0.282 1.808 75.0% 0.359\\nModel Wikipedia-en Gutenberg'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='Model Wikipedia-en Gutenberg\\nℓ2 ℓ2-r PAR CA MI ℓ2 ℓ2-r PAR CA MI\\nBase-LCM 0.229 0.283 1.770 69.6% 0.071 0.207 0.264 1.780 67.8% -0.184\\nOne-Tower 0.324 0.311 2.087 80.9% 1.202 0.284 0.281 2.051 75.1% 0.725'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='Two-Tower 0.307 0.297 2.079 78.8% 1.307 0.267 0.267 2.077 73.0% 0.684\\nQuant-LCM-c 0.306 0.317 1.842 79.5% 0.744 0.269 0.281 1.774 72.1% 0.419\\nQuant-LCM-d 0.295 0.311 1.592 76.0% 0.323 0.276 0.290 1.599 72.0% 0.153'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='Table 3- Comparing architectures.Pre-training evaluation results on the four select corpora. For each subset,\\nwe reportℓ2 (L2 distance inSONAR space), ℓ2-r (round-trip L2 distance after decoding and re-encoding the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='generated embeddings),PAR (similarity to preceding embeddings) andCA (contrastive accuracy)\\nfact that when many plausible next sentence continuations are possible,Base-LCM generates their'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='average in SONAR space (instead of sampling one of plausible modes) which may not correspond to\\nany relevant point inSONAR space. This hypothesis is also highlighted by the poorBase-LCM\\nperformance in term ofCA and MI scores.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='performance in term ofCA and MI scores.\\nWe do not notice any significant difference inCA scores between diffusionLCMs and Quant-LCM\\nvariants. MI scores, on the contrary, are consistently higher for diffusion-based models compared'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='to Quant-LCM. At the same time, diffusionLCMs tend to paraphrase more the context in the\\ngenerated embeddings, which also correlates with an increasedMI score. Still, Quant-LCM variants'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='significantly outperformBase-LCM on MI metric. Now comparing the different variants,Quant-\\nLCM-c outperforms Quant-LCM-d modeling variant: one hypothesis is that predicting codebook'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='indices with cross-entropy loss is harder thanMSE objective whereQuant-LCM-c can more easily\\nlearn combination of left-context vectors for next sentence embedding.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='For diffusionLCMs, we don’t observe any consistent difference betweenOne-Tower and Two-\\nTower when looking across all metrics and datasets. Note that overall, to tackle the next sentence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='prediction task in the SONAR space, diffusion-based methods give clearly better results compared to\\nall other models.\\nInstruction-tuning evaluation. Subsequently, the pre-trained models are instruction-tuned on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='the stories subset ofCosmopedia (Ben Allal et al., 2024) and are evaluated on a held-out subset\\nof Cosmopedia itself. We aim with this finetuning to evaluate the ability of the models to follow\\ninstructions and generate consistent stories.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='instructions and generate consistent stories.\\nFor the sake of comparison, we trained a small Llama (Touvron et al., 2023) on the same training\\ndata (Fineweb-edu) and finetuned it onCosmopedia. This model has 24 transformer layers, each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='with 16 attention heads and a model dimension of 2048 for a total of 1.4B parameters. This model\\nwill be referred to assmaLlama.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='We evaluate the following metrics:\\n• ROUGE-L (R-L). ROUGE-L (F-measure) (Lin, 2004) between the generated and reference stories.\\n• Coherence (Coherence). This reference-free metric is computed with a bidirectional transformer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='model fine-tuned by Jwalapuram et al. (2022) to assign higher scores to positive “natural” documents\\nthan to negative examples with permuted sentences. For reporting, we normalize it with a sigmoid'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='(with a temperature 3.0, empirically set to make the scores of “certainly incoherent” documents\\nclose to 0 and those of “certainly coherent” documents close to 1).\\nModel R-L ↑ Coherence ↑\\nBase-LCM 23.69 0.482\\nOne-Tower 33.40 0.968'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='Base-LCM 23.69 0.482\\nOne-Tower 33.40 0.968\\nTwo-Tower 33.64 0.938\\nQuant-LCM-c 30.87 0.847\\nQuant-LCM-d 28.01 0.704\\nsmaLlama 34.88 0.984\\nTable 4- Comparing architectures.Instruction-tuning evaluation results. For each model we score the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='generated stories on the held-out test prompts and reportR-L (ROUGE-L) scores.\\nThe scores in terms ofR-L and Coherence of the finetuning evaluation are presented in Table 4.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='Those quantitative results are in line with the pretraining evaluation ones. BothR-L and Coherence\\nscores correlate with the model ordering based fromMI scores, mainlyQuant-LCM is outperformed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='by diffusion-based models, and both outperformBase-LCM by a large margin.\\nWe also note thatsmaLlama outperforms theLCMs on this downstream task on both metrics.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='It is well known thatLLMs produce very fluent outputs, that explains the higherRouge-L score.\\nWe also note that theOne-Tower and Two-Tower produce coherent outputs, on par with the\\nsmaLlama outputs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='smaLlama outputs.\\n2.4.2 Importance of the diffusion inference hyper-parameters\\nIn this section we study the effect of different inference hyper-parameters on the quality of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='generated text. To this end, we generate outputs for theC4 test split with theTwo-Tower LCM\\nmodel above, while varying the following hyper-parameters: the guidance scalegscale, the initial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='noise scaleσinit, and the number of inference sample stepsS. We score the generations following the\\nsame protocol above and report the results in Figure 10. We note that as we increase the guidance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='scale, the mutual information between the prefix and the generated suffix increases, and so does\\nparaphrasing as we are paying more attention to the conditioning context variables. In the opposite'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='direction of the mutual information, theℓ2 distance from the ground truth continuation increases as\\nthe model prefers to stay close to the prefix. Regarding the initial noise scale, we observe that values'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='between 0.5 and 0.7 achieve the bestMI score. In particular, the generated individual sentences\\nare usually longer with a higherσinit. The ℓ2 distance on the other hand does not reflect this trend.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='Lastly, we increase the number of inference steps and measure the mutual information (MI) of the\\ngenerated texts. With more inference steps, we can improve the prefix-suffix mutual information, but'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='there is diminishing returns to increasing the inference cost with little qualitative improvement.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='2 4 6\\nGuidance scale (gs)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nMutual information (MI)\\nMI\\nℓ2\\n0.4 0.6 0.8 1.0\\nInitial noise scale (σinit)\\n0.5\\n0.0\\n0.5\\n1.0\\n1.5\\nMutual information (MI)\\nMI\\nℓ2\\n20 40 60 80\\nInference sample steps (S)\\n0.7\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='0.7\\n0.8\\n0.9\\n1.0\\n1.1\\n1.2\\nMutual information (MI)\\nλeps=1.0025\\nλeps=1.00045\\nλeps=10.24\\n0.26\\n0.28\\n0.30\\nL2-distance (ℓ2)\\n0.24\\n0.26\\n0.28\\n0.30\\nL2-distance (ℓ2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='0.24\\n0.26\\n0.28\\n0.30\\nL2-distance (ℓ2)\\nFigure 10 - Importance of inference hyper-parameters.The first panel shows the quality of the generated\\noutput measured withMI and ℓ2 as we vary the guidance scalegscale with fixedσinit = 0.6 and S = 40. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='second panel varies the initial noise scaleσinit with fixed guidancegscale = 3and S= 40. The third panel\\nvaries the inference stepsS while holding the guidance scalegscale = 1.5 and σinit = 0.6 fixed. We consider 3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='values forλeps to see the impact of epsilon-scaling in the regime of large inference steps.\\n2.4.3 Studying the noise schedules\\nIn this section, we compare differentTwo-Tower diffusion LCMs trained with different noise\\nschedules, namely:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='schedules, namely:\\nCosine. Our default cosine noise schedule.\\nQuadratic. The first quadratic schedule (Quadratic-1) hasβ0 = 0.001 and βT = 0.0012, whereas\\nthe second quadratic schedule (Quadratic-2) hasβ0 = 0.02 and βT = 0.022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='Sigmoid. Four different sigmoid schedules with with(α, β) ∈ {(1.5, −1), (1.5, −2), (0.8, −1), (3.5, 0)}.\\nAll schedules are set with the defaultT=100. For the exact description of each noise schedule refer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='to Section 2.3.2 and Figure 5. We selected the quadratic schedule as a commonly used schedule for\\nreference with two configurations, Quadratic-1 closer to the cosine schedule and Quadratic-2 with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='more weight given to lower log-SNR. The selected sigmoid schedules withδ = 1.5 are configured\\nwith γ = −1 and γ = −2, γ = −2 being slightly shifted to the left on the log-SNR distributioni.e.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='more weight to lower log-SNR regimes. We then change theδ parameter of the sigmoid schedule to\\nchoose (δ = 0.8, γ= −1) for a peaked distribution of log-SNR around -1 and (δ = 3.5, γ= 0)for a\\nflat distribution over noise levels.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='flat distribution over noise levels.\\nWe follow the experimental setup described in Section 2.4.1 and report the results of the pre-training\\nevaluation in Table 5. Both quadratic schedules achieve a betterMI score while the wide sigmoid'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='schedule (δ, γ) = (3.5, 0) achieves the highest accuracyCA on both C4 and Wikipedia-en. To\\nfurther understand the differences between those schedules we re-evaluate onC4 while varying the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='guidance scalegscale. The results in Figure 11 confirm that the wide sigmoid schedule (δ = 3.5, γ= 0)\\nhas a much higher contrastive accuracy across the board (rightmost panel) while closely matching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='the cosine schedule in terms of mutual information (MI). This schedule being trained on a wider\\nspectrum of log-SNR learns to contrast more than to regress. Contrast this behavior to the peaked'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='sigmoid schedule (δ = 0.8, γ= −1) where the model focuses on regressing to the target (lowerℓ2),\\nakin to aBase-LCM, resulting in a lower contrastive accuracy.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='Model C4 Wikipedia-en\\nℓ2 ℓ2-r PAR CA MI ℓ2 ℓ2-r PAR CA MI\\nCosine 0.265 0.261 2.265 75.4% 1.134 0.307 0.297 2.079 78.8% 1.307\\nQuadratic-1 0.268 0.264 2.341 75.7% 1.252 0.309 0.300 2.202 79.1% 1.409'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='Quadratic-2 0.270 0.265 2.320 76.2% 1.252 0.312 0.303 2.185 79.7% 1.399\\nSigmoid(1.5, -1) 0.257 0.259 2.226 74% 1.083 0.298 0.292 2.110 77% 1.271\\nSigmoid(1.5, -2) 0.277 0.267 2.291 77.2% 1.173 0.321 0.303 2.179 80.3% 1.308'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='Sigmoid(0.8, -1) 0.252 0.255 2.053 70.6% 0.936 0.285 0.283 1.883 71.7% 1.127\\nSigmoid(3.5, 0) 0.307 0.265 2.347 80.3% 1.154 0.347 0.303 2.187 83.7% 1.288\\nTable 5- Comparing noise schedules.Results of the pre-training evaluation on two corpora,C4 and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='Wikipedia-en.\\n2 4 6\\nGuidance scale (gs)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\nMutual information (MI)\\nCosine\\nQuadratic-1\\nQuadratic-2\\nSigmoid(3.5)\\nSigmoid(0.8)\\n2 4 6\\nGuidance scale (gs)\\n0.24\\n0.26\\n0.28\\n0.30\\n0.32\\n0.34\\nL2 distance (ℓ2)\\n2 4 6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='0.26\\n0.28\\n0.30\\n0.32\\n0.34\\nL2 distance (ℓ2)\\n2 4 6\\nGuidance scale (gs)\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\nContrastive accuracy (AC)\\nFigure 11 - Comparing noise schedules.The prefix-suffix mutual information (MI), theℓ2 distance and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='contrastive accuracy (CA) scores of evaluatedC4 documents while varying the guidance scale(gscale) under\\ndifferent schedules.\\n2.4.4 Studying the loss weighting strategies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='2.4.4 Studying the loss weighting strategies\\nIn this section we compare the baselineTwo-Tower diffusion LCM trained with the simplified\\nobjective (i.e., ω(t) = 1, ∀t)) to models trained with the clamped-SNR weighting strategy. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='consider two sets of(λmin, λmax): (λmin, λmax) = (0, 10) and (λmin, λmax) = (0.001, 5). All models in\\nthis section are trained with the cosine noise schedule.\\nFragility as a sample weighing strategyAs introduced in Equation (19), we train in this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='section aTwo-Tower LCMmodel with loss terms weighted by the sample’s fragility.\\nω(x0) = sigmoid(a fragility(x0) +b) (24)\\nGiven that estimating the fragility score of each sample as defined in Section 2.5.2 can be very'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='costly, we resort to training a simple MLP (3-layers) on 50M sampled sentences to approximate these\\nfragility scores. This model is referred to as F. Henceforth, the sample weight is:\\nω(x0) = sigmoid(a F(x) +b) (25)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='ω(x0) = sigmoid(a F(x) +b) (25)\\nThe two hyper-parameters (a, b) are chosen so that extremely fragile sentences contribute less to\\nthe loss (ω(x0) ≈ 0), and so that sample weights should increase smoothly as sample robustness\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='improves. Figure 12 plots the sample weights distribution evaluated on a pre-training dataset with\\na = −4 and b = 3.5.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nSample Weight\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDensity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='Sample Weight\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDensity\\nFigure 12 - Resulting distribution of fragility sample weightsω(x0) with ω(x0) = sigmoid(−4 F(x) + 3.5).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='We follow the experimental setup described in Section 2.4.1 and report the results of the pre-training\\nevaluation in Table 6. We observe that weighting with clamped SNRs does not improve the quality'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='of generated texts as measured with our pre-training evaluation metrics. When it comes to the\\nfragility-aware weighting strategy, we observe an improvement in the contrastive accuracy of the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='model. In the remainder of this work we default to the simplified training objective (ω(t) = 1, ∀t).\\nModel C4 Wikipedia-en\\nℓ2 ℓ2-r PAR CA MI ℓ2 ℓ2-r PAR CA MI\\nBaseline ω(t) = 1 0.265 0.261 2.265 75.4% 1.134 0.307 0.297 2.079 78.8% 1.307'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='SNR (0,10) 0.280 0.264 2.334 74.8% 1.107 0.320 0.296 2.102 77.9% 1.212\\nSNR (0.001,5) 0.266 0.261 2.269 73.4% 1.094 0.304 0.291 2.007 76.6% 1.295\\nFragility 0.2815 0.273 2.306 76.5% 1.103 0.321 0.308 2.118 80.7% 1.193'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='Table 6- Comparing weighting strategies.Results of the pre-training evaluation on two corpora,C4 and\\nWikipedia-en.\\n2.5 Analysis\\n2.5.1 Inference efficiency ofLCMs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='2.5 Analysis\\n2.5.1 Inference efficiency ofLCMs\\nWe compare in this section the inference computational cost of theTwo-Tower LCMto that of a\\nvanilla LLM as a function of the total length in tokens of the prompt and output combined. We chose'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='the theoretical number of FLOPs, independent of any specific optimization. These optimizations are\\ngeneric to the transformer architecture and also apply to ourLCM. We include in this comparison'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='two configurations ofLCMs; the 1.6B used in the previous ablation studies and a 7B model we\\nscale up to in the following sections. For bothLCMs, we estimate the inference cost with inference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='sample stepsS = 40. Given the quadratic complexity of the attention mechanism in transformers, the\\ncomplexity sharply increases with the context size (see upper right corner of Figure 13’s left panel).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='The complexity of theLCM depends on how the context is sentencized: a context length of 200\\ntokens split into 10 sentences (20 tokens each) will incur a higher cost than the same 200 tokens split\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='0 10000 20000 30000 40000 50000 60000\\nTotal context size in tokens\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\nInference Flops\\n×1015\\nLLama2-7b\\nTwo-tower LCM (7B)\\nTwo-tower LCM (1.6B)\\n0 500 1000 1500 2000 2500 3000\\nTotal context size in tokens\\n0.0\\n0.2\\n0.4\\n0.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='Total context size in tokens\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n×1014\\nLLama2-7b\\nTwo-tower LCM (7B)\\nTwo-tower LCM (1.6B)\\nFigure 13 - Theoretical inference Flops ofLCMs and LLLms. We evaluate the inference flops for different text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='lengths (inLlama2 tokens) with a variable average sentence length. Only extremely short sentences (≤ 10\\ntokens) favor LLMs.\\ninto 5 sentences (40 tokens each). We account for this by computing the cost on a range of sentence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='lengths but report the total context size on the x-axis (context size = sentence length× number\\nof sentences). TheLCM shows substantially better scalability with respect to increasing context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='size. The inference computational cost of theLCM includes the three steps of (1) encoding into\\nSONAR, (2)LCM prediction in the sentence space then (3) decoding with aSONAR decoder. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='inference cost ofLCMs varies significantly depending on the average length in tokens per sentence.\\nFor extremely short sentences (less than 10 tokens), anLLM is more computationally efficient (see\\nlower left corner of Figure 13’s right panel).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='lower left corner of Figure 13’s right panel).\\n2.5.2 Fragility of SONAR space\\nWhen we perform modeling in a latent space, we primarily rely on the induced geometry (L2-distance).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='However, the homogeneous Euclidean geometry of any latent representation will not perfectly match\\nthe underlying text semantics. This is evidenced by the fact that a small perturbation in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='embedding space may result in a drastic loss of semantic information after decoding. We dub such\\nembeddings “fragile”. For this reason, we aim to quantify the fragility of semantic embeddings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='(namely SONAR codes) to understand the quality of theLCM training data and how this fragility\\ncan hinder theLCM training dynamics.\\nGiven a text fragmentw and itsSONAR code x = encode(w), we define the fragility ofw as:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='fragility(w) := −Eα∼U([0,1]), ϵ∼N(0,I) [score(w, wα,ϵ)] , (26)\\nxα,ϵ = denormalize\\n\\x00√\\n1 − α normalize(x) +√α ϵ\\n\\x01\\n, (27)\\nwα,ϵ = decode(xα,ϵ), (28)\\nwhere normalize and denormalize are the normalization and denormalization operators introduced'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='in Equation (4) with the goal of makingx’s coordinates scale-independent. The “encode“ operation\\nmaps text fragments intoSONAR space, and the “decode“ operation produces a text fragment from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='a given vector in theSONAR space. For eachα in [0, 1], xα,ϵ is the perturbed version ofx where a\\nnoise vector of varianceα is linearly combined withx. The perturbed vector is then decoded into a\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='text fragmentwα,ϵ. This perturbation is similar to the variance-preserving noising used in diffusion\\nLCMs (see Section 2.3.2).\\nThe “score” operator in Equation (26) is set to be a semantic similarity metric comparing the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='perturbed textwα,ϵ to the originalw. We considered the following options:\\n• Auto-Encoding BLEU. score(w, wα,ϵ) =BLEU(w, wα,ϵ).\\n• External cosine similarity.Provided an external text encoder (read unrelated toSONAR) that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='encodes a text fragmentw into encodeext(w) score(w, wα,ϵ) =CS(encodeext(w), encodeext(wα,ϵ)),\\nwhere CS is the cosine similarity measure. Compared to Auto-EncodingBLEU, this method is\\ntypically more robust to paraphrasing.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='typically more robust to paraphrasing.\\nFinetuned robust decoder. To serve as a testbed for our fragility analysis, a newSONAR\\ndecoder for English text is finetuned on a sample of our pre-training data. In order to improve the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='decoder’s robustness to imperfectly generated embeddings from theLCM, we follow Equation (27)\\nand add random noise vectors toSONAR embeddings during training. As reported in Table 7, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='finetuned SONAR decoder exhibits stronger performance across a range of public corpora.\\nModel Flores CNN DailyMail Gutenberg C4\\nBase SONARdecoder 79.5 75.9 70.5 75.7\\nFinetuned SONARdecoder 88.0 87.6 85.6 87.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='Finetuned SONARdecoder 88.0 87.6 85.6 87.5\\nTable 7- Comparing SONAR decoders.Raw reconstruction performance of our baseSONAR decoder vs. the\\nnew decoder trained with noised embeddings. Scores are Auto-Encoding BLEU on random subsets of 10k'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='sentences from each dataset, except forFlores where we use the entire dev split.\\nFragility study. We sample 50M random text fragments, and for each sample we generate 9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='perturbations corresponding to different noise levelsα ∈ [0.1, 0.2, . . . ,0.9]. For the external cosine\\nsimilarity metric we usemGTE as external encoder (Zhang et al., 2024).\\n0 50 100 150 200 250\\nSentence Length in Characters\\n0.20\\n0.25\\n0.30\\n0.35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='Sentence Length in Characters\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\n0.45\\n0.50\\n0.55\\nAuto-Encoding BLEU Score\\n0.0 0.2 0.4 0.6 0.8 1.0\\nα\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\nAuto-Encoding BLEU Score\\nBLEU: Base decoder\\nBLEU: Finetuned decoder\\nCosSim: Base decoder'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='BLEU: Finetuned decoder\\nCosSim: Base decoder\\nCosSim: Finetuned decoder\\n0.72\\n0.74\\n0.76\\n0.78\\n0.80\\n0.82\\n0.84\\n0.86\\nCosine Similarity\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0\\nCosine Similarity\\nBLEU: Base decoder\\nBLEU: Finetuned decoder\\nCosSim: Base decoder'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='BLEU: Finetuned decoder\\nCosSim: Base decoder\\nCosSim: Finetuned decoder\\nFigure 14 - Fragility scores.Auto-Encoding BLEU and external cosine similarity. In the left-hand panel as a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='function of the text length (α-averaged) and in the right-hand panel as a function of the noise varianceα.\\nWe depict in the right panel of Figure 14 the curves of both score functions with respect to the noise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='level α. We observe thatBLEU scores decrease faster than the cosine similarity. Most importantly,\\nfragility scores are sensitive to the choice of the decoder. In particular, both Auto-Encoding BLEU\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='0.0 0.2 0.4 0.6 0.8 1.0\\nScore value\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDensity\\nScore\\nBLEU: Base decoder\\nBLEU: Finetuned decoder\\nCosSim: Base decoder\\nCosSim: Finetuned decoder'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='CosSim: Base decoder\\nCosSim: Finetuned decoder\\nFigure 15 - Auto-Encoding BLEU scores and cosine similarity (α-averaged) distributions.\\nand cosine similarity scores decrease at a markedly slower rate for theFinetuned decoder than for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='the Base one as the amount of noise increases. We note also that the overall score distribution (after\\naveraging over allα), shown in Figure 15, exhibits a large spread of fragility scores acrossSONAR\\nsamples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='samples.\\nOne factor that can explain such a discrepancy is the text length. Compared to the Auto-Encoding\\nBLEU metric (which drops only by 1–2% for long sentences), fragility is more sensitive to the length'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='of sentences and drops faster for both similarity metrics. This shows that using a max sentence\\nlength over250 can be extremely challenging forSONAR and theLCM model. On the other hand,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='even if short sentences are on average more robust, splitting a long sentence in the wrong place may\\nresult in shorter but more fragile sub-sentences.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='result in shorter but more fragile sub-sentences.\\nTaking a closer look at the 5% most fragile embeddings, we notice that they are very noisy. Typically,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='they correspond to hyperlinks, references, unique ids, code-switched or numerical entries. These are\\nlikely artifacts that theSONAR models were not exposed to during training or where theSONAR'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='tokenizer fails. Fragility can thus be used to filter out hard samples from the training data. We also\\nobserve that short but complex technical phrases can be more fragile than common language phrases\\nof similar length.\\n3 Scaling the model to 7B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='of similar length.\\n3 Scaling the model to 7B\\nThis section describes our effort to scale our model to 7B parameters and compare the performance\\nagainst other approaches such as token-based LLMs on more challenging tasks such as summarization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='and summarization expansion (detailed in Section 3.1).\\nBased on the results in Section 2.4 where the two diffusion-basedLCMs (One-Tower and Two-\\nTower) outperform the other variants, we decided to scale a diffusion model to 7B parameters. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='chose to scaleTwo-Tower given its smaller memory footprint, particularly when processing long\\ncontexts with a shallower contextualizer tower\\nThe large 7BTwo-Tower diffusion LCM has 5 layers in its contextualizer and 14 layers in its'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='denoiser. Its dimension has been extended todmodel=4096. Each self-attention layer has 32 attention\\nheads. All other parameters are kept the same as for the 1.6BTwo-Tower model. The model is\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='pre-trained on a dataset of 2.3B documents, representing 2.7T tokens and 142.4B concepts/sentences.\\nWe pre-trained this model on Meta’s RSC for 124k optimization steps spanning 256 A100 GPUs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='with a total batch size of 1M concepts. We further extend the context length of this model to cover\\n2048 concepts instead of the 128 concepts in the ablation experiments. We trained using the AdamW'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='optimizer with(β1, β2) = (0.9, 0.95), ϵ = 1e-5 and a weight decay of 0.1. We use a cosine learning\\nrate schedule, with warm-up of 10,000 steps up toLR = 3e-4. To improve training stability we clip'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='gradients at a maximum norm ofg = 10. We subsequently finetune the 7BTwo-Tower LCMon\\npublicly available instruction tuning datasets following Chung et al. (2024). Each sample consists of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='a prompt and an answer and we back-propagate on answer sentences only. Each answer sequence\\nis suffixed with the phrase “End of response.” to teach the model when to stop generating. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='finetuning data totals 389M sentences, of which 53M are answers (i.e., targets). For supervised\\nfinetuning, we use a cosine learning rate schedule with an initial rate ofLR = 3e-5 and finetune the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='model for 7 epochs with a batch size of 262K sentences (prompts and answers combined). We will\\nrefer to the pre-trained model asTwo-Tower-7B and the finetuned model asTwo-Tower-7B-IT.\\n3.1 Evaluation Tasks and Data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='3.1 Evaluation Tasks and Data\\nThis section describes the tasks on which we are evaluating and benchmarking our proposed model.\\nWe detail datasets, baselines and metrics. For each task, the dataset was processed with the same'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='sentence splitter andSONAR encoder as used in theLCM training.\\n3.1.1 Metrics\\nAs longform text generation is the main challenge forLCM, our benchmarking is mainly focused'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='on generative tasks, which are notoriously more difficult to evaluate automatically. Therefore,\\nwe evaluate them with multiple automatic metrics, chosen to focus on complementary aspects on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='generation quality. All metrics used in this section are summarized in Table 8.\\nFor summarization and summary expansion (defined below), we report the traditional reference-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='Rouge-L metric (Lin, 2004). As summarization models have a tendency to copy content from the\\nsource or from its own generated prefix, we report two additional word-based metrics. To evaluate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='how much content is directly copied from the source, we report the proportion of word 3-grams of\\nthe source that are present in the output (OVL-3). To evaluate repetitiveness of the generated texts,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='we report the portion of duplicated word 4-grams in the output (REP-4).\\nTo complement word-based metrics with summarization-focused neural evaluation, we use two metrics'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='introduced by Clark et al. (2023): average probabilities of the SEAHORSE classifiers for Q4 (whether\\nall the information in the summary is fully attributable to the source), denoted asSH-4 in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='following and Q5 (whether the summary captures the main ideas of the source), denoted asSH-5.\\nAs a metric of the overall fluency of the generated sentences, we report an average probability that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='the sentence is linguistically acceptable, as predicted by a classifier trained by Krishna et al. (2020)\\non theCoLA dataset (Warstadt et al., 2019), further referred to asCoLA. To evaluate the local'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='coherence of the generated text, we report the average cosine similarity between eachn’th and\\nn + 2’th sentence (Parola et al., 2023).\\n3.1.2 Summarization\\nTask and datasets. When considering a relatively long document, a summarization task can be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='described as the act of generating a much shorter corresponding document that includes the essential\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='Task Area Metric Description Reference\\nSummarization Target similarity R-L ROUGE-L Lin (2004)\\nSource similarity OVL-3 N-grams overlap (N=3)\\nGrammaticality REP-4 Portion of duplicated N-grams (N=4) Welleck et al. (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='Fluency CoLA Sentence fluency classifier score Krishna et al. (2020)\\nAttribution SH-4 Seahorse-Large-Q4 score Clark et al. (2023)\\nSemantic coverage SH-5 Seahorse-Large-Q5 coverage score Clark et al. (2023)\\nSummary\\nExpansion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='Summary\\nExpansion\\nGrammaticality REP-4 (see above) Welleck et al. (2019)\\nFluency CoLA (see above) Krishna et al. (2020)\\nTable 8- Summary of automatic metrics used in different tasks in Section 3.1. Order mostly follows paper’s\\nnarrative.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='narrative.\\ninformation contained in the long document and the same logical structure linking the various pieces\\nof essential information.\\nSummarization techniques can range from more extractive to more abstractive. Extractive techniques'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='attempt to preserve in the summary the same vocabulary as that found in the long document,\\nthereby shortening the long by removing details and superfluous wording. Abstractive techniques, on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='the other hand, attempt to produce the summary by rephrasing the essential pieces of information\\nfound in the long document. Our work focuses more on abstractive summarization, as such type of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='summarization cannot be performed without some form of understanding and reasoning.\\nWe use theCNN DailyMail(Hermann et al., 2015) andXSum (Narayan et al., 2018) datasets. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='also report results on the challengingLCFO corpus which takes long documents as input, approx.\\n5k words (Costa-jussà et al., 2024). The task is to provide abstractive summaries with lengths'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='representing 20%, 10%, and 5% of the input document. Detailed statistics are provided in Table 9.\\n#Llama2 Tokens #Sentences\\nDataset #Docs Q1 Q2 Q3 Q1 Q2 Q3\\nCNN DailyMail 11.5k 605/61 892/78 1266/97 10/3 14/4 21/4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='XSum 11.3k 273/25 445/30 735/35 25/1 30/1 35/1\\nLCFO.5% 249 6559/341 7214/378 7916/418 209/12 295/15 527/18\\nLCFO.10% 249 6559/654 7214/718 7916/796 209/22 295/27 527/32\\nLCFO.20% 249 6559/1276 7214/1403 7916/1524 209/41 295/48 527/59'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='Table 9- Statistics of the test split of evaluation benchmarks. For each subset we report the number of\\ndocuments and statistics of document and summary length in terms of sentences andLlama2 tokens. Each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='table cell shows “document/summary” length quartiles.\\nBaselines. For CNN DailyMailand XSum, we compare against several baselines of different\\narchitectures (encoder-decoder transformer, decoder-only LLMs) that are known to perform well'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='on summarization tasks. For encoder-decoder transformer models, we use T5 (Raffel et al., 2020).\\nFor decoder-only LLMs, we chooseGemma-7B, Llama-3.1-8B andMistral-7B-v0.3. We chose'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='the published instruction-tuned models to compare with theLCM with the same training regime,\\nand have a similar size (7B). Note that while T5 has much smaller sizes than theLCM, this is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='compensated by using models that are fine-tuned explicitly on the target evaluation dataset.\\nSummarization results. Table 10 contains the results of different baselines and ourLCM model for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='summarization (CNN DailyMailand XSum). We can notice that theLCM produces competitive\\nRouge-L scores when compared to a specifically tunedLLM (T5-3B) and even surpasses the\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='Model Paradigm CNN DailyMail\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGround truth — 100.00 0.170 0.684 0.850 0.683 0.586\\nT5-3B SFT 37.56 0.174 0.854 0.946 0.773 0.503\\nGemma-7B-IT IFT 31.14 0.245 1.032 0.963 0.740 0.560'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='Mistral-7B-v0.3-IT IFT 36.06 0.200 0.780 0.972 0.780 0.676\\nLlama-3.1-8B-IT IFT 34.97 0.248 0.928 0.973 0.763 0.692\\nTwo-Tower-7B-IT IFT 36.47 0.177 0.757 0.767 0.723 0.459\\nModel Paradigm XSum\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='Ground truth — 100.00 0.108 0.399 0.987 0.352 0.418\\nT5-3B — 17.11 0.221 0.671 0.939 0.680 0.450\\nGemma-7B-IT IFT 18.20 0.177 0.620 0.769 0.546 0.446\\nMistral-7B-v0.3-IT IFT 21.22 0.162 0.480 0.922 0.633 0.621'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='Llama-3.1-8B-IT IFT 20.35 0.186 0.501 0.941 0.687 0.658\\nTwo-Tower-7B-IT IFT 23.71 0.106 0.464 0.683 0.358 0.284\\nTable 10- Performance on theCNN DailyMailand XSum summarization tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='instruct-finetuned LLMs. Our model tends to generate more abstractive summaries rather than\\nextractive ones, as shown by the lowerOVL-3 scores. TheLCM produces fewer repetitions compared'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='to LLMs, and more importantly, the repetition rate is closer to the ground truth one. TheLCM\\ngenerates globally less fluent summaries according toCoLA classifier. However, we can remark that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='even the human generated ground truth gets a lower score compared to theLLM. A similar behavior\\nis observed for the source attribution (SH-4) and semantic coverage (SH-5). This may be explained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='by model-based metrics that are more biased towardsLLM generated content.\\nLong-context summarization results. Table 11 presents the results for long-context summariza-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='tion (LCFO.5%, LCFO.10% and LCFO.20%). This is a challenging task for most of the models. For\\nexample, Mistral-7B-v0.3-IT seems to be unable to follow the length instruction of the summary–it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='always generates summaries which length is about 50% of the source.Mistral-7B-v0.3-IT also\\nhas the highestSH-4 score, i.e., source attribution. The summaries generated byGemma-7B-IT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='tend to be longer than requested, whileLlama-3.1-8B-IT generates summaries which length is the\\nclosest to the requested size.\\nThe LCM has only seen a limited amount of long documents in the pretraining and fine-tuning data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='Nevertheless, it performs well for this task. It outperformsMistral-7B-v0.3-IT and Gemma-7B-IT\\nin the metricRouge-L for the 5 and 10% conditions, and is close toGemma-7B-IT for the 20%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='condition. We also observe that the LCM yields high SH-5 scores for all conditions, i.e., the\\nsummaries can be attributed to the source.\\nFinally, we observe thatLlama-3.1-8B-IT performs substantially better than the otherLLMs,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='according toRouge-L, while allLLMs have similar performance on theCNN DailyMailand XSum\\nsummarization tasks. This could be explained by training data contamination forLlama-3.1-8B-IT,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='or by the fact that the other twoLLMs struggle to handle the long input context.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='Method WR LCFO.5%\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGemma-7B-IT 0.107 25.21 0.151 4.711 0.688 0.357 0.174\\nMistral-7B-v0.3-IT 0.512 21.36 0.532 5.997 0.854 0.656 0.296\\nLlama-3.1-8B-IT 0.076 37.67 0.190 2.767 0.931 0.488 0.314'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='Two-Tower-7B-IT 0.060 26.88 0.162 2.473 0.796 0.628 0.196\\nLCFO.10%\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGemma-7B-IT 0.150 29.25 0.164 6.427 0.667 0.377 0.194\\nMistral-7B-v0.3-IT 0.549 25.00 0.537 6.289 0.848 0.660 0.306'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='Llama-3.1-8B-IT 0.128 42.85 0.243 3.804 0.907 0.486 0.310\\nTwo-Tower-7B-IT 0.089 29.38 0.202 3.00 0.791 0.623 0.183\\nLCFO.20%\\nR-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑) SH-4 (↑) SH-5 (↑)\\nGemma-7B-IT 0.257 33.32 0.201 9.188 0.603 0.425 0.239'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='Mistral-7B-v0.3-IT 0.493 28.82 0.527 5.806 0.858 0.658 0.293\\nLlama-3.1-8B-IT 0.179 46.92 0.272 4.783 0.888 0.485 0.315\\nTwo-Tower-7B-IT 0.140 31.74 0.253 3.664 0.779 0.613 0.187'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='Table 11- Performance on the long-context summarization task of LCFO. WR is the word count ratio\\nbetween the generated text and the source document.\\n4 Large Concept ModelExtensions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='4 Large Concept ModelExtensions\\nIn this section, we explore several extension of theLarge Concept Model. First, we evaluate\\nthe LCM on the new task of summary expansion,i.e., given a summary, create a longer text. We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='then showcase the good zero-shot generalization performance of theLCM. Finally, we explore an\\napproach to add higher level information beyond sentences.\\n4.1 Summary Expansion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='4.1 Summary Expansion\\nTask and datasets. When considering a short and concise document that has similar properties\\nto those of a summary (i.e., mainly a stand-alone document that abstracts from details), a summary'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='expansion task can be described as the act of generating a much longer document that preserves\\nthe essential elements found in the corresponding short document, as well as the logical structure'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='that connects such elements. As this is a more freely generative task, an additional requirement\\nto be taken into consideration is that of coherence (for example, the detailed information included'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='in one generated sentence should not contradict that included in another sentence). The summary\\nexpansion task presented here consists in taking summaries as inputs, fromCNN DailyMailand'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='XSum, and generating a long document. Note that the goal is not to recreate the factual information\\nof the initial document rather than evaluating the capability of the model to extend the input text in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='a meaningful and fluent way. We use similar baselines and metrics as in the previous section 3.1.2.\\nResults. Table 12 shows the results of the summary expansion forCNN DailyMailand XSum.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='First of all, regarding the word count ratio, we can see different behaviours for the two corpora.\\nFor CNN DailyMail, the models tend to generate texts that are 6 times larger than the input.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='Llama-3.1-8B-IT produces even longer outputs (factor 8 instead of 6). But forXSum, while\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='CNN DailyMail\\nMethod WR R-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑)\\nGemma-7B-IT 6.8 35.54 0.801 2.104 0.951\\nMistral-7B-v0.3-IT 6.4 34.24 0.817 2.063 0.959\\nLlama-3.1-8B-IT 8.5 37.76 0.822 2.582 0.844\\nTwo-Tower-7B-IT 6.3 30.85 0.726 2.911 0.474\\nXSum'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='Two-Tower-7B-IT 6.3 30.85 0.726 2.911 0.474\\nXSum\\nMethod WR R-L(↑) OVL-3 (↑) REP-4 (↓) CoLA (↑)\\nGemma-7B-IT 19.5 17.89 0.963 10.238 0.116\\nMistral-7B-v0.3-IT 1.6 29.31 0.893 2.268 0.939\\nLlama-3.1-8B-IT 19.8 28.84 0.915 2.543 0.898'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='Llama-3.1-8B-IT 19.8 28.84 0.915 2.543 0.898\\nTwo-Tower-7B-IT 7.1 23.82 0.561 1.542 0.603\\nTable 12- Performance on the summary expansion tasks ofCNN DailyMailand XSum, evaluated with the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='metrics described in Table 8. WR is the word count ratio between the hypothesis and the source summary.\\nGemma-7B-IT and Llama-3.1-8B-IT generate very long texts (almost 20 times longer than the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='prompt), theLCM generates output of approximately the same length ratio as forCNN DailyMail.\\nOnly Mistral-7B-v0.3-IT fails to generate long outputs for this corpus.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='Then, we clearly see a different trend compared to the summarization task. TheLLMs get higher\\nRouge-L scores compared to theLCM. As mentioned above, the goal of this task is not to recreate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='the original document. However, theR-L score tells us how much of the content of the full document\\ncan be recreated. Contrary to theLLMs, our model tends to generate different sentences compared'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='to the original document from which the summary has been created (with an exception forGemma-\\n7B-IT on XSum). This is expected since our model generates embeddings that are then processed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='by a decoder trained on a translation task that tends to paraphrase the initial content. However, the\\nCoLA results show that this comes along with lower fluency, especially forCNN DailyMail.\\n4.2 Zero-shot generalization performance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='4.2 Zero-shot generalization performance\\nSONAR is a semantic space that can represent 200 languages. In this paper, all experiments\\npresented so far have been done on English text. In this section, we explore the capability of our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='proposed LCM approach to process other languages in a zero-shot fashion by leveragingSONAR’s\\nability to represent multilingual data.\\nWe use theXLSum (Hasan et al., 2021) corpus, a large scale multilingual abstractive news summa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='rization benchmark covering 45 languages. We score model outputs using the multilingual rouge\\nscoring scripts released with the benchmark.5 Note thatRouge-L scores are heavily dependent on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='language-specific text tokenization and stemming. Unless provided in the aforementioned scripts, we\\ntokenize the model outputs and references with the default tokenization from Lin (2004). Languages'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='like Korean, Telugu and Tamil can benefit from a more appropriate stemming and tokenization.\\nWe compare the LCM performance withLlama-3.1-8B-IT which officially supports eight languages:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. According to The Llama3\\nteam (2024), the model has seen many additional languages during pretraining, but was instruction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='5https://github.com/csebuetnlp/xl-sum/tree/master/multilingual_rouge_scoring\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='Vietnamese\\nEnglish\\nPashto\\nSwahili\\nBurmese\\nFrench\\nHausa\\nUrdu\\nHindi\\nIndonesian\\nPortuguese\\nArabic\\nSpanish\\nWelsh\\nSomali\\nTurkish\\nRussian\\nPersian\\nJapanese\\nIgbo\\nChinese simplified\\nKirundi\\nThai\\nTigrinya\\nSerbian Cyrillic\\nScottish Gaelic\\nChinese traditional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='Scottish Gaelic\\nChinese traditional\\nYoruba\\nUkrainian\\nAzerbaijani\\nAmharic\\nMarathi\\nTamil\\nSinhala\\nOromo\\nTelugu\\nKorean\\nPunjabi\\nBengali\\nGujarati\\nKyrgyz\\nNepali\\n0\\n10\\n20\\n30\\nXLSUM Rouge-L\\nTwo-Tower-7B-IT Llama-3.1-8B-IT Officially supported Llama3 language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='Figure 16 - Rouge-L scores onXLSum for Llama-3.1-8B-IT and Two-Tower-7B-IT.\\nfinetuned on those eight languages only. TheLCM, on the other hand, has never seen any language\\nother than English, and we do not use the EnglishXLSum training data either.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='We reportRouge-L scores for 42 languages in Figure 16. Three languages were excluded since they\\nare currently not supported bySONAR: Pidgin, Serbian in Latin script and Uzbek in Cyrillic script.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='The LCM substantially outperformsLlama-3.1-8B-IT on English (23.5 compared to 20.7Rouge-L)\\nand on the average over all six languages officially supported by both models and included inXLSum'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='(20.2 versus 19.7Rouge-L).6 We also observe that theLCM generalizes very well to many other\\nlanguages, in particular low-resource languages like Southern Pashto, Burmese, Hausa or Welsch'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='which all haveRouge-L scores greater than 20. Other well performing low-resource languages are\\nSomali, Igbo or Kirundi. Finally, theLCM obtains aRouge-L score of 30.4 on Vietnamese. Overall,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='these results highlight the impressive zero-shot generalization performance of theLCM to languages\\nit has never seen.\\n4.3 Exploring explicit planning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='4.3 Exploring explicit planning\\nWhen writing long-form text, it is often practical to first think about how to structure our narrative.\\nIndeed Dijk (1977) states that all texts have an innate macrostructurei.e., a global discourse structure'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='spanning the entirety of the text. In general scientific discourse, one such macrostructure is that of\\nproblem-solving (Heffernan and Teufel, 2022), where an author must first motivate and describe the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='problem, before proposing an appropriate solution.\\nComposing such a macrostructure is no easy task. Yet in the realm ofLLMs, there have been many\\nrecent efforts to help guide model generation using similar structures. One such popular approach'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='is that of creating outlines, which provide a high-level overview of the desired narrative (Li et al.,\\n2024). An alternative approach to outlines is creating summaries of future targets, which the model\\ncan then expand upon (Sun et al., 2022).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='can then expand upon (Sun et al., 2022).\\nGiven the nature of theLCM operating at the concept level, it naturally creates long-form output.\\nTherefore, it is important to ensure that the model is capable of creating coherent generations given'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='the multitudes of possibilities for next concept prediction. In order to address this, we envision an\\n6English, French, Hindi, Portuguese, Spanish and Thai.\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='explicit capability for planning. Similar to creating summaries, we propose a complementaryplanning\\nmodel which creates a high-level overview of what should be generated next, given the prior context.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='The proposed plan could span multiple concepts, such as a paragraph. TheLCM is conditioned on\\nthis plan, before it then generates the subsequent output sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='Operationally the model predicts auto-regressively a sequence of concepts followed by abreak concept,\\nwhich represents a natural topic cadence such as a paragraph break. Once thebreak concept is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='predicted, the large planning model (LPM) generates a plan in order to condition theLCM for\\nprediction of the subsequent sequence. The model then continues generation as usual conditioned on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='both the prior concepts and the proposed plan. An overview of the approach is shown in Figure 17.\\nFigure 17 - LCM conditioned on both the prior context and a high-level plan for the next sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='Although we envision a separate (but complementary) model for planning, we present here an initial\\nexperiment using a simplified single-model approach where theLCM is trained in a multitask setting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='to also predict both thebreak concepts and plans. Additionally, instead of the idealized plan spanning\\nmultiple concepts (such as a paragraph), we use a single concept in order to capture what should'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='come next (i.e. aplan concept). We call this simplified multitask approach a Large Planning Concept\\nModel (LPCM).\\nMethodology\\nIn order to evaluate this single-model approach, we perform an ablation study. As a baseline method,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='we train aOne-Tower LCM(cf. Section 2.3.3) without any visibility tobreak or plan concepts.\\nWe then subsequently train aLPCM with the same number of parameters as the baseline. Both\\nmodels were trained on the same data7 for the same number of steps.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='Data preprocessing. In order to representbreak concepts, we begin by first segmenting the data\\ninto paragraphs. Given that most real world datasets are absent of paragraph structure (or it is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='not easy to recover), we apply the Segment Any Text (Frohmann et al., 2024) paragraph splitting\\nAPI8. We additionally force each paragraph to be less than 10 sentences, and merge small (e.g. one'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='sentence) consecutive paragraphs together. In order to representplan concepts, we generate synthetic\\nhigh-level topic description for each preceding segmented paragraph using an existing open-sourced'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='LLM, namely Llama-3.1-8B-IT, which offers a good trade-off between the generated topic quality\\nand the generation speed. The system prompt used to generate these topic descriptions is listed in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='Appendix C. In total we process approximately320M paragraphs with topic descriptions, spanning\\n1.5B segmented concepts (i.e. approximately 30B tokens).\\nMetrics. We focus on coherence as our main measure of evaluation. Previous ablations (cf.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='Section 2.4) used the coherence metric introduced by Jwalapuram et al. (2022). However, we explore\\n7Compared to previous experiments, we use a different data mixture more favorable for long-form generation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='8https://github.com/segment-any-text/wtpsplit\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='here LLM-as-a-judge as an alternative. Specifically, we use Llama-3.1-8B-IT in order to evaluate the\\ncoherence of the generated model outputs, which is prompted to return an overall coherence score'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='between [0, 5]. The prompt used is listed in Appendix D. In order to validate this prompt, we evaluate\\nit against a dataset of human judgements introduced by Jwalapuram et al. (2022), and observed it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='reported an agreement9 with human annotators which improves upon their coherence model. We\\ntherefore choose this metric for our evaluation. To be consistent across both model results, we do'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='not include the specialbreak or plan concepts generated by theLPCM when calculating coherence\\nscores.\\nLlama-3.1-8B-IT (↑)\\nLPCM 2.82 ± 0.62\\nBaseline 2.74 ± 0.70\\nTable 13- LPCM ablation coherence score results.\\nResults'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='Results\\nWe provide the results of our ablation experiment in Table 13. Results are reported over a held-out\\nsubset ofCosmopedia (Ben Allal et al., 2024) following instruction fine-tuning, similar to previous'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='ablations (cf. Section 2.4). We observe that theLPCM achieves significantly10 higher coherence\\nscores (significance was measured using a paired t-test) than the baselineOne-Tower LCM. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='finding suggests that theLPCM is capable of producing significantly more coherent outputs than\\nthe LCM as a result of the additional structure coming from predicted plan concepts, helping the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='LPCM produce a more coherent narrative, which is essential for the objective of generating long-form\\noutput.\\n5 Related work\\n5.1 Sentence representations\\nMultilingual sentence representations Learning effective sentence embeddings has been a well'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='studied subject in recent years. Significant progress has been made in this field, largely due to the\\ncapabilities of transformer-based language models that by learning contextual representations for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='individual tokens (Devlin et al., 2018; Conneau et al., 2020), are able to effectively learn the semantics\\nof language. However, such models are not optimal to create sentence representations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='Following approaches built upon these initial works, and aimed at learning general sentence repre-\\nsentations, leveraging dual encoder architectures (Guo et al., 2018; Reimers and Gurevych, 2019;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='Ni et al., 2021). These architectures encode the source and target into a common embedding space,\\nand use a distance-based metric to create an alignment loss that approximate semantically identical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='sentences. Such architectures have been extended to leverage multilingual data to create general,\\naligned embedding spaces across languages (Feng et al., 2020; Janeiro et al., 2024; Sturua et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='2024). Initial approaches leveraged the contrastive loss to align translations across languages (Feng\\net al., 2020; Yang et al., 2019), using only translation data to train. Other architectural changes,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='namely using token-level objectives combined with the sentence level objectives, have proven useful\\nto improve the quality of multilingual sentence representations based on translation data only (Li\\n9Krippendorff’s α = 0.48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='9Krippendorff’s α = 0.48\\n10Significance observed at the 99.9% level.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='et al., 2023; Janeiro et al., 2024). Recent approaches explore using data from other tasks, besides\\ntranslation data, to increase the generality of the sentence representations (Wang et al., 2024b; Mohr'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='et al., 2024). Other approaches change their embeddings per task, either with task-specific prompts\\n(Wang et al., 2024b; Su et al., 2022; Lee et al., 2024b) or with task-specific parameters (Sturua et al.,\\n2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='2024).\\nAnother successful line of work to create general purpose, multilingual, sentence representations is to\\nleverage the translation objective.LASER (Artetxe and Schwenk, 2019), andSONAR (Duquenne'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='et al., 2023b) leverage an encoder-decoder architecture, with a fixed-size sentence representation\\nbetween the encoder and the decoder, trained with a translation objective.SONAR is initialized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='from the NLLB-200 model (NLLB Team et al., 2022), and covers 200 languages, making it one of the\\nopen-source models with the widest language coverage.SONAR also provides open-source speech'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='encoders aligned to their sentence encoders for 73 languages (Seamless Communication et al., 2023a),\\naligned through a teacher-student approach.SONAR has been used as the basis for several works'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='(Seamless Communication et al., 2023a,b; Chen et al., 2023a), and its speech decoders have been\\nextended to keep the expressiveness of the original speech (Duquenne et al., 2023a).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='Joint speech/text sentence representations There has been a large body of research on\\nunsupervised representation learning for monolingual (Baevski et al., 2020) and multilingual speech'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='(Babu et al., 2022), with recently w2v-bert (Chung et al., 2021) that combines contrastive learning and\\nmasked language modeling to learn self-supervised representations from speech. Other works explored'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='multilingual and multimodal (speech/text) pre-training methods, including mSLAM (Bapna et al.,\\n2022). Finally, Duquenne et al. (2021), followed by Khurana et al. (2022), introduced multilingual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='and multimodal sentence embeddings, extending a pre-existing multilingual text sentence embedding\\nspace to the speech modality with a distillation approach. Duquenne et al. (2022, 2023c) also showed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='that it is possible to efficiently decode multilingual speech sentence embeddings with decoders trained\\non text sentence embeddings into different languages, to perform zero-shot speech translation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='LLM based sentence representations Several text representation methods have been proposed\\nwhich are based on existingLLMs. Wang et al. (2024a) proposed extracting text embeddings from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='the last token of LLMs fine-tuned with instructions on contrastive data. Lee et al. (2024a) improved\\ntext embedding capabilities of fine-tuned LLMs by removing the causal attention mask and applying'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='extra nonlinear layers before pooling the token embeddings. Embeddings as a service are supported\\nby some commercial LLM providers, for example,Mistral-embed.11 Such embeddings proved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='competitive on retrieval benchmarks; however, to the best of our knowledge, their applicability to\\nreconstructing the texts back from the embedding space has not been demonstrated.\\n5.2 Multilingual LLMs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='5.2 Multilingual LLMs\\nMost of the leadingLLMs have been trained on texts in several languages. Table 1 summarizes\\nthe coverage of several of them. Nevertheless, the pretraining data of theseLLMs seems to be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='mainly English texts. For example, The Llama3 team (2024) mentions that pretraining data contains\\nsignificantly more English texts, requiring continued pre-training with multilingual data, out of which\\n34.6% is translated reasoning data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='34.6% is translated reasoning data.\\nThere are also several efforts to trainLLMs optimized on specific languages, e.g. LeoLM for\\nGerman,12 Fuano for Italian (Bacciu et al., 2024),ALLaM for Arabic (Bari et al., 2024), and several'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='11https://docs.mistral.ai/capabilities/embeddings/\\n12https://laion.ai/blog/leo-lm/\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='models for Chinese:ErniBot,13 Tongyi Qianwen,14 or ChatGLM (Team GLM et al., 2024).\\nSome adaptations of LLMs to a massive number of languages also exist. LOLA (Srivastava et al., 2024)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='is a recent mixture-of-experts LLM supporting 160 languages, MALA-500 (Ji et al., 2024) adapts\\nLLaMA2 to 546 languages. However, such models typically face a trade-off between language coverage'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='and other capabilities. For example, the Aya model (Üstün et al., 2024), following instructions in\\n101 languages, was superseded by Aya-23 (Aryabumi et al., 2024) that exchanged some breadth for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='depth, focusing on 23 languages only. The LCM architecture, combining a language-agnostic model\\nfor knowledge and reasoning with potentially language-specialized encoders and decoders, is expected\\nto exhibit this trade-off to a lesser extent.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='to exhibit this trade-off to a lesser extent.\\n5.3 Alternative LLM architectures\\nPredicting the next state in the embedding space is a core idea of the Joint Embedding Predictive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='Architecture (Jepa) proposed by LeCun (2022). This idea has been implemented for images (I-JEPA\\nby Assran et al. (2024)) and video (V-JEPA by Bardes et al. (2024)) as a self-supervised approach'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='to learning representations. For language, equivalent models have not yet been explored.\\nSentence embeddings for language modeling. For text completion, Ippolito et al. (2020)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='proposed a sentence-level language model operating by choosing the next sentence from a finite set\\nof candidates. Their model demonstrated success in selecting appropriate continuations for short'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='stories, but it has not been scaled to longer inputs or to fully generative outputs. Golestani et al.\\n(2021) studied a similar problem in the even more restrictive sentence ordered setting, but with a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='more thorough study of architectural choices. The INSET architecture (Huang et al., 2020) solves the\\nsentence infilling task by combining a denoising autoencoder that encodes sentences into fixed-size'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='vectors and decodes them back and a bidirectional transformer that predicts the embedding of a\\nmissing sentence.\\nMarfurt and Henderson (2021) and Cornille et al. (2024) used predicted next sentence embeddings in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='a fully generative setting, for summarization and generic language modeling, respectively. However,\\ntheir architectures considered sentence-level connections only as an addition to the token-level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='connections across sentences, not as their replacement.\\nIn a recent work of An et al. (2024), the SentenceVAE architecture performs language modeling on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='the sentence level using a sentence encoder to prepare the inputs and a sentence decoder to produce\\nthe outputs. However, its input and output embedding spaces are not tied, so the inference is only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='possible by decoding each predicted sentence into text and then re-encoding it for adding it to the\\ncontext.\\nLanguage modeling with diffusion. A series of more recent works tried adapting diffusion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='modeling, originally developed for continuous data, to the discrete text domain. The PLANNER\\narchitecture (Zhang et al., 2023) consists of a variational autoencoder for paragraphs and a diffusion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='model trained to predict latent autoencoder representations conditional on the textual context or on\\nthe class label. Lovelace et al. (2024) augmented a decoder-only language model with an encoded'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='semantic proposal of the continuation text, with an easily guidable diffusion model predicting the\\nembedding of the next proposal. A TEncDM model (Shabalin et al., 2024) performs diffusion in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='space of contextual token embeddings which are then decoded non-autoregressively.\\n13http://research.baidu.com/Blog/index-view?id=183\\n14https://www.alibabacloud.com/en/solutions/generative-ai\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='Some applications of diffusion to sequence modeling have targeted the planning capabilities of the\\nsequence models. Semformer (Yin et al., 2024) proposed training transformers language models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='to plan several steps ahead by including special planning tokens, the representations of which are\\ntrained to be informative about the future tokens. Ye et al. (2024) applied discrete diffusion to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='language models as an alternative to autoregressive generation, more suitable for tasks that require\\nmulti-step planning. Ubukata et al. (2024) give an overview of applications of diffusion for planning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='tasks, but most of them are not concerned with the language domain.\\nOverall, while many of the previous works used hidden representations for language modeling or'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='related tasks, all of them either relied on token-level inputs or outputs, or were not intented for\\ngenerating texts of arbitrary length. TheLCM seems to be the first fully generative language model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='implemented fully in a highly semantic, reconstructable sentence representation space.\\n6 Limitations\\nIn this section we discuss the possible limitations of the presented Large Concept Modeling approach.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='Choice of the embedding space.The choice and design of the embedding space plays a crucial\\nrole in theLCM modeling approach.\\n• The SONAR embedding space was chosen for its good multilingual and multimodal represen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='tations, as well as the availability of a massively multilingual decoder, which achieves excellent\\nresults in both translation and auto-encoding. However, theSONAR model was trained on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='very specific training data, namely bitext machine translation data containing rather short\\nsentences. This has several consequences:\\n1. SONAR is trained to sustain a local geometry (sentences with very similar meanings are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='geometrically close) with no special guarantees for sentences that are only loosely related.\\nYet, predicting next sentences distribution requires the space to operate well globally.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='2. SONAR auto-encodes surprisingly well texts containing links, references, or merely\\nnumbers or code data. Yet, such texts tend to be fragile, highlighting a distribution'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='mismatch between theSONAR training data and commonly usedLLM pre-training text\\ncorpora. Therefore, the accurate prediction of the sentences containing such a content\\n(non-negligible inLCM pre-training data) will be hard for anyLCM SONARbased model.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='For instance, the factuality of fragile generated sentences may easily be compromised.\\n• Using a frozen encoder represents some interesting trade-offs. Any frozen encoder which is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='learned in a different data context, and with no a-priori strong connection toLCM modeling,\\nmay be suboptimal compared to encoders that are learned in an end-to-end fashion (with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='the loss coming from the decoder). At the same time, learning an encoder within end-to-end\\ntraining can be challenging and the resulting space is not guaranteed to result in good semantic\\nrepresentations shared across languages and modalities.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='Training the concept representation and theLCM end-to-end would also be less data and\\ncompute efficient since all modeling data should be multilingual and -modal, bearing the risk\\nof modality competition.\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='Concept granularity\\n• In this work, the definition of concepts is interpreted at sentence level. However, the manifold of\\npossible next sentences is very wide, attributing a proper probability to each of such sentences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='is much harder (even with a modeling within the latent space) that to the discrete set of tokens.\\n• In NLP, we encounter sentences of variable length. Combinatorial complexity of possible next'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='sentences grows exponentially with the maximum character length. The choice of granularity\\nfor LCM is not trivial as long sentences (>120 characters) could reasonably be considered as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='several concepts. However, any finer splitting of such sentences does not necessary separate\\nwell these concepts. This shows the limitation of a fixed size embedding representation for one'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='sentence. Text splitting (such as sentence splitting) or one-to-many mapping of a sentence into\\nseveral embeddings is a major future direction of research.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='• Each document in a training corpus typically contains a sequence of unique sentences or a little\\nnumber of repetitions. This data sparsity effect manifests as well at large corpora level: the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='large majority of sentences are merely unique. In principle, this issue can be addressed with\\nhigher-level semantic embedding representations. These higher-level representations come with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='trade-off between requirement of lossless data encoding (think of named-entities or numbers,\\ncritical in many language modeling tasks) and good level of abstraction to enable reasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='capabilities. Compared to a monolingual auto-encoder which would simply compress input\\nsentences, SONAR offers semantic representations with good auto-encoding quality but still\\ncertainly sacrificing generalization capacities.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='certainly sacrificing generalization capacities.\\n• This generalization issue can be partially mitigated by splitting or encoding input text as new\\nconceptual units which are more commonly shared across source documents. This is in the spirit'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='of stemming or lemmatization techniques studied in NLP for words. That being said, building\\nsuch conceptual units that are also language and modality agnostic is a challenging task. Such'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='shared multilingual and multimodal conceptual units are also key for generalization across\\nlanguages and across modalities. To maximize cross-lingual and cross-modal transfers,Large'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='Concept Modelsshould be exposed to a richer variety of multilingual and multi-modal data.\\nContinuous versus discrete\\n• Diffusion modeling has proven to be very efficient in generative modeling of continuous data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='like images or speech. As previously stated, sentences in theSONAR space, despite being\\nrepresented as continuous vectors, remain discrete combinatorial objects. This makes diffusion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='modeling struggle on the text modality (either at word or sentence embedding level).\\n• The contrastive nature of cross-entropy loss based on softmax outputs which is used for next'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='token prediction plays a critical role for many downstream task where higher accuracy is\\nrequired (e.g. MCQ tasks, code or math generation). On the opposite, continuous diffusion\\nmodeling does not allow to integrate such a contrastive objective.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='• The Quant-LCM could be a way to address the discrete nature of text while modeling on\\ncoarse-to-fine semantic units shared across languages and modalities. The limited performance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='of theQuant-LCM approaches presented in this paper may be explained by the fact that\\nSONAR space was not trained to be efficiently quantizable, yielding a significant number'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='of codebooks and a large amount of units per codebook. Therefore, the currentSONAR\\nquantization suffers from the exponentially increasing number of RVQ units combinations which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='does not solve the data sparsity/uniqueness issue discussed earlier. This indicates once again\\nthe importance of developing a new representation space, either continuous or discrete, for the\\nLarge Concept Model.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='7 Acknowledgments\\nWe would like to thank Robbie Adkins, Can Balioglu, Joy Chen, Pascale Fung, Jason Holland, Amita\\nKamath, Justine Kao, Sagar Miglani, Alice Rakotoarison, Abhilasha Sancheti, Arjang Talattof, Ellen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='Tan, Carleigh Wood, Shireen Yates, Bokai Yu and Luke Zettlemoyer for comments and suggestions\\non this work, as well helping us to improve this paper.\\n8 Conclusion and Future Work'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='8 Conclusion and Future Work\\nCurrent best practice for large scale language modeling is to operate at the token level, i.e. to\\nlearn to predict the next tokens given a sequence of preceding tokens. There is a large body of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='research on improvements ofLLMs, but most works concentrate on incremental changes and do\\nnot question the main underlying architecture. In this paper, we have proposed a new architecture,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='named aLarge Concept Model(LCM), which substantially differs from currentLLMs in two\\naspects: 1) all modeling is performed in a high-dimensional embedding space instead of on a discrete'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='token representation; and 2) modeling is not instantiated in a particular language or modality, but\\nat a higher semantic and abstract level. We have named the general form of this representation\\na “concept”.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='a “concept”.\\nInthispaper, toverifythefeasibilityofthehigh-levelidea, wehaveassumedthataconceptcorresponds\\nto a sentence in the text domain, or an equivalent speech segment, and that the embeddings are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='obtained by the freely availableSONAR sentence encoder (Duquenne et al., 2023b). With respect to\\nthe specific architecture of theLCM, we have first shown that directly minimizing the MSE loss in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='the embedding space does not yield good results. We then explored several architectures based on a\\ndiffusion process: theOne-Tower and Two-Tower LCM, as well as aQuant-LCM which uses'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='quantization of SONAR representations and then modeling on these discrete units. These ablation\\nexperiments were performed with models with 1.6B parameters and focused on the generative task of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='continuing a sequence of sentences. We have then scaled our models to a size of 7B parameters and\\ninstruction-finetuned them on several summarization and summary expansion tasks. We provide a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='detailed comparison to other public models of the same size, namelyGemma, Mistral and Llama.\\nBy design, aLCM exhibits strong zero-shot generalization performance. In this paper, we trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='models on English texts only, and applied them to text in other languages, without any additional\\ntraining data, neither aligned nor unlabeled. TheLCM outperforms Llama-3.1-8B-IT on English'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='and on the average over foreign languages officially supported by theLLM. TheLCM itself could\\nalso be trained on multilingual- and model data to acquire knowledge from these sources. We will'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='explore this in future versions of theLCM. In short, all languages and modalities are first class\\ncitizens and handled equally at all stages of aLCM.\\nWe have observed that next sentence prediction is substantially more challenging than next token'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='prediction. First, given that we operate in an embedding space and at a higher semantic level, the\\nnumber of possible sentences is virtually unlimited, while token vocabularies are usually in the range'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='of 100k. Second, even given a long context, there is unavoidably more ambiguity in choosing the\\nnext sentence than the next token. And third, the usual softmax output layer over the fixed size'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='token vocabulary provides a normalized probability distribution over all possible token continuations.\\nTheoretically, a diffusion process should be able to learn a probability distribution over an output'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='embedding space, but our current experimental evidence indicates that more research is needed to\\ntake full advantage of the properties ofLarge Concept Models. As an example, the ability'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='to sample multiple embeddings and associate a score would enable beam search to find the best\\nsequence of sentences. Finally, small modeling errors could yield predictions in the embedding space\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='which do not correspond to valid sentences, i.e. that cannot be decoded into a syntactically and\\nsemantically correct sentence. We will work on alternative concept embeddings toSONAR which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='would be better suited to the next sentence prediction task, and would improve modeling approaches\\nin that concept embedding space.\\nWe see the models and results discussed in this paper as a step towards increasing scientific diversity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='and a move away from current best practice in large scale language modeling. We acknowledge that\\nthere is still a long path to reach the performance of current flagshipLLMs. This will require of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='course further improving the core architecture, but also careful data selection and curation, extensive\\nablations, optimized and diverse instruction fine-tuning, and finally, scaling to models with more\\nthan 70B parameters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='than 70B parameters.\\nWe open-source the full training code of all ourLCM variants, together with a set of supporting\\nscripts,15 to make it easy for other teams to trainLCM models. By these means, we hope to foster'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='research on alternativeLLMs and contribute to advance the field of machine intelligence.\\nReferences\\nA. Aghajanyan, L. Yu, A. Conneau, W.-N. Hsu, K. Hambardzumyan, S. Zhang, S. Roller, N. Goyal, O. Levy,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='and L. Zettlemoyer. Scaling laws for generative mixed-modal language models. InInternational Conference\\non Machine Learning, pages 265–279. PMLR, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='on Machine Learning, pages 265–279. PMLR, 2023.\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru, M. Debbah, Étienne Goffinet, D. Hesslow,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='J. Launay, Q. Malartic, D. Mazzotta, B. Noune, B. Pannier, and G. Penedo. The Falcon series of open\\nlanguage models. ArXiv, abs/2311.16867, 2023. URLhttps://arxiv.org/pdf/2311.16867.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='H. An, Y. Chen, Z. Sun, and X. Li. SentenceVAE: Enable next-sentence prediction for large language models\\nwith faster speed, higher accuracy and longer context.arXiv preprint arXiv:2408.00655, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='Anthropic. The Claude 3 model family: Opus, sonnet, haiku, 2024. URLhttps://www-cdn.anthropic.com/\\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='M. Artetxe and H. Schwenk. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer\\nand beyond. TACL, pages 597–610, 2019.\\nV.Aryabumi, J.Dang, D.Talupuru, S.Dash, D.Cairuz, H.Lin, B.Venkitesh, M.Smith, K.Marchisio, S.Ruder,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='et al. Aya 23: Open weight releases to further multilingual progress.arXiv preprint arXiv:2405.15032, 2024.\\nM. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas. Self-supervised'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='learning from images with a joint-embedding predictive architecture.ArXiv, abs/2301.08243, 2024. URL\\nhttps://arxiv.org/pdf/2301.08243.\\nA. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='A. Baevski, A. Conneau, and M. Auli. XLS-R: Self-supervised Cross-lingual Speech Representation Learning\\nat Scale. InProc. Interspeech 2022, pages 2278–2282, 2022. doi: 10.21437/Interspeech.2022-143.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='A. Bacciu, G. Trappolini, A. Santilli, E. Rodolà, and F. Silvestri. Fauno: The italian large language model\\nthat will leave you senza parole!ArXiv, abs/2306.14457, 2024. URLhttps://arxiv.org/pdf/2306.14457.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='A. Baevski, Y. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: A framework for self-supervised learning of\\nspeech representations.NeurIPS, 33:12449–12460, 2020.\\nC. Balioglu. fairseq2, 2023. URLhttp://github.com/facebookresearch/fairseq2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau. mslam:\\nMassively multilingual joint pre-training for speech and text.arXiv preprint arXiv:2202.01374, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='15https://github.com/facebookresearch/large_concept_model\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='A. Bardes, Q. Garrido, J. Ponce, X. Chen, M. Rabbat, Y. LeCun, M. Assran, and N. Ballas. Revisiting\\nfeature prediction for learning visual representations from video.ArXiv, abs/2404.08471, 2024. URL\\nhttps://arxiv.org/pdf/2404.08471.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='https://arxiv.org/pdf/2404.08471.\\nM. S. Bari, Y. Alnumay, N. A. Alzahrani, N. M. Alotaibi, H. A. Alyahya, S. AlRashed, F. A. Mirza, S. Z.\\nAlsubaie, H. A. Alahmed, G. Alabduljabbar, R. Alkhathran, Y. Almushayqih, R. Alnajim, S. Alsubaihi,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='M. A. Mansour, M. Alrubaian, A. Alammari, Z. Alawami, A. Al-Thubaity, A. Abdelali, J. Kuriakose,\\nA. Abujabal, N. Al-Twairesh, A. Alowisheq, and H. Khan. ALLaM: Large language models for arabic and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='english. ArXiv, abs/2407.15390, 2024. URLhttps://arxiv.org/pdf/2407.15390.\\nL. Ben Allal, A. Lozhkov, G. Penedo, T. Wolf, and L. von Werra. Cosmopedia, 2024. URL https://\\nhuggingface.co/datasets/HuggingFaceTB/cosmopedia.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='huggingface.co/datasets/HuggingFaceTB/cosmopedia.\\nJ. Betker, G. Goh, L. Jing, TimBrooks, J. Wang, L. Li, LongOuyang, JuntangZhuang, JoyceLee, YufeiGuo,\\nWesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao, and A. Ramesh. Improving image generation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='with better captions, 2023. URLhttps://api.semanticscholar.org/CorpusID:264403242.\\nBigScience Workshop. BLOOM: a 176b-parameter open-access multilingual language model. ArXiv,\\nabs/2211.05100, 2023. URLhttps://arxiv.org/pdf/2211.05100.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='Chameleon team. Chameleon: Mixed-modal early-fusion foundation models.ArXiv, abs/2405.09818, 2024.\\nURL https://arxiv.org/pdf/2405.09818.\\nM. Chen, P.-A. Duquenne, P. Andrews, J. Kao, A. Mourachko, H. Schwenk, and M. R. Costa-jussà. BLASER:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='A text-free speech-to-speech translation evaluation metric. In ACL, pages 9064–9079, 2023a. URL\\nhttps://aclanthology.org/2023.acl-long.504.\\nM. Chen, K. Heffernan, O. Çelebi, A. Mourachko, and H. Schwenk. xSIM++: An improved proxy to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='bitext mining performance for low-resource languages. In ACL, pages 101–109, 2023b. URL https:\\n//aclanthology.org/2023.acl-short.10.\\nR. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers.arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='preprint arXiv:1904.10509, 2019.\\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma,\\net al. Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):1–53,\\n2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='2024.\\nY.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu. W2v-bert: Combining contrastive\\nlearning and masked language modeling for self-supervised speech pre-training. In2021 IEEE Automatic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='Speech Recognition and Understanding Workshop (ASRU), pages 244–250. IEEE, 2021.\\nE. Clark, S. Rijhwani, S. Gehrmann, J. Maynez, R. Aharoni, V. Nikolaev, T. Sellam, A. Siddhant, D. Das,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='and A. Parikh. Seahorse: A multilingual, multifaceted dataset for summarization evaluation. InProceedings\\nof the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9397–9413, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettle-\\nmoyer, and V. Stoyanov. Unsupervised cross-lingual representation learning at scale. InACL, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='N. Cornille, M.-F. Moens, and F. Mai. Learning to plan for language modeling from unlabeled data.arXiv\\npreprint arXiv:2404.00614, 2024.\\nM. R. Costa-jussà, P. Andrews, M. C. Megliogli, J. Chen, J. Chuang, D. Dale, C. Ropers, A. Mourachko,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='E. Sánchez, H. Schwenk, T. Tran, A. Turkatenko, and C. Wood. LCFO: Long context and long form output\\ndataset and benchmarking.ArXiv, 2024.\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='for language understanding.arXiv preprint arXiv:1810.04805, 2018.\\nP. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis.Advances in neural information\\nprocessing systems, 34:8780–8794, 2021.\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='V. Dijk. Text and Context: Explorations in the Semantics and Pragmatics of Discourse.Longman, 1977.\\nM. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazaré, M. Lomeli, L. Hosseini, and H. Jégou.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='The Faiss library.ArXiv, abs/2401.08281, 2024. URLhttps://arxiv.org/pdf/2401.08281.\\nP.-A. Duquenne, H. Gong, and H. Schwenk. Multimodal and multilingual embeddings for large-scale speech'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='mining. In NeurIPS, volume 34, pages 15748–15761, 2021. URLhttps://proceedings.neurips.cc/paper/\\n2021/file/8466f9ace6a9acbe71f75762ffc890f1-Paper.pdf.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='P.-A. Duquenne, H. Gong, B. Sagot, and H. Schwenk. T-modules: Translation modules for zero-shot cross-\\nmodal machine translation. In EMNLP, pages 5794–5806, 2022. URLhttps://aclanthology.org/2022.\\nemnlp-main.391.pdf.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='emnlp-main.391.pdf.\\nP.-A. Duquenne, K. Heffernan, A. Mourachko, B. Sagot, and H. Schwenk. Sonar expressive: Zero-shot\\nexpressive speech-to-speech translation, 2023a.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='expressive speech-to-speech translation, 2023a.\\nP.-A. Duquenne, H. Schwenk, and B. Sagot. SONAR: sentence-level multimodal and language-agnostic\\nrepresentations, 2023b. URLhttps://arxiv.org/abs/2308.11466.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='P.-A. Duquenne, H. Schwenk, and B. Sagot. Modular speech-to-text translation for zero-shot cross-modal\\ntransfer. In Interspeech, 2023c.\\nF. Feng, Y. Yang, D. Cer, N. Arivazhagan, and W. Wang. Language-agnostic BERT sentence embedding.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='arXiv preprint arXiv:2007.01852, 2020.\\nM. Frohmann, I. Sterner, I. Vulić, B. Minixhofer, and M. Schedl. Segment Any Text: A universal approach\\nfor robust, efficient and adaptable sentence segmentation. InEMNLP, pages 11908–11941, 2024. URL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='https://aclanthology.org/2024.emnlp-main.665.\\nO. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene: Scene-based text-to-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='image generation with human priors. InEuropean Conference on Computer Vision, pages 89–106. Springer,\\n2022.\\nGemini Team Google. Gemini 1.5 unlocking multimodal understanding across millions of tokens of conte.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='ArXiv, abs/2403.05530, 2024. URLhttps://arxiv.org/pdf/2403.05530.\\nM. Golestani, S. Z. Razavi, Z. Borhanifard, F. Tahmasebian, and H. Faili. Using BERT encoding and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='sentence-level language model for sentence ordering. InInternational Conference on Text, Speech, and\\nDialogue, pages 318–330. Springer, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='Dialogue, pages 318–330. Springer, 2021.\\nP. Goyal. Accurate, large minibatch sg d: training imagenet in 1 hour.arXiv preprint arXiv:1706.02677, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='M. Guo, Q. Shen, Y. Yang, H. Ge, D. Cer, G. H. Abrego, K. Stevens, N. Constant, Y.-H. Sung, B. Strope, et al.\\nEffective parallel corpus mining using bilingual sentence embeddings.arXiv preprint arXiv:1807.11906,\\n2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='2018.\\nT. Hang, S. Gu, C. Li, J. Bao, D. Chen, H. Hu, X. Geng, and B. Guo. Efficient diffusion training via min-snr\\nweighting strategy. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages\\n7441–7451, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='7441–7451, 2023.\\nT. Hasan, A. Bhattacharjee, M. S. Islam, K. Mubasshir, Y.-F. Li, Y.-B. Kang, M. S. Rahman, and R. Shahriyar.\\nXL-sum: Large-scale multilingual abstractive summarization for 44 languages. InFindings of the Association'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='for Computational Linguistics: ACL-IJCNLP 2021, pages 4693–4703, Online, Aug. 2021. Association for\\nComputational Linguistics. URLhttps://aclanthology.org/2021.findings-acl.413.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='K. Heffernan and S. Teufel. Problem-solving recognition in scientific text. InProceedings of the Thirteenth\\nLanguage Resources and Evaluation Conference, pages 6045–6058, Marseille, France, June 2022. European'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='Language Resources Association. URLhttps://aclanthology.org/2022.lrec-1.650.\\nK. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='machines to read and comprehend.Advances in neural information processing systems, 28, 2015.\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='J. Ho and T. Salimans. Classifier-free diffusion guidance.arXiv preprint arXiv:2207.12598, 2022.\\nJ. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models.CoRR, abs/2006.11239, 2020. URL\\nhttps://arxiv.org/abs/2006.11239.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='https://arxiv.org/abs/2006.11239.\\nJ. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J.\\nFleet, and T. Salimans. Imagen video: High definition video generation with diffusion models.ArXiv,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='abs/2210.02303, 2022. URLhttps://arxiv.org/abs/2210.02303.\\nM. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd. spaCy: Industrial-strength Natural Language\\nProcessing in Python, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='Processing in Python, 2020.\\nY. Huang, Y. Zhang, O. Elachqar, and Y. Cheng. INSET: Sentence infilling with inter-sentential transformer.\\nIn ACL, pages 2502–2515, 2020. URLhttps://aclanthology.org/2020.acl-main.226.pdf.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='D. Ippolito, D. Grangier, D. Eck, and C. Callison-Burch. Toward better storylines with sentence-level language\\nmodels. ArXiv, abs/2005.05255, 2020. URLhttps://arxiv.org/pdf/2005.05255.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='J. M. Janeiro, B. Piwowarski, P. Gallinari, and L. Barrault. Mexma: Token-level objectives improve sentence\\nrepresentations, 2024. URLhttps://arxiv.org/abs/2409.12737.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='S. Ji, Z. Li, I. Paul, J. Paavola, P. Lin, P. Chen, D. O’Brien, H. Luo, H. Schütze, J. Tiedemann, et al. Emma-500:\\nEnhancing massively multilingual adaptation of large language models.arXiv preprint arXiv:2409.17892,\\n2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='2024.\\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas,\\nE. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix,\\nand W. E. Sayed. Mixtral.ArXiv, abs/2401.04088, 2024. URLhttps://arxiv.org/pdf/2401.04088.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='P. Jwalapuram, S. Joty, and X. Lin. Rethinking self-supervision objectives for generalizable coherence modeling.\\nIn ACL, pages 6044–6059, 2022. URLhttps://aclanthology.org/2022.acl-long.418.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generative\\nmodels. Advances in neural information processing systems, 35:26565–26577, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='S. Khurana, A. Laurent, and J. Glass. Samu-xlsr: Semantically-aligned multimodal utterance-level cross-lingual\\nspeech representation.arXiv preprint arXiv:2205.08180, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='D. Kingma and R. Gao. Understanding diffusion objectives as the elbo with simple data augmentation.\\nAdvances in Neural Information Processing Systems, 36, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='N. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efficient transformer.arXiv preprint arXiv:2001.04451,\\n2020.\\nK. Krishna, J. Wieting, and M. Iyyer. Reformulating unsupervised style transfer as paraphrase generation. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='Empirical Methods in Natural Language Processing, 2020.\\nY. LeCun. A path towards autonomous machine intelligence, 2022. URLhttps://openreview.net/pdf?id=\\nBZ5a1r-kVsf.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='BZ5a1r-kVsf.\\nC. Lee, R. Roy, M. Xu, J. Raiman, M. Shoeybi, B. Catanzaro, and W. Ping. NV-Embed: Improved techniques\\nfor training LLMs as generalist embedding models.arXiv preprint arXiv:2405.17428, 2024a.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization.\\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523–\\n11532, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='11532, 2022.\\nJ. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, Y. Luan,\\nS. M. K. Duddu, G. H. Abrego, W. Shi, N. Gupta, A. Kusupati, P. Jain, S. R. Jonnalagadda, M.-W.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='Chang, and I. Naim. Gecko: Versatile text embeddings distilled from large language models, 2024b. URL\\nhttps://arxiv.org/abs/2403.20327.\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='K. Lee and S. Sengupta. Introducing the ai research supercluster — meta’s cutting-edge ai supercomputer for\\nai research, 2022. URLhttps://ai.facebook.com/blog/ai-rsc/.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='Y. Li, Q. Chen, W. Yan, W. Wang, Q. Zhang, and H. Sundaram. Advancing precise outline-conditioned text\\ngeneration with task duality and explicit outline control, 2024. URLhttps://arxiv.org/abs/2305.14459.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='Z. Li, S. Huang, Z. Zhang, Z.-H. Deng, Q. Lou, H. Huang, J. Jiao, F. Wei, W. Deng, and Q. Zhang. Dual-\\nalignment pre-training for cross-lingual sentence embedding. In A. Rogers, J. Boyd-Graber, and N. Okazaki,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers), pages 3466–3478, Toronto, Canada, July 2023. Association for Computational Linguistics.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='doi: 10.18653/v1/2023.acl-long.191. URL https://aclanthology.org/2023.acl-long.191.\\nC.-Y. Lin. Rouge: A package for automatic evaluation of summaries. InText summarization branches out,\\npages 74–81, 2004.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='pages 74–81, 2004.\\nS. Lin, B. Liu, J. Li, and X. Yang. Common diffusion noise schedules and sample steps are flawed. In\\nProceedings of the IEEE/CVF winter conference on applications of computer vision, pages 5404–5411, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='S. Liu, H. Lu, and J. Shao. Improved residual vector quantization for high-dimensional approximate nearest\\nneighbor search.arXiv preprint arXiv:1509.05195, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='J. Lovelace, V. Kishore, Y. Chen, and K. Q. Weinberger. Diffusion guided language modeling.ArXiv,\\nabs/2408.04220, 2024. URLhttps://arxiv.org/pdf/2408.04220.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='A. Lozhkov, L. Ben Allal, L. von Werra, and T. Wolf. Fineweb-edu, May 2024. URLhttps://huggingface.\\nco/datasets/HuggingFaceFW/fineweb-edu.\\nC. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='model sampling in around 10 steps.Advances in Neural Information Processing Systems, 35:5775–5787,\\n2022.\\nA. Marfurt and J. Henderson. Sentence-level planning for especially abstractive summarization. InACL,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='pages 1–14, 2021. URLhttps://aclanthology.org/2021.newsum-1.1.pdf.\\nB. Minixhofer, J. Pfeiffer, and I. Vulić. Where’s the point? self-supervised multilingual punctuation-agnostic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='sentence segmentation. InProceedings of the 61st Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 7215–7235, Toronto, Canada, July 2023. Association for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='Computational Linguistics. URLhttps://aclanthology.org/2023.acl-long.398.\\nI. Mohr, M. Krimmel, S. Sturua, M. K. Akram, A. Koukounas, M. Günther, G. Mastrapas, V. Ravishankar,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='J. F. Martínez, F. Wang, Q. Liu, Z. Yu, J. Fu, S. Ognawala, S. Guzman, B. Wang, M. Werk, N. Wang,\\nand H. Xiao. Multi-task contrastive learning for 8192-token bilingual text embeddings, 2024. URL\\nhttps://arxiv.org/abs/2402.17016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='https://arxiv.org/abs/2402.17016.\\nN. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Vanderwende, P. Kohli, and J. Allen. A\\ncorpus and cloze evaluation for deeper understanding of commonsense stories. InNAACL, pages 839–849,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='June 2016. URLhttps://aclanthology.org/N16-1098.\\nS. Narayan, S. B. Cohen, and M. Lapata. Don’t give me the details, just the summary! topic-aware\\nconvolutional neural networks for extreme summarization.arXiv preprint arXiv:1808.08745, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='J. Ni, G. H. Abrego, N. Constant, J. Ma, K. B. Hall, D. Cer, and Y. Yang. Sentence-t5: Scalable sentence\\nencoders from pre-trained text-to-text models.arXiv preprint arXiv:2108.08877, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. InInternational conference\\non machine learning, pages 8162–8171. PMLR, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='on machine learning, pages 8162–8171. PMLR, 2021.\\nA. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen. GLIDE:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='Towards photorealistic image generation and editing with text-guided diffusion models. In K. Chaudhuri,\\nS. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors,Proceedings of the 39th International'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pages 16784–\\n16804. PMLR, 17–23 Jul 2022. URLhttps://proceedings.mlr.press/v162/nichol22a.html.\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='M. Ning, M. Li, J. Su, A. A. Salah, and I. O. Ertugrul. Elucidating the exposure bias in diffusion models.\\narXiv preprint arXiv:2308.15321, 2023.\\nNLLB Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. Mejia-\\nGonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko,\\nC. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered machine'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='translation, 2022. URLhttps://arxiv.org/abs/2207.04672.\\nOpenAI. GPT-4 technical report.ArXiv, abs/2303.08774, 2024. URLhttps://arxiv.org/pdf/2303.08774.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine\\ntranslation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='pages 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.\\ndoi: 10.3115/1073083.1073135. URLhttps://aclanthology.org/P02-1040.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='A. Parola, J. M. Lin, A. Simonsen, V. Bliksted, Y. Zhou, H. Wang, L. Inoue, K. Koelkebeck, and R. Fusaroli.\\nSpeech disturbances in schizophrenia: Assessing cross-linguistic generalizability of nlp automated measures'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='of coherence. Schizophrenia Research, 259:59–70, 2023.\\nW. Peebles and S. Xie. Scalable diffusion models with transformers. InProceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, pages 4195–4205, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general\\nconditioning layer. InProceedings of the AAAI conference on artificial intelligence, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised\\nmultitask learners.OpenAI blog, 1(8):9, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='multitask learners.OpenAI blog, 1(8):9, 2019.\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring\\nthe limits of transfer learning with a unified text-to-text transformer.arXiv e-prints, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the\\nlimits of transfer learning with a unified text-to-text transformer.Journal of Machine Learning Research,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='21(140):1–67, 2020. URLhttp://jmlr.org/papers/v21/20-074.html.\\nN. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese Bert-networks.arXiv preprint\\narXiv:1908.10084, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='arXiv:1908.10084, 2019.\\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent\\ndiffusion models. CoRR, abs/2112.10752, 2021. URLhttps://arxiv.org/abs/2112.10752.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Borsos, F. de Chaumont Quitry,\\nP. Chen, D. E. Badawy, W. Han, E. Kharitonov, H. Muckenhirn, D. Padfield, J. Qin, D. Rozenberg,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='T. N. Sainath, J. Schalkwyk, M. Sharifi, M. T. Ramanovich, M. Tagliasacchi, A. Tudor, M. Velimirovic,\\nD. Vincent, J. Yu, Y. Wang, V. Zayats, N. Zeghidour, Y. Zhang, Z. Zhang, L. Zilka, and C. H. Frank.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='Audiopalm: A large language model that can speak and listen. CoRR, abs/2306.12925, 2023. URL\\nhttps://doi.org/10.48550/arXiv.2306.12925.\\nT. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='arXiv:2202.00512, 2022.\\nSeamless Communication, L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler,\\nP.-A. Duquenne, B. Ellis, H. Elsahar, J. Haaheim, J. Hoffman, M.-J. Hwang, H. Inaguma, C. Klaiber,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='I. Kulikov, P. Li, D. Licht, J. Maillard, R. Mavlyutov, A. Rakotoarison, K. R. Sadagopan, A. Ramakrishnan,\\nT. Tran, G. Wenzek, Y. Yang, E. Ye, I. Evtimov, P. Fernandez, C. Gao, P. Hansanti, E. Kalbassi, A. Kallet,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='A. Kozhevnikov, G. M. Gonzalez, R. S. Roman, C. Touret, C. Wong, C. Wood, B. Yu, P. Andrews,\\nC. Balioglu, P.-J. Chen, M. R. Costa-jussà, M. Elbayad, H. Gong, F. Guzmán, K. Heffernan, S. Jain, J. Kao,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='A. Lee, X. Ma, A. Mourachko, B. Peloquin, J. Pino, S. Popuri, C. Ropers, S. Saleem, H. Schwenk, A. Sun,\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='P. Tomasello, C. Wang, J. Wang, S. Wang, and M. Williamson. Seamless: Multilingual expressive and\\nstreaming speech translation.ArXiv, abs/2312.05187, 2023a. URLhttps://arxiv.org/abs/2312.05187.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='Seamless Communication, L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, P.-A. Duquenne,\\nH. Elsahar, H. Gong, K. Heffernan, J. Hoffman, C. Klaiber, P. Li, D. Licht, J. Maillard, A. Rakotoarison,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='K. R. Sadagopan, G. Wenzek, E. Ye, B. Akula, P.-J. Chen, N. E. Hachem, B. Ellis, G. M. Gonzalez,\\nJ. Haaheim, P. Hansanti, R. Howes, B. Huang, M.-J. Hwang, H. Inaguma, S. Jain, E. Kalbassi, A. Kallet,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='I. Kulikov, J. Lam, D. Li, X. Ma, R. Mavlyutov, B. Peloquin, M. Ramadan, A. Ramakrishnan, A. Sun,\\nK. Tran, T. Tran, I. Tufanov, V. Vogeti, C. Wood, Y. Yang, B. Yu, P. Andrews, C. Balioglu, M. R.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='Costa-jussà, O. Celebi, M. Elbayad, C. Gao, F. Guzmán, J. Kao, A. Lee, A. Mourachko, J. Pino, S. Popuri,\\nC. Ropers, S. Saleem, H. Schwenk, P. Tomasello, C. Wang, J. Wang, and S. Wang. SeamlessM4T - massively'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='multilingual & multimodal machine translation, 2023b. URLhttps://arxiv.org/abs/2308.11596.\\nA. Shabalin, V. Meshchaninov, E. Chimbulatov, V. Lapikov, R. Kim, G. Bartosh, D. Molchanov, S. Markov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='and D. Vetrov. Tencdm: Understanding the properties of diffusion model in the space of language model\\nencodings. ArXiv, abs/2402.19097, 2024. URLhttps://arxiv.org/pdf/2402.19097.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='N. Shazeer. Glu variants improve transformer.arXiv preprint arXiv:2002.05202, 2020.\\nJ. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models.CoRR, abs/2010.02502, 2020. URL\\nhttps://arxiv.org/abs/2010.02502.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='https://arxiv.org/abs/2010.02502.\\nN. Srivastava, D. Kuchelev, T. M. Ngoli, K. Shetty, M. Röder, D. Moussallem, H. Zahera, and A.-C. N. Ngomo.\\nLola–an open-source massively multilingual large language model.arXiv preprint arXiv:2409.11272, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='S. Sturua, I. Mohr, M. K. Akram, M. Günther, B. Wang, M. Krimmel, F. Wang, G. Mastrapas, A. Koukounas,\\nN. Wang, and H. Xiao. jina-embeddings-v3: Multilingual embeddings with task lora, 2024. URLhttps:\\n//arxiv.org/abs/2409.10173.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='//arxiv.org/abs/2409.10173.\\nH. Su, W. Shi, J. Kasai, Y. Wang, Y. Hu, M. Ostendorf, W.-t. Yih, N. A. Smith, L. Zettlemoyer, and T. Yu.\\nOne embedder, any task: Instruction-finetuned text embeddings.arXiv preprint arXiv:2212.09741, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position\\nembedding. Neurocomputing, 568:127063, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='embedding. Neurocomputing, 568:127063, 2024.\\nX. Sun, Z. Sun, Y. Meng, J. Li, and C. Fan. Summarize, outline, and elaborate: Long-text generation via'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='hierarchical supervision from extractive summaries. InProceedings of the 29th International Conference\\non Computational Linguistics, pages 6392–6402, Gyeongju, Republic of Korea, Oct. 2022. International'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='Committee on Computational Linguistics. URLhttps://aclanthology.org/2022.coling-1.556.\\nTeam GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang, D. Rojas, G. Feng, H. Zhao, H. Lai,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Sun, J. Li, L. Zhao, L. Wu,\\nL. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An,\\nY. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='ChatGLM: A family of large language models from glm-130b to glm-4 all tools.ArXiv, abs/2406.12793,\\n2024. URL https://arxiv.org/pdf/2406.12793.\\nThe Llama3 team. The Llama 3 herd of models.ArXiv, abs/2407.21783, 2024. URLhttps://arxiv.org/pdf/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='2407.21783.\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava,\\nS. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez,\\nM. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta,\\nK. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,\\nS. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='T. Ubukata, J. Li, and K. Tei. Diffusion model for planning: A systematic literature review. ArXiv,\\nabs/2408.10266, 2024. URLhttps://arxiv.org/pdf/2408.10266.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='A. Üstün, V. Aryabumi, Z.-X. Yong, W.-Y. Ko, D. D’souza, G. Onilude, N. Bhandari, S. Singh, H.-L. Ooi,\\nA. Kayid, et al. Aya model: An instruction finetuned open-access multilingual language model.arXiv\\npreprint arXiv:2402.07827, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='preprint arXiv:2402.07827, 2024.\\nC. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and\\nF. Wei. Neural codec language models are zero-shot text to speech synthesizers.CoRR, abs/2301.02111,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='2023. URL https://doi.org/10.48550/arXiv.2301.02111.\\nL. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with large\\nlanguage models. InACL, pages 11897–11916, 2024a. URLhttps://aclanthology.org/2024.acl-long.642.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Multilingual e5 text embeddings: A\\ntechnical report, 2024b. URLhttps://arxiv.org/abs/2402.05672.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='A. Warstadt, A. Singh, and S. R. Bowman. Neural network acceptability judgments. Transactions of\\nthe Association for Computational Linguistics, 7:625–641, 2019. doi: 10.1162/tacl_a_00290. URL\\nhttps://aclanthology.org/Q19-1040.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='https://aclanthology.org/Q19-1040.\\nS. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with unlikelihood\\ntraining. arXiv preprint arXiv:1908.04319, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='training. arXiv preprint arXiv:1908.04319, 2019.\\nY. Yang, G. H. Abrego, S. Yuan, M. Guo, Q. Shen, D. Cer, Y.-H. Sung, B. Strope, and R. Kurzweil. Improving'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='multilingual sentence embedding using bi-directional dual encoder with additive margin softmax.arXiv\\npreprint arXiv:1902.08564, 2019.\\nJ. Ye, J. Gao, S. Gong, L. Zheng, X. Jiang, Z. Li, and L. Kong. Beyond autoregression: Discrete diffusion for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='complex reasoning and planning.ArXiv, abs/2410.14157, 2024. URLhttps://arxiv.org/pdf/2410.14157.\\nY. Yin, J. Ding, K. Song, and Y. Zhang. Semformer: Transformer language models with semantic planning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='ArXiv, abs/2409.11143, 2024. URLhttps://arxiv.org/pdf/2409.11143.\\nN. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495–507, 2021.\\nB. Zhang and R. Sennrich. Root mean square layer normalization.Advances in Neural Information Processing\\nSystems, 32, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='Systems, 32, 2019.\\nX. Zhang, Y. Zhang, D. Long, W. Xie, Z. Dai, J. Tang, H. Lin, B. Yang, P. Xie, F. Huang, M. Zhang, W. Li,\\nand M. Zhang. mgte: Generalized long-context text representation and reranking models for multilingual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='text retrieval, 2024. URLhttps://arxiv.org/abs/2407.19669.\\nY. Zhang, J. Gu, Z. Wu, S. Zhai, J. Susskind, and N. Jaitly. Planner: Generating diversified paragraph via\\nlatent language diffusion model. InNeurIPS, 2023.\\n47'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='A Technical consideration for data preparation\\nSince our modeling approach uses a fixed encoder and a fixed document segmentation method, we\\ndecided to use pre-computedSONAR embeddings instead of producing them on-the-fly for each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='training run. This allows for faster iteration on the same data mix, trading expensive GPU compute\\nagainst storage capacity.\\nAs we are storing sequences ofSONAR embedding, which are fixed size tensors of 1024 floats, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='storage requirements become more demanding than storing the raw text. For one terra bytes of raw\\ntext data we need to store between fifteen and twenty terra bytes of encoded data. Overall, this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='trade-off in space vs compute reduces the GPU memory occupation and the compute load and lets\\nuse iterate faster. Typically, with on single GPU we can produce around 300-400SONAR sentence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='embeddings per second whereas by loading precomputed data (potentially from remote storage) we\\ncan load over 20 thousand embeddings per second per GPU (with around 15 CPU per GPU).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='We store sequences ofSONAR embeddings with 16 bits precision (FP16) in parquet datasets.\\nEmbeddings remain aligned with the segmented texts and the parquet binary format and library'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='ecosystem is well suited for storing and loading efficiently such complex data structures. Parquet\\nalso lets us store extra data (such as quality metrics for each sentences) and enables non-trivial last\\nmile data filtering and transformation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='mile data filtering and transformation.\\nFor training theLCM, we processed around four billion documents, generating 310 billion sentences\\nwith an average of 27 tokens per sentences for 88 characters length on average; totaling a bit more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='than 889 terra-bytes of raw text.\\nB Open Sourced Code\\nIn the spirit of reproducibility, we release under an open source license the training, evaluation\\nand data processing code for theLCM. This is available athttps://github.com/facebookresearch/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='large_concept_model.\\nThe training code is based on theFairseq2 framework (Balioglu, 2023) that allowed us to build and\\niterate over the different model architectures discussed above. WhileFairseq2 shares the same name'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='as the popular fairseq toolchain, its API architecture is different. It is not a monolithic toolchain but\\na set of modules that can be composed, this allows us to build different architectures side by side\\nand easily share training components.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='and easily share training components.\\nWe also release our evaluation framework so the evaluation tasks reported in 3.1 and comparison\\nbetween theLCM and other models can easily be reproduced. The evaluation framework provides a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='clear abstraction betweenpredictors, tasks and data loading, which again, makes it modular and lets\\nus describe a set of tasks to be evaluated. The evaluation framework can be run locally or distributed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='over a SLURM cluster to run evaluations at scale.\\nFinally, we release an updated version of stopes16 to simplify large scale data pre-processing on a\\nSLURM cluster. This was used to run the sentence segmentation andSONAR encoding described'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='in section 2.2. The stopes data processing framework deals with scheduling and monitoring large\\nnumber of jobs on SLURM or to run everything locally for small scale jobs. It provides an API'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='compatible withray.data17 that makes it easy to process large datasets in blocks and apply transform\\n16https://github.com/facebookresearch/stopes\\n17https://docs.ray.io/\\n48'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='function over it. This makes our code reusable outside of a SLURM cluster as it can also be used\\nwith a ray.io cluster.\\nC System prompt: Generation of Topic Descriptions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='C System prompt: Generation of Topic Descriptions\\nYou are a topic description generator. Your job is to read an extract of text and then generate a\\ntopic description. The extract may be well formed or not. The topic description you will write will'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='be at most one sentence in length, and use as few words as possible. However, it can not be generic\\nand it can not contain any profanity.\\nHere is an example of an extract, an ideal topic description, and some examples of bad topic\\ndescriptions:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='descriptions:\\nExample extract: “One day, one neighborhood of the city was completely devasted. Glass windows\\nwere shattered, shops turned upside down, and many civilian killed. Superman instantly recognized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='the signature of one of his old enemies, Voltar, who he had barely beaten in the past. This was a\\nmessage to him: \"I challenge you! Come find me!\"\"\\nAn example of a good topic description: An old enemy of Superman’s, Voltar, appeared and challenged'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='him.\\nAn example of a bad topic description: Superman\\nAn example of a bad topic description: Voltar\\nAn example of a bad topic description:\\nD User prompt: LLM As a Judge - Coherence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='D User prompt: LLM As a Judge - Coherence\\nBelow is a text extract. Your task is to analyze the extract and assign a coherence score between 0\\nand 5 inclusive, where:\\n0: The text is completely incoherent and lacks any logical connection.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='1: The text has some minor connections, but overall it is disjointed and hard to follow.\\n2: The text has some coherence, but it is still difficult to understand due to unclear relationships\\nbetween ideas.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='between ideas.\\n3: The text is moderately coherent, with some clear connections between ideas, but may lack depth\\nor clarity.\\n4: The text is highly coherent, with clear and logical connections between ideas, making it easy to\\nfollow.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='follow.\\n5: The text is extremely coherent, with a clear and concise structure, making it effortless to\\nunderstand.\\nYou will provide a score ONLY. Do NOT also provide an explanation.\\nThe extract: <extract>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-13T00:48:12+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-13T00:48:12+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'LargeConceptModels.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='The extract: <extract>\\nAfter examining the extract, the coherence score between 0 and 5 inclusive is:\\n49')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 250, chunk_overlap = 50)\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "splitted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8eefe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000016F9FAB6F90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000016F9FB0D400>, model_name='meta-llama/llama-4-scout-17b-16e-instruct', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq.chat_models import ChatGroq\n",
    "llm = ChatGroq(api_key = , model = \"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356d5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
